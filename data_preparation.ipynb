{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tkwa2OSFsubn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42MOlrg3mDLK",
        "outputId": "6e48e146-dfa4-41b8-e168-f14d777df2a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nuqV2fDpswAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4be40c5-ee2a-4306-a42a-04a561ad6575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.2)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=620e3c4e975875a0e8b5e3df47286dede6d445a912de405677bc86e57cf3158a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.29.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "# Install SentencePiece for subword tokenization and text normalization in NLP tasks.\n",
        "!pip install sentencepiece\n",
        "\n",
        "# Install rouge-score for evaluating the quality of text summarization and generation using the ROUGE metric.\n",
        "!pip install rouge-score\n",
        "\n",
        "\n",
        "# Upgrade or install Accelerate for optimizing numerical computations, leveraging hardware acceleration techniques.\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kh1FC6bVs4zI"
      },
      "outputs": [],
      "source": [
        "# path to the research papers data folder\n",
        "input_data_path = '/content/drive/MyDrive/NLP Project/Project/Inupt_Data/scisummnet_release1.1__20190413/top1000_complete'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NUgc-CS4tGlm"
      },
      "outputs": [],
      "source": [
        "# loop through each folder and fetch the file paths.\n",
        "import os\n",
        "\n",
        "research_paper_fps= []\n",
        "\n",
        "summary_fps = []\n",
        "\n",
        "for folder_name in os.listdir(input_data_path):\n",
        "  # construct research paper folder path\n",
        "  research_paper_fp = os.path.join(input_data_path, folder_name, 'Documents_xml', folder_name + '.xml')\n",
        "\n",
        "  # construct summary folder path\n",
        "  summary_fp = os.path.join(input_data_path, folder_name, 'summary', folder_name + '.gold'+'.txt')\n",
        "\n",
        "  # check if both the file paths exist\n",
        "  if os.path.exists(research_paper_fp) and os.path.exists(summary_fp):\n",
        "      research_paper_fps.append(research_paper_fp)\n",
        "      summary_fps.append(summary_fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IppPy3wCyEig"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "\n",
        "# Function to parse the summary file\n",
        "def parse_summary(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = [line.strip() for line in file.readlines()[1:]]\n",
        "    return ' '.join(content)\n",
        "\n",
        "# Function to process an individual XML file\n",
        "def parse_xml(file_path):\n",
        "    # Parse the XML file\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extract the paper title\n",
        "    title = root.find('.//S').text\n",
        "\n",
        "    # Initialize a string to hold the rest of the paper content\n",
        "    content = ''\n",
        "\n",
        "    # # Removing acknowledgements which is of leats importance and ignoring abstracts as we have to predict the summaries\n",
        "    # sections = root.findall('.//SECTION[@title!=\"Acknowledgements\"]')\n",
        "\n",
        "\n",
        "    # Extract the text from all <S> tags except the first one (which is the title)\n",
        "    for s_tag in root.findall('.//S')[1:]:\n",
        "        text = s_tag.text\n",
        "        if text:\n",
        "            content += text + ' '  # Adding a space for separation between sections\n",
        "\n",
        "    return title, content\n",
        "\n",
        "    # # Extract the text from the filtered sections\n",
        "    # for section in sections:\n",
        "    #   for s_tag in section.findall('.//S'):\n",
        "    #     text = s_tag.text\n",
        "    #     if text:\n",
        "    #       content += text + ' '  # Adding a space for separation between sections\n",
        "\n",
        "    # return title, content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iNaPmrzPtMzR"
      },
      "outputs": [],
      "source": [
        "# Setting options to display the full DataFrame content\n",
        "pd.set_option('display.max_columns', None)  # Shows all columns\n",
        "pd.set_option('display.max_rows', None)     # Shows all rows\n",
        "pd.set_option('display.max_colwidth', None) # Shows full width of showing columns\n",
        "pd.set_option('display.width', None)        # Auto-detects the width of the terminal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4ejSmEDItOJs"
      },
      "outputs": [],
      "source": [
        "# Process each file and store the results in a DataFrame\n",
        "data = [parse_xml(file_path) for file_path in research_paper_fps]\n",
        "summary_data = [parse_summary(file_path) for file_path in summary_fps]\n",
        "df = pd.DataFrame(data, columns=['title', 'content'])\n",
        "df['summary'] = summary_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-Y91o38itQMM"
      },
      "outputs": [],
      "source": [
        "# storing the pre processed data into csv file to make ingore above functions after this step and use directly this file\n",
        "df.to_csv('Preprocessed_summarized_Data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_IMrJavrcurk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mofwTmBDu2zk"
      },
      "outputs": [],
      "source": [
        "df_processed = pd.read_csv(\"/content/Preprocessed_summarized_Data.csv\",encoding=\"utf-8\", on_bad_lines=\"skip\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_processed.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hlLoNgs2c0qo",
        "outputId": "c5ce72fe-ee54-4947-f93c-3df0a8d01d15"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                  title  \\\n",
              "0                                                         Class-Based N-Gram Models Of Natural Language   \n",
              "1  Syntax-Based Alignment Of Multiple Translations: Extracting Paraphrases And Generating New Sentences   \n",
              "2                                                                Assigning Time-Stamps To Event-Clauses   \n",
              "3                                       Hedge Trimmer: A Parse-And-Trim Approach To Headline Generation   \n",
              "4                Discriminative Training And Maximum Entropy Models For Statistical Machine Translation   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               content  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics. IBM T. J. Watson Research Center We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics. In a number of natural language processing tasks, we face the problem of recovering a string of English words after it has been garbled by passage through a noisy channel. To tackle this problem successfully, we must be able to estimate the probability with which any particular string of English words will be presented as input to the noisy channel. In this paper, we discuss a method for making such estimates. We also discuss the related topic of assigning words to classes according to statistical behavior in a large body of text. In the next section, we review the concept of a language model and give a definition of n-gram models. In Section 3, we look at the subset of n-gram models in which the words are divided into classes. We show that for n = 2 the maximum likelihood assignment of words to classes is equivalent to the assignment for which the average mutual information of adjacent classes is greatest. Finding an optimal assignment of words to classes is computationally hard, but we describe two algorithms for finding a suboptimal assignment. In Section 4, we apply mutual information to two other forms of word clustering. First, we use it to find pairs of words that function together as a single lexical entity. Then, by examining the probability that two words will appear within a reasonable distance of one another, we use it to find classes that have some loose semantic coherence. In describing our work, we draw freely on terminology and notation from the mathematical theory of communication. The reader who is unfamiliar with this field or who has allowed his or her facility with some of its concepts to fall into disrepair may profit from a brief perusal of Feller (1950) and Gallagher (1968). In the first of these, the reader should focus on conditional probabilities and on Markov chains; in the second, on entropy and mutual information. Source-channel setup. Figure 1 shows a model that has long been used in automatic speech recognition (Bahl, Jelinek, and Mercer 1983) and has recently been proposed for machine translation (Brown et al. 1990) and for automatic spelling correction (Mays, Demerau, and Mercer 1990). In automatic speech recognition, y is an acoustic signal; in machine translation, y is a sequence of words in another language; and in spelling correction, y is a sequence of characters produced by a possibly imperfect typist. In all three applications, given a signal y, we seek to determine the string of English words, w, which gave rise to it. In general, many different word strings can give rise to the same signal and so we cannot hope to recover w successfully in all cases. We can, however, minimize our probability of error by choosing as our estimate of w that string W for which the a posteriori probability of W given y is greatest. For a fixed choice of y, this probability is proportional to the joint probability of * and y which, as shown in Figure 1, is the product of two terms: the a priori probability of W and the probability that y will appear at the output of the channel when * is placed at the input. The a priori probability of W, Pr (W), is the probability that the string W will arise in English. We do not attempt a formal definition of English or of the concept of arising in English. Rather, we blithely assume that the production of English text can be characterized by a set of conditional probabilities, Pr(wk w), in terms of which the probability of a string of words, w, can be expressed as a product: Here, wki-1 represents the string wi w2 • • • wk_i . In the conditional probability Pr(wk I -1 , ) we call wk-1 the history and wk the prediction. We refer to a computational mechanism for obtaining these conditional probabilities as a language model. Often we must choose which of two different language models is the better one. The performance of a language model in a complete system depends on a delicate interplay between the language model and other components of the system. One language model may surpass another as part of a speech recognition system but perform less well in a translation system. However, because it is expensive to evaluate a language model in the context of a complete system, we are led to seek an intrinsic measure of the quality of a language model. We might, for example, use each language model to compute the joint probability of some collection of strings and judge as better the language model that yields the greater probability The perplexity of a language model with respect to a sample of text, S. is the reciprocal of the geometric average of the probabilities of the predictions in S. If S has I S I words, then the perplexity is Pr (5)-1/1s1. Thus, the language model with the smaller perplexity will be the one that assigns the larger probability to S. Because the perplexity depends not only on the language model but also on the text with respect to which it is measured, it is important that the text be representative of that for which the language model is intended. Because perplexity is subject to sampling error, making fine distinctions between language models may require that the perplexity be measured with respect to a large sample. In an n-gram language model, we treat two histories as equivalent if they end in the same n - 1 words, i.e., we assume that for k > n, Pr (wk w1k-1) is equal to Pr (wk I wifcin1+1). For a vocabulary of size V, a 1-gram model has V - 1 independent parameters, one for each word minus one for the constraint that all of the probabilities add up to 1. A 2-gram model has V(V - 1) independent parameters of the form Pr (102 I wi ) and V - 1 of the form Pr (w) for a total of V2 - 1 independent parameters. In general, an n-gram model has V&quot; - 1 independent parameters: V&quot;-1(V - 1) of the form Pr (wn I wr1), which we call the order-n parameters, plus the 17n-1-1 parameters of an (n - 1)-gram model. We estimate the parameters of an n-gram model by examining a sample of text, tf, which we call the training text, in a process called training. If C(w) is the number of times that the string w occurs in the string 1-T, then for a 1-gram language model the maximum likelihood estimate for the parameter Pr (w) is C(w)/T. To estimate the parameters of an n-gram model, we estimate the parameters of the (n -1)-gram model that it contains and then choose the order-n parameters so as to maximize Pr (tnT trii -1). Thus, the order-n parameters are We call this method of parameter estimation sequential maximum likelihood estimation. We can think of the order-n parameters of an n-gram model as constituting the transition matrix of a Markov model the states of which are sequences of n - 1 words. Thus, the probability of a transition between the state W1W2 • • ' Wn-1 and the state w2w3 • • • wn is Pr (w I W1102 • • • wn-i ) . The steady-state distribution for this transition matrix assigns a probability to each (n - 1)-gram, which we denote S(w7-1). We say that an n-gram language model is consistent if, for each string w7-1, the probability that the model assigns to win-1 is S(win-1). Sequential maximum likelihood estimation does not, in general, lead to a consistent model, although for large values of T, the model will be very nearly consistent. Maximum likelihood estimation of the parameters of a consistent n-gram language model is an interesting topic, but is beyond the scope of this paper. The vocabulary of English is very large and so, even for small values of n, the number of parameters in an n-gram model is enormous. The IBM Tangora speech recognition system has a vocabulary of about 20,000 words and employs a 3-gram language model with over eight trillion parameters (Averbuch et al. 1987). We can illustrate the problems attendant to parameter estimation for a 3-gram language model with the data in Table 1. Here, we show the number of 1-, 2-, and 3-grams appearing with various frequencies in a sample of 365,893,263 words of English text from a variety of sources. The vocabulary consists of the 260,740 different words plus a special Number of n-grams with various frequencies in 365,893,263 words of running text. unknown word into which all other words are mapped. Of the 6.799 x 1010 2-grams that might have occurred in the data, only 14,494,217 actually did occur and of these, 8,045,024 occurred only once each. Similarly, of the 1.773 x 1016 3-grams that might have occurred, only 75,349,888 actually did occur and of these, 53,737,350 occurred only once each. From these data and Turing's formula (Good 1953), we can expect that maximum likelihood estimates will be 0 for 14.7 percent of the 3-grams and for 2.2 percent of the 2-grams in a new sample of English text. We can be confident that any 3-gram that does not appear in our sample is, in fact, rare, but there are so many of them that their aggregate probability is substantial. As n increases, the accuracy of an n-gram model increases, but the reliability of our parameter estimates, drawn as they must be from a limited training text, decreases. Jelinek and Mercer (1980) describe a technique called interpolated estimation that combines the estimates of several language models so as to use the estimates of the more accurate models where they are reliable and, where they are unreliable, to fall back on the more reliable estimates of less accurate models. If Pri (w I &I.-1) is the conditional probability as determined by the jth language model, then the interpolated estimate, Pr(wi I w'i-1), is given by Given values for Pr(i) 0, the A1(w) are chosen, with the help of the EM algorithm, so as to maximize the probability of some additional sample of text called the held-out data (Baum 1972; Dempster, Laird, and Rubin 1977; Jelinek and Mercer 1980). When we use interpolated estimation to combine the estimates from 1-, 2-, and 3-gram models, we choose the As to depend on the history, w1, only through the count of the 2gram, We expect that where the count of the 2-gram is high, the 3-gram estimates will be reliable, and, where the count is low, the estimates will be unreliable. We have constructed an interpolated 3-gram model in which we have divided the As into 1,782 different sets according to the 2-gram counts. We estimated these As from a held-out sample of 4,630,934 words. We measure the performance of our model on the Brown corpus, which contains a variety of English text and is not included in either our training or held-out data (Kiera and Francis 1967). The Brown corpus contains 1,014,312 words and has a perplexity of 244 with respect to our interpolated model. Clearly, some words are similar to other words in their meaning and syntactic function. We would not be surprised to learn that the probability distribution of words in the vicinity of Thursday is very much like that for words in the vicinity of Friday. Of Peter F. Brown and Vincent J. Della Pietra Class-Based n-gram Models of Natural Language course, they will not be identical: we rarely hear someone say Thank God it's Thursday! or worry about Thursday the 13th. If we can successfully assign words to classes, it may be possible to make more reasonable predictions for histories that we have not previously seen by assuming that they are similar to other histories that we have seen. Suppose that we partition a vocabulary of V words into C classes using a function, 7r, which maps a word, wi, into its class, ci. We say that a language model is an ngram class model if it is an n-gram language model and if, in addition, for 1 < k < n, independent parameters: V - C of the form Pr (w j c,), plus the C&quot; - 1 independent parameters of an n-gram language model for a vocabulary of size C. Thus, except in the trivial cases in which C --= V or n 1, an n-gram class language model always has fewer independent parameters than a general n-gram language model. Given training text, tr, the maximum likelihood estimates of the parameters of a 1-gram class model are where by C(c) we mean the number of words in tf for which the class is c. From these equations, we see that, since c = r(w), Pr (w) = Pr (w I c) Pr (c) = C(w)/T. For a 1-gram class model, the choice of the mapping it has no effect. For a 2-gram class model, the sequential maximum likelihood estimates of the order-2 parameters maximize Pr (tII ti) or, equivalently, log Pr(tr I t1) and are given by By definition, Pr (ci c2) = Pr (ci) Pr (c2 I ci), and so, for sequential maximum likelihood estimation, we have Since C(ci ) and Ec c(cio are the numbers of words for which the class is ci in the strings ti. and tiT-1 respectively, the final term in this equation tends to 1 as T tends to infinity. Thus, Pr (ci c2) tends to the relative frequency of ci c2 as consecutive classes in the training text. Therefore, since Ew c(ww2)/(T— 1) tends to the relative frequency of w2 in the training text, and hence to Pr (w2), we must have, in the limit, where H(w) is the entropy of the 1-gram word distribution and /(ci , c2) is the average mutual information of adjacent classes. Because L(7r) depends on 7r only through this average mutual information, the partition that maximizes L(7r) is, in the limit, also the partition that maximizes the average mutual information of adjacent classes. We know of no practical method for finding one of the partitions that maximize the average mutual information. Indeed, given such a partition, we know of no practical method for demonstrating that it does, in fact, maximize the average mutual information. We have, however, obtained interesting results using a greedy algorithm. Initially, we assign each word to a distinct class and compute the average mutual information between adjacent classes. We then merge that pair of classes for which the loss in average mutual information is least. After V — C of these merges, C classes remain. Often, we find that for classes obtained in this way the average mutual information can be made larger by moving some words from one class to another. Therefore, after having derived a set of classes from successive merges, we cycle through the vocabulary moving each word to the class for which the resulting partition has the greatest average mutual information. Eventually no potential reassignment of a word leads to a partition with greater average mutual information. At this point, we stop. It may be possible to find a partition with higher average mutual information by simultaneously reassigning two or more words, but we regard such a search as too costly to be feasible. To make even this suboptimal algorithm practical one must exercise a certain care in implementation. There are approximately (V-02/2 merges that we must investigate to carry out the ith step. The average mutual information remaining after any one of them is the sum of (V — 02 terms, each of which involves a logarithm. Since altogether we must make V — C merges, this straightforward approach to the computation is of order V5. We cannot seriously contemplate such a calculation except for very small values of V. A more frugal organization of the computation must take advantage of the redundancy in this straightforward calculation. As we shall see, we can make the computation of the average mutual information remaining after a merge in constant time, independent of V. Suppose that we have already made V —k merges, resulting in classes Ck(1), Ck (2), , Ck (k) and that we now wish to investigate the merge of Ck (i) with Ck (j for 1 < i <j < k. Let pk(1, m) -= Pr (Ck (0, Ck(m)), i.e., the probability that a word in class Ck (m) follows a word in class Ck(1). Let and let The average mutual information remaining after V — k merges is We use the notation i+ j to represent the cluster obtained by merging Ck(i) and Ck(i)• If we know Ik. SO), and sk(j), then the majority of the time involved in computing Ik(i,j) is devoted to computing the sums on the second line of equation (15). Each of these sums has approximately V - k terms and so we have reduced the problem of evaluating Ik(i,j) from one of order V2 to one of order V. We can improve this further by keeping track of those pairs 1,m for which pk(1,m) is different from 0. We recall from Table 1, for example, that of the 6.799 x 1010 2-grams that might have occurred in the training data, only 14,494,217 actually did occur. Thus, in this case, the sums required in equation (15) have, on average, only about 56 non-zero terms instead of 260,741, as we might expect from the size of the vocabulary By examining all pairs, we can find that pair, i <j, for which the loss in average mutual information, Lk (i, j) - Ik(i, j), is least. We complete the step by merging Ck(i) and Ck(j) to form a new cluster Ck_i (i). If j k, we rename Ck(k) as Ck_i (i) and for 1 i,j, we set Ck-i (1) to Ck(/). Obviously, Ik-i = Ik(i,j). The values of Pk-1, prk_i, and qk_...1 can be obtained easily from Pk, plk, prk, and qk. If 1 and m both denote indices neither of which is equal to either i or j, then it is easy to establish that Finally, we must evaluate sk_1(i) and Lk_1(/, i) from equations 15 and 16. Thus, the entire update process requires something on the order of V2 computations in the course of which we will determine the next pair of clusters to merge. The algorithm, then, is of order V3. Although we have described this algorithm as one for finding clusters, we actually determine much more. If we continue the algorithm for V - 1 merges, then we will have a single cluster which, of course, will be the entire vocabulary. The order in which clusters are merged, however, determines a binary tree the root of which corresponds reps representatives representative rep Sample subtrees from a 1,000-word mutual information tree. to this single cluster and the leaves of which correspond to the words in the vocabulary. Intermediate nodes of the tree correspond to groupings of words intermediate between single words and the entire vocabulary. Words that are statistically similar with respect to their immediate neighbors in running text will be close together in the tree. We have applied this tree-building algorithm to vocabularies of up to 5,000 words. Figure 2 shows some of the substructures in a tree constructed in this manner for the 1,000 most frequent words in a collection of office correspondence. Beyond 5,000 words this algorithm also fails of practicality. To obtain clusters for larger vocabularies, we proceed as follows. We arrange the words in the vocabulary in order of frequency with the most frequent words first and assign each of the first C words to its own, distinct class. At the first step of the algorithm, we assign the (C + 1)st most probable word to a new class and merge that pair among the resulting C + 1 classes for which the loss in average mutual information is least. At the kth step of the algorithm, we assign the (C + k)th most probable word to a new class. This restores the number of classes to C + 1, and we again merge that pair for which the loss in average mutual information is least. After V — C steps, each of the words in the vocabulary will have been assigned to one of C classes. We have used this algorithm to divide the 260,741-word vocabulary of Table 1 into 1,000 classes. Table 2 contains examples of classes that we find particularly interesting. Table 3 contains examples that were selected at random. Each of the lines in the tables contains members of a different class. The average class has 260 words and so to make the table manageable, we include only words that occur at least ten times and we include no more than the ten most frequent words of any class (the other two months would appear with the class of months if we extended this limit to twelve). The degree to which the classes capture both syntactic and semantic aspects of English is quite surprising given that they were constructed from nothing more than counts of bigrams. The class {that tha theat} is interesting because although tha and theat are not English words, the computer has discovered that in our data each of them is most often a mistyped that. Table 4 shows the number of class 1-, 2-, and 3-grams occurring in the text with various frequencies. We can expect from these data that maximum likelihood estimates will assign a probability of 0 to about 3.8 percent of the class 3-grams and to about .02 percent of the class 2-grams in a new sample of English text. This is a substantial improvement over the corresponding numbers for a 3-gram language model, which are 14.7 percent for word 3-grams and 2.2 percent for word 2-grams, but we have achieved this at the expense of precision in the model. With a class model, we distinguish between two different words of the same class only according to their relative frequencies in the text as a whole. Looking at the classes in Tables 2 and 3, we feel that this is reasonable for pairs like John and George or liberal and conservative but perhaps less so for pairs like little and prima or Minister and mover. We used these classes to construct an interpolated 3-gram class model using the same training text and held-out data as we used for the word-based language model we discussed above. We measured the perplexity of the Brown corpus with respect to this model and found it to be 271. We then interpolated the class-based estimators with the word-based estimators and found the perplexity of the test data to be 236, which is a small improvement over the perplexity of 244 we obtained with the word-based model. In the previous section, we discussed some methods for grouping words together according to the statistical similarity of their surroundings. Here, we discuss two additional types of relations between words that can be discovered by examining various co-occurrence statistics. The mutual information of the pair w1 and w2 as adjacent words is If w2 follows wi less often than we would expect on the basis of their independent frequencies, then the mutual information is negative. If w2 follows wi more often than we would expect, then the mutual information is positive. We say that the pair w1 w2 is sticky if the mutual information for the pair is substantially greater than 0. In Table 5, we list the 20 stickiest pairs of words found in a 59,537,595-word sample of text from the Canadian parliament. The mutual information for each pair is given in bits, which corresponds to using 2 as the base of the logarithm in equation 18. Most of the pairs are proper names such as Pontius Pilate or foreign phrases that have been adopted into English such as mutatis mutandis and avant garde. The mutual information for Hum pty Dumpty, 22.5 bits, means that the pair occurs roughly 6,000,000 times more than one would expect from the individual frequencies of Hum pty and Dumpty. Notice that the property of being a sticky pair is not symmetric and so, while Hum pty Dumpty forms a sticky pair, Dumpty Hum pty does not. Instead of seeking pairs of words that occur next to one another more than we would expect, we can seek pairs of words that simply occur near one another more than we would expect. We avoid finding sticky pairs again by not considering pairs of words that occur too close to one another. To be precise, let Prnear (w1 w2) be the probability that a word chosen at random from the text is w1 and that a second word, chosen at random from a window of 1,001 words centered on wi but excluding the words in a window of 5 centered on w1, is w2. We say that w1 and w2 are semantically sticky if Prnear (W1W2) is much larger than Pr (w1) Pr (w2) . Unlike stickiness, semantic stickiness is symmetric so that if w1 sticks semantically to w2, then w2 sticks semantically to w1. In Table 6, we show some interesting classes that we constructed, using Prnear (w1 w2), in a manner similar to that described in the preceding section. Some classes group together words having the same morphological stem, such as performance, performed, perform, performs, and performing. Other classes contain words that are semantically related but have different stems, such as attorney, counsel, trial, court, and judge. We have described several methods here that we feel clearly demonstrate the value of simple statistical techniques as allies in the struggle to tease from words their linguistic secrets. However, we have not as yet demonstrated the full value of the secrets thus gleaned. At the expense of a slightly greater perplexity, the 3-gram model with word classes requires only about one-third as much storage as the 3-gram language model in which each word is treated as a unique individual (see Tables 1 and 4). Even when we combine the two models, we are not able to achieve much improvement in the perplexity. Nonetheless, we are confident that we will eventually be able to make significant improvements to 3-gram language models with the help of classes of the kind that we have described here. The authors would like to thank John Lafferty for his assistance in constructing word classes described in this paper.    \n",
              "1  We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets. These FSAs are good representations of paraphrases. They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets. Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations. In the past, paraphrases have come under the scrutiny of many research communities. Information retrieval researchers have used paraphrasing techniques for query reformulation in order to increase the recall of information retrieval engines (Sparck Jones and Tait, 1984). Natural language generation researchers have used paraphrasing to increase the expressive power of generation systems (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999). And researchers in multi-document text summarization (Barzilay et al., 1999), information extraction (Shinyama et al., 2002), and question answering (Lin and Pantel, 2001; Hermjakob et al., 2002) have focused on identifying and exploiting paraphrases in the context of recognizing redundancies, alternative formulations of the same meaning, and improving the performance of question answering systems. In previous work (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Shinyama et al., 2002), paraphrases are represented as sets or pairs of semantically equivalent words, phrases, and patterns. Although this is adequate in the context of some applications, it is clearly too weak from a generative perspective. Assume, for example, that we know that text pairs (stock market rose, stock market gained) and (stock market rose, stock prices rose) have the same meaning. If we memorized only these two pairs, it would be impossible to infer that, in fact, consistent with our intuition, any of the following sets of phrases are also semantically equivalent: {stock market rose, stock market gained, stock prices rose, stock prices gained } and {stock market, stock prices } in the context of rose or gained; {market rose }, {market gained }, {prices rose } and {prices gained } in the context of stock; and so on. In this paper, we propose solutions for two problems: the problem ofparaphrase representation and the problem of paraphrase induction. We propose a new, finite-statebased representation of paraphrases that enables one to encode compactly large numbers of paraphrases. We also propose algorithms that automatically derive such representations from inputs that are now routinely released in conjunction with large scale machine translation evaluations (DARPA, 2002): multiple English translations of many foreign language texts. For instance, when given as input the 11 semantically equivalent English translations in Figure 1, our algorithm automatically induces the FSA in Figure 2, which represents compactly 49 distinct renderings of the same semantic meaning. Our FSAs capture both lexical paraphrases, such as {fighting, battle}, {died, were killed} and structural paraphrases such as {last week’s fighting, the battle of last week}. The contexts in which these are correct paraphrases are also conveniently captured in the representation. In previous work, Langkilde and Knight (1998) used word lattices for language generation, but their method involved hand-crafted rules. Bangalore et al. (2001) and Barzilay and Lee (2002) both applied the technique of multi-sequence alignment (MSA) to align parallel corpora and produced similar FSAs. For their purposes, they mainly need to ensure the correctness of consensus among different translations, so that different constituent orderings in input sentences do not pose a serious problem. In contrast, we want to ensure the correctness of all paths represented by the FSAs, and direct application of MSA in the presence of different constituent orderings can be problematic. For example, when given as input the same sentences in Figure 1, one instantiation of the MSA algorithm produces the FSA in Figure 3, which contains many “bad” paths such as the battle of last week’s fighting took at least 12 people lost their people died in the fighting last week’s fighting (See Section 4.2.2 for a more quantitative analysis.). It’s still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)). But we chose to approach this problem from another direction. As a result, we propose a new syntax-based algorithm to produce FSAs. In this paper, we first introduce the multiple translation corpus that we use in our experiments (see Section 2). We then present the algorithms that we developed to induce finite-state paraphrase representations from such data (see Section 3). An important part of the paper is dedicated to evaluating the quality of the finite-state representations that we derive (see Section 4). Since our representations encode thousands and sometimes millions of equivalent verbalizations of the same meaning, we use both manual and automatic evaluation techniques. Some of the automatic evaluations we perform are novel as well. The data we use in this work is the LDC-available Multiple-Translation Chinese (MTC) Corpus1 developed for machine translation evaluation, which contains 105 news stories (993 sentences) from three sources of journalistic Mandarin Chinese text. These stories were independently translated into English by 11 translation agencies. Each sentence group, which consists of 11 semantically equivalent translations, is a rich source for learning lexical and structural paraphrases. In our experiments, we use 899 of the sentence groups — the sentence groups with sentences longer than 45 words were dropped. Our syntax-based alignment algorithm, whose pseudocode is shown in Figure 4, works in three steps. In the first step (lines 1-5 in Figure 4), we parse every sentence in a sentence group and merge all resulting parse trees into a parse forest. In the second step (line 6), we extract an FSA from the parse forest and then we compact it further using a limited form of bottom-up alignment, which we call squeezing (line 7). In what follows, we describe each step in turn. Top-down merging. Given a sentence group, we pass each of the 11 sentences to Charniak’s (2000) parser to get 11 parse trees. The first step in the algorithm is to merge these parse trees into one parse-forest-like structure using a top-down process. Let’s consider a simple case in which the parse forest contains one single tree, Tree 1 in Figure 5, and we are adding Tree 2 to it. Since the two trees correspond to sentences that have the same meaning and since both trees expand an S node into an NP and a V P, it is reasonable to assume that NP1 is a paraphrase of NP2 and V P1 is a paraphrase of V P2. We merge NP1 with NP2 and V P1 with V P2 and continue the merging process on each of the subtrees recursively, until we either reach the leaves of the trees or the two nodes that we examine are expanded using different syntactic rules. When we apply this process to the trees in Figure 5, the NP nodes are merged all the way down to the leaves, and we get “12” as a paraphrase of “twelve” and “people” as a paraphrase of “persons”; in contrast, the two VPs are expanded in different ways, so no merging is done beyond this level, and we are left with the information that “were killed” is a paraphrase of “died”. We repeat this top-down merging procedure with each of the 11 parse trees in a sentence group. So far, only constituents with same syntactic type are treated as paraphrases. However, later we shall see that we can match word spans whose syntactic types differ. Keyword checking. The matching process described above appears quite strict – the expansions must match exactly for two nodes to be merged. But consider the following parse trees: 1. (S (NP1 people)(VP1 were killed in this battle)) 2. (S (NP2 this battle)(VP2 killed people)) If we applied the algorithm described above, we would mistakenly align NP1 with NP2 and V P1 with V P2 — the algorithm described so far makes no use of lexical information. To prevent such erroneous alignments, we also implement a simple keyword checking procedure. We note that since the word “battle” appears in both V P1 and NP2, this can serve as an evidence against the merging of (NP1, NP2) and (V P1, V P2). A similar argument can be constructed for the word “people”. So in this example we actually have double evidence against merging; in general, one such clue suffices to stop the merging. Our keyword checking procedure acts as a filter. A list of keywords is maintained for each node in a syntactic tree. This list contains all the nouns, verbs, and adjectives that are spanned by a syntactic node. Before merging two nodes, we check to see whether the keyword lists associated with them share words with other nodes. That is, supposed we just merged nodes A and B, and they are expanded with the same syntactic rule into A1A2...An and B1B2...Bn respectively; before we merge each Ai with Bi, we check for each Bi if its keyword list shares common words with any Aj (j =� i). If they do not, we continue the top-down merging process; otherwise we stop. In our current implementation, a pair of synonyms can not stop an otherwise legitimate merging, but it’s possible to extend our keyword checking process with the help of lexical resources such as WordNet in future work. Mapping Parse Forests into Finite State Automata. The process of mapping Parse Forests into Finite State Automata is simple. We simply traverse the parse forest top-down and create alternative paths for every merged node. For example, the parse forest in Figure 5 is mapped into the FSA shown at the bottom of the same figure. In the FSA, there is a word associated with each edge. Different paths between any two nodes are assumed to be paraphrases of each other. Each path that starts from the BEGIN node and ends at the END node corresponds to either an original input sentence or a paraphrase sentence. Squeezing. Since we adopted a very strict matching criterion in top-down merging, a small difference in the syntactic structure of two trees prevents some legitimate mergings from taking place. This behavior is also exacerbated by errors in syntactic parsing. Hence, for instance, three edges labeled detroit at the leftmost of the top FSA in Figure 6 were kept apart. To compensate for this effect, our algorithm implements an additional step, which we call squeezing. If two different edges that go into (or out of) the same node in an FSA are labeled with the same word, the nodes on the other end of the edges are merged. We apply this operation exhaustively over the FSAs produced by the top-down merging procedure. Figure 6 illustrates the effect of this operation: the FSA at the top of this figure is compressed into the more compact FSA shown at the bottom of it. Note that in addition to reducing the redundant edges, this also gives us paraphrases not available in the FSA before squeezing (e.g. {reduced to rubble, blasted to ground}). Therefore, the squeezing operation, which implements a limited form of lexically driven alignment similar to that exploited by MSA algorithms, leads to FSAs that have a larger number of paths The evaluation for our finite state representations and algorithm requires careful examination. Obviously, what counts as a good result largely depends on the application one has in mind. If we are extracting paraphrases for question-reformulation, it doesn’t really matter if we output a few syntactically incorrect paraphrases, as long as we produce a large number of semantically correct ones. If we want to use the FSA for MT evaluation (for example, comparing a sentence to be evaluated with the possible paths in FSA), we would want all paths to be relatively good (which we will focus on in this paper), while in some other applications, we may only care about the quality of the best path (not addressed in this paper). Section 4.1 concentrates on evaluating the paraphrase pairs that can be extracted from the FSAs built by our system, while Section 4.2 is dedicated to evaluating the FSAs directly. By construction, different paths between any two nodes in the FSA representations that we derive are paraphrases (in the context in which the nodes occur). To evaluate our algorithm, we extract paraphrases from our FSAs and ask human judges to evaluate their correctness. We compare the paraphrases we collect with paraphrases that are derivable from the same corpus using a cotraining-based paraphrase extraction algorithm (Barzilay and McKeown, 2001). To the best of our knowledge, this is the most relevant work to compare against since it aims at extracting paraphrase pairs from parallel corpus. Unlike our syntax-based algorithm which treats a sentence as a tree structure and uses this hierarchical structural information to guide the merging process, their algorithm treats a sentence as a sequence of phrases with surrounding contexts (no hierarchical structure involved) and cotrains classifiers to detect paraphrases and contexts for paraphrases. It would be interesting to compare the results from two algorithms so different from each other. For the purpose of this experiment, we randomly selected 300 paraphrase pairs (Ssyn) from the FSAs produced by our system. Since the co-training-based algorithm of Barzilay and McKeown (2001) takes parallel corpus as input, we created out of the MTC corpus 55 × 993 sentence pairs (Each equivalent translation set of cardinality 11 was mapped into (2) equivalent translation pairs.). Regina Barzilay kindly provided us the list of paraphrases extracted by their algorithm from this parallel corpus, from which we randomly selected another set of 300 paraphrases (Scotr). phrases produced by the syntax-based alignment (Ssyn) and co-training-based (Scotr) algorithms. The resulting 600 paraphrase pairs were mixed and presented in random order to four human judges. Each judge was asked to assess the correctness of 150 paraphrase pairs (75 pairs from each system) based on the context, i.e., the sentence group, from which the paraphrase pair was extracted. Judges were given three choices: “Correct”, for perfect paraphrases, “Partially correct”, for paraphrases in which there is only a partial overlap between the meaning of two paraphrases (e.g. while {saving set, aid package} is a correct paraphrase pair in the given context, {set, aide package} is considered partially correct), and “Incorrect”. The results of the evaluation are presented in Table 1. Although the four evaluators were judging four different sets, each clearly rated a higher percentage of the outputs produced by the syntax-based alignment algorithm as “Correct”. We should note that there are parameters specific to the co-training algorithm that we did not tune to work for this particular corpus. In addition, the cotraining algorithm recovered more paraphrase pairs: the syntax-based algorithm extracted 8666 pairs in total with 1051 of them extracted at least twice (i.e. more or less reliable), while the numbers for the co-training algorithm is 2934 out of a total of 16993 pairs. This means we are not comparing the accuracy on the same recall level. Aside from evaluating the correctness of the paraphrases, we are also interested in the degree of overlap between the paraphrase pairs discovered by the two algorithms so different from each other. We find that out of the 1051 paraphrase pairs that were extracted from more than one sentence group by the syntax-based algorithm, 62.3% were also extracted by the co-training algorithm; and out of the 2934 paraphrase pairs from the results of co-training algorithm, 33.4% were also extracted by the syntax-based algorithm. This shows that in spite of the very different cues the two different algorithms rely on, they do discover a lot of common pairs. In order to (roughly) estimate the recall (of lexical synonyms) of our algorithm, we use the synonymy relation in WordNet to extract all the synonym pairs present in our corpus. This extraction process yields the list of all WordNet-consistent synonym pairs that are present in our data. (Note that some of the pairs identified as synonyms by WordNet, like “follow/be”, are not really synonyms in the contexts defined in our data set, which may lead to artificial deflation of our recall estimate.) Once we have the list of WordNet-consistent paraphrases, we can check how many of them are recovered by our method. Table 2 gives the percentage of pairs recovered for each range of average sentence length (ASL) in the group. Not surprisingly, we get higher recall with shorter sentences, since long sentences tend to differ in their syntactic structures fairly high up in the parse trees, which leads to fewer mergings at the lexical level. The recall on the task of extracting lexical synonyms, as defined by WordNet, is not high. But after all, this is not what our algorithm has been designed for. It’s worth noticing that the syntax-based algorithm also picks up many paraphrases that are not identified as synonyms in WordNet. Out of 3217 lexical paraphrases that are learned by our system, only 493 (15.3%) are WordNet synonyms, which suggests that paraphrasing is a much richer and looser relation than synonymy. However, the WordNetbased recall figures suggest that WordNet can be used as an additional source of information to be exploited by our algorithm. We noted before that apart from being a natural representation of paraphrases, the FSAs that we build have their own merit and deserve to be evaluated directly. Since our FSAs contain large numbers of paths, we design automatic evaluation metrics to assess their qualities. If we take our claims seriously, each path in our FSAs that connects the start and end nodes should correspond to a well-formed sentence. We are interested in both quantity (how many sentences our automata are able to produce) and quality (how good these sentences are). To answer the first question, we simply count the number of paths produced by our FSAs. Table 3 gives the statistics on the number of paths produced by our FSAs, reported by the average length of sentences in the input sentence groups. For example, the sentence groups that have between 10 and 20 words produce, on average, automata that can yield 4468 alternative, semantically equivalent formulations. Note that if we always get the same degree of merging per word across all sentence groups, the number of paths would tend to increase with the sentence length. This is not the case here. Apparently we are getting less merging with longer sentences. But still, given 11 sentences, we are capable of generating hundreds, thousands, and in some cases even millions of sentences. Obviously, we should not get too happy with our ability to boost the number of equivalent meanings if they are incorrect. To assess the quality of the FSAs generated by our algorithm, we use a language model-based metric. We train a 4-gram model over one year of the Wall Street Journal using the CMU-Cambridge Statistical Language Modeling toolkit (v2). For each sentence group SG, we use this language model to estimate the average entropy of the 11 original sentences in that group (ent(SG)). We also compute the average entropy of all the sentences in the corresponding FSA built by our syntax-based algorithm (ent(FSA)). As the statistics in Table 4 show, there is little difference between the average entropy of the original sentences and the average entropy of the paraphrase sentences we produce. To better calibrate this result, we compare it with the average entropy of 6 corresponding machine translation outputs (ent(MTS)), which were also made available by LDC in conjunction with the same corpus. As one can see, the difference between the average entropy of the machine produced output and the average entropy of the original 11 sentences is much higher than the difference between the average entropy of the FSA-produced outputs and the average entropy of the original 11 sentences. Obviously, this does not mean that our FSAs only produce well-formed sentences. But it does mean that our FSAs produce sentences that look more like human produced sentences than machine produced ones according to a language model. Not surprisingly, the language model we used in Section 4.2.1 is far from being a perfect judge of sentence quality. Recall the example of “bad” path we gave in Section 1: the battle of last week’s fighting took at least 12 people lost their people died in the fighting last week’s fighting. Our 4-gram based language model will not find any fault with this sentence. Notice, however, that some words (such as “fighting” and “people”) appear at least twice in this path, although they are not repeated in any of the source sentences. These erroneous repetitions indicate mis-alignment. By measuring the frequency of words that are mistakenly repeated, we can now examine quantitatively whether a direct application of the MSA algorithm suffers from different constituent orderings as we expected. For each sentence group, we get a list of words that never appear more than once in any sentence in this group. Given a word from this list and the FSA built from this group, we count the total number of paths that contain this word (C) and the number of paths in which this word appears at least twice (CT, i.e. number of erroneous repetitions). We define the repetition ratio to be CT/C, which is the proportion of “bad” paths in this FSA according to this word. If we compute this ratio for all the words in the lists of the first 499 groups2 and the corresponding FSAs produced by an instantiation of the MSA algorithm3, the average repetition ratio is 0.0304992 (14.76% of the words have a non-zero repetition ratio, and the average ratio for these words is 0.206671). In comparison, the average repetition ratio for our algorithm is 0.0035074 (2.16% of the words have a non-zero repetition ratio4, and the average ratio for these words is 0.162309). The presence of different constituent orderings does pose a more serious problem to the MSA algorithm. Recently, Papineni et al. (2002) have proposed an automatic MT system evaluation technique (the BLEU score). Given an MT system output and a set of reference translations, one can estimate the “goodness” of the MT output by measuring the n-gram overlap between the output and the reference set. The higher the overlap, i.e., the closer an output string is to a set of reference translations, the better a translation it is. We hypothesize that our FSAs provide a better representation against which the outputs of MT systems can be evaluated because they encode not just a few but thousands of equivalent semantic formulations of the desired meaning. Ideally, if the FSAs we build accept all and only the correct renderings of a given meaning, we can just give a test sentence to the reference FSA and see if it is accepted by it. Since this is not a realistic expectation, we measure the edit distance between a string and an FSA instead: the smaller this distance is, the closer it is to the meaning represented by the FSA. To assess whether our FSAs are more appropriate representations for evaluating the output of MT systems, we perform the following experiment. For each sentence group, we hold out one sentence as test sentence, and try to evaluate how much of it can be predicted from the other 10 sentences. We compare two different ways of estimating the predictive power. (a) we compute the edit distance between the test sentence and the other 10 sentences in the set. The minimum of this distance is ed(input). (b) we use dynamic programming to efficiently compute the minimum distance (ed(FSA)) between the test sentence and all the paths in the FSA built from the other 10 sentences. The smaller the edit distance is, the better we are predicting a test sentence. Mathematically, the difference between these two measures ed(input) − ed(FSA) characterizes how much is gained in predictive power by building the FSA. We carry out the experiment described above in a “leave-one-out” fashion (i.e. each sentence serves as a test sentence once). Now let edgain be the average of ed(input) − ed(FSA) over the 11 runs for a given group. We compute this for all 899 groups and find the mean for edgain to be 0.91 (std. dev = 0.78). Table 5 gives the count for groups whose edgain falls into the specified range. We can see that the majority of edgain falls under 2. We are also interested in the relation between the predictive power of the FSAs and the number of reference translations they are derived from. For a given group, we randomly order the sentences in it, set the last one as the test sentence, and try to predict it with the first 1, 2, 3, ... 10 sentences. We investigate whether more sentences Let ed(FSAn) be the edit distance from the test sentence to the FSA built on the first n sentences; similarly, let ed(inputn) be the minimum edit distance from the test sentence to an input set that consists of only the first n sentences. Table 6 reports the effect of using different number of reference translations. The first column shows that each translation is contributing to the predictive power of our FSA. Even when we add the tenth translation to our FSA, we still improve its predictive power. The second column shows that the more sentences we add to the FSA the larger the difference between its predictive power and that of a simple set. The results in Table 6 suggest that our FSA may be used in order to refine the BLEU metric (Papineni et al., 2002). In this paper, we presented a new syntax-based algorithm that learns paraphrases from a newly available dataset. The multiple translation corpus that we use in this paper is the first instance in a series of similar corpora that are built and made publicly available by LDC in the context of a series of DARPA-sponsored MT evaluations. The algorithm we proposed constructs finite state representations of paraphrases that are useful in many contexts: to induce large lists of lexical and structural paraphrases; to generate semantically equivalent renderings of a given meaning; and to estimate the quality of machine translation systems. More experiments need to be carried out in order to assess extrinsically whether the FSAs we produce can be used to yield higher agreement scores between human and automatic assessments of translation quality. In our future work, we wish to experiment with more flexible merging algorithms and to integrate better the top-down and bottom-up processes that are used to induce FSAs. We also wish to extract more abstract paraphrase patterns from the current representation. Such patterns are more likely to get reused – which would help us get reliable statistics for them in the extraction phase, and also have a better chance of being applicable to unseen data. We thank Hal Daum´e III, Ulrich Germann, and Ulf Hermjakob for help and discussions; Eric Breck, Hubert Chen, Stephen Chong, Dan Kifer, and Kevin O’Neill for participating in the human evaluation; and the Cornell NLP group and the reviewers for their comments on this paper. We especially want to thank Regina Barzilay and Lillian Lee for many valuable suggestions and help at various stages of this work. Portions of this work were done while the first author was visiting Information Sciences Institute. This work was supported by the Advanced Research and Development Activity (ARDA)’s Advance Question Answering for Intelligence (AQUAINT) Program under contract number MDA908-02-C-0007, the National Science Foundation under ITR/IM grant IIS0081334 and a Sloan Research Fellowship to Lillian Lee. Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the National Science Foundation or the Sloan Foundation.    \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  We describe a procedure for arranging into a time-line the contents of news stories describing the development of some situation. We describe the parts of the system that deal with 1. breaking sentences into event-clauses and 2. resolving both explicit and implicit temporal references. Evaluations show a performance of 52%, compared to humans. Linguists who have analyzed news stories (Schokkenbroek,1999; Bell,1997; Ohtsuka and Brewer,1992, etc.) noticed that “narratives1 are about more than one event and these events are temporally ordered. Though it seems most logical to recapitulate events in the order in which they happened, i.e. in chronological order, the events are often presented in a different sequence”. The same paper states that “it is important to reconstruct the underlying event order2 for narrative analysis to assign meaning to the sequence in which the events are narrated at the level of discourse structure....If the underlying event structure cannot be reconstructed, it may well be impossible to understand the narrative at all, let alone assign meaning to its structure”. Several psycholinguistic experiments show the influence of event-arrangement in news stories on the ease of comprehension by readers. Duszak (1991) had readers reconstruct a news story from the randomized sentences. According to his experiments readers have a default strategy by which—in the absence of cues to the contrary—they re-impose chronological order on events in the discourse. The problem of reconstructing the chronological order of events becomes more complicated if we have to deal with separate news stories, written at different times and describing the development of some situation, as is the case for multidocument summarization. By judicious definition, one can make this problem easy or hard. Selecting only specific items to assign time-points to, and then measuring correctness on them alone, may give high performance but leave much of the text unassigned. We address the problem of assigning a time-point to every clause in the text. Our approach is to break the news stories into their constituent events and to assign timestamps—either time-points or time-intervals—to these events. When assigning time-stamps we analyze both implicit time references (mainly through the tense system) and explicit ones (temporal adverbials) such as ‘on Monday’, ‘in 1998’, etc. The result of the work is a prototype program which takes as input set of news stories broken into separate sentences and produces as output a text that combines all the events from all the articles, organized in chronological order. As data we used a set of news stories about an earthquake in Afghanistan that occurred at the end of May in 1998. These news stories were taken from CNN, ABC, and APW websites for the DUC-2000 meeting. The stories were all written within one week. Some of the texts were written on the same day. In addition to a description of the May earthquake, these texts contain references to another earthquake that occurred in the same region in February 1998. To divide sentences into event-clauses we use CONTEX (Hermjakob, 1997), a parser that produces a syntactic parse tree augmented with semantic labels. CONTEX uses machine learning techniques to induce a grammar from a given treebanks. A sample output of CONTEX is given in Appendix 1. To divide a sentence into event-clauses the parse tree output by CONTEX is analyzed from left to right (root to leaf). The ::CAT field for each node provides the necessary information about whether the node under consideration forms a part of its upper level event or whether it introduces a new event. ::CAT features that indicate new events are: S-CLAUSE, S-SNT, SSUB-CLAUSE, S-PART-CLAUSE, S-RELCLAUSE. These features mark clauses which contain both subject (one or several NPs) and predicate (VP containing one or several verbs). The above procedure classifies a clause containing more than one verb as a simple clause. Such clauses are treated as one event and only one time-point will be assigned to them. This is fine when the second verb is used in the same tense as the first, but may be wrong in some cases, as in He lives in this house now and will stay here for one more year. There are no such clauses in the analyzed data, so we ignore this complication for the present. The parse tree also gives information about the tense of verbs, used later for time assignment. In order to facilitate subsequent processing, we wish to rephrase relative clauses as full independent sentences. We therefore have to replace pronouns where it is possible by their antecedents. Very often the parser gives information about the referential antecedents (in the example below, Russia). Therefore we introduced the rule: if it is possible to identify the referent, put it into the event-clause: Here the antecedent for which is identified as the relief, and gives which <the relief> was costing lives instead of which <poor coordination> was costing lives. Fortunately, in most cases our rule works correctly. Although the event-identifier works reasonably well, breaking text into event-clauses needs further investigation. Table 1 shows the performance of the system. Two kinds of mistakes are made by the event identifier: those caused by CONTEX (it does not identify clauses with omitted predicate, etc.) and those caused by the fact that our clause identifier does too shallow analysis of the parse tree. According to (Bell, 1997) “time is expressed at different levels—in the morphology and syntax of the verb phrase, in time adverbials whether lexical or phrasal, and in the discourse structure of the stories above the sentence”. For the present work we use slightly modified time representations suggested in (Allen, 1991). Formats used for time representation: We use two anchoring time points: We require that the first sentence for each article contains time information. For example: The date information is in bold. We denote by Ti the reference time-point for the article, where i 3 YYYY—year number, DDD—absolute number of the day within the year (1–366), W-—umber of the day in a week (1- Monday, ... 7- Saturday). If it is impossible to point out the day of the week then W is assigned 0. means that it is the time point of article i. The symbol Ti is used as a comparative time-point if the time the article was written is unknown. The information in brackets gives the exact date the article was written, which is the main anchor point for the time-stamper. The information about hours, minutes and seconds is ignored for the present. 2. Last time point assigned in the same sentence While analyzing different event-clauses within the same sentence we keep track of what time-point was most recently assigned within this sentence. If needed, we can refer to this time-point. In case the most recent time information assigned is not a date but an interval we record information about both time boundaries. When the program proceeds to the next sentence, the variable for the most recently assigned date becomes undefined. In most cases this assumption works correctly (example 5.2–5.3): The last time interval assigned for sentence 5.2 is {1998:53:0}---{1998:71:0}, which gives an approximate range of days when the previous earthquake happened. But the information in sentence 5.3 is about the recent earthquake and not about the previous one of 3 months earlier, which is why it would be a mistake to point Monday and Tuesday within that range. Mani and Wilson (2000) point out “over half of the errors [made by his time-stamper] were due to propagation of spreading of an incorrect event time to neighboring events”. The rule of dropping the most recently assigned date as an anchor point when proceeding to the next sentence very often helps us to avoid this problem. There are however cases where dropping the most recent time as an anchor when proceeding to the next sentence causes errors: It is clear that sentence 4.9 is the continuation of sentence 4.8 and refers to the same time point (February earthquake). In this case our rule assigns the wrong time to 4.9.1. Still we retain this rule because it is more frequently correct than incorrect. First, the text divided into event-clauses is run through a program that extracts all the date-stamps (made available by Kevin Knight, ISI). In most cases this program does not miss any date-stamps and extracts only the correct ones. The only cases in which it did not work properly for the texts were: Here the modal verb MAY was assumed to be the month, given that it started with a capital letter. Tuberculosis is already common in the area where people live in close quarters and have poor hygiene here the noun quarters, which in this case is used in the sense immediate contact or close range (Merriam-Webster dictionary), was assumed to be used in the sense the fourth part of a measure of time (Merriam-Webster dictionary). After extracting all the date-phrases we proceed to time assignment. When assigning a time to an event, we select the time to be either the most recently assigned date or, if the value of the most recently assigned date is undefined, to the date of the article. We use a set of rules to perform this selection. These rules can be divided into two main categories: those that work for sentences containing explicit date information, and those that work for sentences that do not. If the day-of-the-week used in the eventclause is the same as that of the article (or the most recently assigned date, if it is defined), and there no words before it could signal that the described event happened earlier or will happen later, then the time-point of the article (or the most recently assigned date, if it is defined) is assigned to this event. If before or after a day-ofthe-week there is a word/words signaling that the event happened earlier of will happen later then the time-point is assigned in accordance with this signal-word and the most recently assigned date, if it is defined. If the day-of-the-week used in the eventclause is not the same as that of the article (or the most recently assigned date, if it is defined), then if there are words pointing out that the event happened before the article was written or the tense used in the clause is past, then the time for the event-clause is assigned in accordance with this word (such words we call signal-words), or the most recent day corresponding to the current day-of-the-week is chosen. If the signal-word points out that the event will happen after the article was written or the tense used in the clause is future, then the time for the event-clause is assigned in accordance with the signal word or the closest subsequent day corresponding to the current day-of-the-week. helicopters evacuated 50 of the most seriously injured to emergency medical centers. The time for article 5 is (06/06/1998:Tuesday 15:17:00). So, the time assigned to this eventclause is: 5.3.1 {1998:151:1}, {1998:152:2}. The rules are the same as for a day-of-theweek, but in this case a time-range is assigned to the event-clause. The left boundary of the range is the first day of the month, the right boundary is the last day of the month, and though it is possible to figure out the days of weeks for these boundaries, this aspect is ignored for the present. earthquake in the same region killed 2,300 people and left thousands of people homeless. The time for article 4 is (05/30/1998:Saturday 14:41:00). So, the time assigned to this eventclause is 4.8.1 {1998:32:0}---{1998:60:0}. In the analyzed corpus there is a case where the presence of a name of month leads to a wrong time-stamping: Because of February, a wrong time-interval is assigned to clause 6.3.3, namely {1998:32:0}--{1998:60:0}. As this event-clause is a description of the latest news as compared to some figures it should have the time-point of the article. Such cases present a good possibility for the use of machine learning techniques to disambiguate between the cases where we should take into account date-phrase information and where not. We might have date-stamps where the words weeks, days, months, years are used with modifiers. For example remote mountainous area rocked three months earlier by another massive quake 5.2.4 that <another massive quake> claimed some 2,300 victims. In event-clause 5.2.3 the expression three months earlier is used. It is clear that to get the time for the event it is not enough to subtract 3 months from the time of the article because the above expression gives an approximate range within which this event could happen and not a particular date. For such cases we invented the following rule: For event 5.2.3 the time range will be {1998:53:0)---{1998:71:0) (the exact date of the article is {1998:152:2}). If the modifier used with weeks, days, months or years is several, then the multiplier used in (1) is equal to 2. If an event-clause does not contain any datephrase but contains one of the words ‘when’, ‘since’, ‘after’, ‘before’, etc., it might mean that this clause refers to an event, the time of which can be used as a reference point for the event under analysis. In this case we ask the user to insert the time for this reference event manually. This rule can cause problems in cases where ‘after’ or ‘before’ are used not as temporal connectors but as spatial ones, though in the analyzed texts we did not face this problem. If the current event-clause refers to a timepoint in Present/Past Perfect tense, then an openended time-interval is assigned to this event. The starting point is unknown; the end-point is either the most recently assigned date or the time-point of the article. If the current event-clause contains a verb in future tense (one of the verbs ‘shall’, ‘will’, ‘should’, ‘would’, ‘might’ is present in the clause) then the open-ended time-interval assigned to this event-clause has the starting point at either the most recently assigned date or the date of the article. Other tenses that can be identified with the help of CONTEX are Present and Past Indefinite. In the analyzed data all the verbs in Present Indefinite are given the most recently assigned date (or the date of the article). The situation with Past Indefinite is much more complicated and requires further investigation of more data. News stories usually describe the events that already took place at some time in the past, which is why even if the day when the event happened is not over, past tense is very often used for the description (this is especially noticeable for US news of European, Asian, African and Australian events). This means that very often an event-clause containing a verb in Past Indefinite Tense can be assigned the most recently assigned date (or the date of the article). It might prove useful to use machine learned rules for such cases. If there is no verb in the event-clause then the most recently assigned date (or the date of the article) is assigned to the event-clause. We ran the time-stamper program on two types of data: list of event-clauses extracted by the event identifier and list of event-clauses created manually. Tables 2 and 3 show the results. In the former case we analyzed only the correctly identified clauses. One can see that even on manually created data the performance of the time-stamper is not 100%. Why? Some errors are caused by assigning the time based on the date-phrase present in the eventclause, when this date-phrase is not an adverbial time modifier but an attribute. For example, The third event describes the May 30 earthquake but the time interval given for this event is {1998:32:0}---{1998:60:0} (i.e., the event happened in February). It might be possible to use machine learned rules to correct such cases. One more significant source of errors is the writing style: When the reader sees early this morning he or she tends to assign to this clause the time of the article, but later as seeing looked for two days, realizes that the time of the clause containing early this morning is two days earlier than the time of the article. It seems that the errors caused by the writing style can hardly be avoided. If an event happened at some time-point but according to the information in the sentence we can assign only a time-interval to this event (for example, February Earthquake) then we say that the time-interval is assigned correctly if the necessary time-point is within this time-interval After stamping all the news stories from the analyzed set, we arrange the event-clauses from all the articles into a chronological order. After doing that we obtain a new set of event-clauses which can easily be divided into two subsets–the first one containing all the references to the February earthquake, the second one containing the list of event-clauses in chronological order, describing what happened in May. Such a text where all the events are organized in a chronological order might be very helpful in multidocument summarization, where it is important to include into the final summary not only the most important information but also the most recent one. The output of the presented system gives the information about the timeorder of the events described in several documents. Several linguistic and psycholinguistic studies deal with the problem of time-arrangement of different texts. The research presented in these studies highlights many problems but does not solve them. As for computational applications of time theories, most work was done on temporal expressions that appear in scheduling dialogues (Busemann et al., 1997; Alexandresson et al., 1997). There are many constraints on temporal expressions in this domain. The most relevant prior work is (Mani and Wilson, 2000), who implemented their system on news stories, introduced rules spreading time-stamps obtained with the help of explicit temporal expressions throughout the whole article, and invented machine learning rules for disambiguating between specific and generic use of temporal expressions (for example, whether Christmas is used to denote the 25th of December or to denote some period of time around the 25th of December). They also mention a problem of disambiguating between temporal expression and proper name, as in ‘USA Today’. Bell (1997) notices “more research is needed on the effects of time structure on news comprehension. The hypothesis that the noncanonical news format does adversely affect understanding is a reasonable one on the basis of comprehension research into other narrative genres, but the degree to which familiarity with news models may mitigate these problems is unclear”. This research can greatly improve the performance of time-stamper and might lead to a list of machine learning rules for time detection. In this paper we made an attempt to not just analyze and decode temporal expressions but to apply this analysis throughout the whole text and assign time-stamps to such type of clauses, which later could be used as separate sentences in various natural language applications, for example in multidocument summarization. text number of manually number of time point percentage of number created event-clauses correctly assigned to correct manually created clauses assignment target 1 7 6 85.71 target 2 27 20 74.07 target 3 5 4 80.00 target 4 28 26 92.85 target 5 33 30 90.91 target 6 58 37 63.79 Total 158 123 77.85    \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           and Abstracts for Nice Summaries, In Workon Automatic Philadelphia, PA, pp. 9-14. Edmundson, H. (1969). “New methods in automatic of the 16(2). Grefenstett, G. (1998). Producing intelligent telegraphic text reduction to provide an audio scanning serfor the blind. In Notes of the AIII Spring on Intelligent Text Summarization, In this paper we present Hedge Trimmer, a HEaDline GEneration system that creates a headline for a newspaper story by removing constituents from a parse tree of the first sentence until a length threshold has been reached. Linguistically-motivated heuristics guide the choice of which constituents of a story should be preserved, and which ones should be deleted. Our focus is on headline generation for English newspaper texts, with an eye toward the production of document surrogates—for cross-language information retrieval—and the eventual generation of readable headlines from speech broadcasts. In contrast to original newspaper headlines, which are often intended only to catch the eye, our approach produces informative abstracts describing the main theme or event of the newspaper article. We claim that the construction of informative abstracts requires access to deeper linguistic knowledge, in order to make substantial improvements over purely statistical approaches. In this paper, we present our technique for producing headlines using a parse-and-trim approach based on the BBN Parser. As described in Miller et al. (1998), the BBN parser builds augmented parse trees according to a process similar to that described in Collins (1997). The BBN parser has been used successfully for the task of information extraction in the SIFT system (Miller et al., 2000). The next section presents previous work in the area of automatic generation of abstracts. Following this, we present feasibility tests used to establish the validity of an approach that constructs headlines from words in a story, taken in order and focusing on the earlier part of the story. Next, we describe the application of the parse-and-trim approach to the problem of headline generation. We discuss the linguistically-motivated heuristics we use to produce results that are headlinelike. Finally, we evaluate Hedge Trimmer by comparing it to our earlier work on headline generation, a probabilistic model for automatic headline generation (Zajic et al, 2002). In this paper we will refer to this statistical system as HMM Hedge We demonstrate the effectiveness of our linguistically-motivated approach, Hedge Trimmer, over the probabilistic model, HMM Hedge, using both human evaluation and automatic metrics. Other researchers have investigated the topic of automatic generation of abstracts, but the focus has been different, e.g., sentence extraction (Edmundson, 1969; Johnson et al, 1993; Kupiec et al., 1995; Mann et al., 1992; Teufel and Moens, 1997; Zechner, 1995), processing of structured templates (Paice and Jones, 1993), sentence compression (Hori et al., 2002; Knight and Marcu, 2001; Grefenstette, 1998, Luhn, 1958), and generation of abstracts from multiple sources (Radev and McKeown, 1998). We focus instead on the construction of headline-style abstracts from a single story. Headline generation can be viewed as analogous to statistical machine translation, where a concise document is generated from a verbose one using a Noisy Channel Model and the Viterbi search to select the most likely summarization. This approach has been explored in (Zajic et al., 2002) and (Banko et al., 2000). The approach we use in Hedge is most similar to that of (Knight and Marcu, 2001), where a single sentence is shortened using statistical compression. As in this work, we select headline words from story words in the order that they appear in the story—in particular, the first sentence of the story. However, we use linguistically motivated heuristics for shortening the sentence; there is no statistical model, which means we do not require any prior training on a large corpus of story/headline pairs. Linguistically motivated heuristics have been used by (McKeown et al, 2002) to distinguish constituents of parse trees which can be removed without affecting grammaticality or correctness. GLEANS (Daumé et al, 2002) uses parsing and named entity tagging to fill values in headline templates. Consider the following excerpt from a news story: In this case, the words in bold form a fluent and accurate headline for the story. Italicized words are deleted based on information provided in a parse-tree representation of the sentence. Our approach is based on the selection of words from the original story, in the order that they appear in the story, and allowing for morphological variation. To determine the feasibility of our headline-generation approach, we first attempted to apply our “select-wordsin-order” technique by hand. We asked two subjects to write headline headlines for 73 AP stories from the TIPSTER corpus for January 1, 1989, by selecting words in order from the story. Of the 146 headlines, 2 did not meet the “select-words-in-order” criteria because of accidental word reordering. We found that at least one fluent and accurate headline meeting the criteria was created for each of the stories. The average length of the headlines was 10.76 words. Later we examined the distribution of the headline words among the sentences of the stories, i.e. how many came from the first sentence of a story, how many from the second sentence, etc. The results of this study are shown in Figure 1. We observe that 86.8% of the headline words were chosen from the first sentence of their stories. We performed a subsequent study in which two subjects created 100 headlines for 100 AP stories from August 6, 1990. 51.4% of the headline words in the second set were chosen from the first sentence. The distribution of headline words for the second set shown in Figure 2. Although humans do not always select headline words from the first sentence, we observe that a large percentage of headline words are often found in the first sentence. The input to Hedge is a story, whose first sentence is immediately passed through the BBN parser. The parse-tree result serves as input to a linguisticallymotivated module that selects story words to form headlines based on key insights gained from our observations of human-constructed headlines. That is, we conducted a human inspection of the 73 TIPSTER stories mentioned in Section 3 for the purpose of developing the Hedge Trimmer algorithm. Based on our observations of human-produced headlines, we developed the following algorithm for parse-tree trimming: More recently, we conducted an automatic analysis of the human-generated headlines that supports several of the insights gleaned from this initial study. We parsed 218 human-produced headlines using the BBN parser and analyzed the results. For this analysis, we used 72 headlines produced by a third participant.1 The parsing results included 957 noun phrases (NP) and 315 clauses (S). We calculated percentages based on headline-level, NP-level, and Sentence-level structures in the parsing results. That is, we counted: Figure 3 summarizes the results of this automatic analysis. In our initial human inspection, we considered each of these categories to be reasonable candidates for deletion in our parse tree and this automatic analysis indicates that we have made reasonable choices for deletion, with the possible exception of trailing PPs, which show up in over half of the human-generated headlines. This suggests that we should proceed with caution with respect to the deletion of trailing PPs; thus we consider this to be an option only if no other is available. preposed adjuncts = 0/218 (0%) conjoined S = 1/218 ( .5%) conjoined VP = 7/218 (3%) relative clauses = 3/957 (.3%) determiners = 31/957 (3%); of these, only 16 were “a” or “the” (1.6% overall) S-LEVEL PERCENTAGES2 time expressions = 5/315 (1.5%) trailing PPs = 165/315 (52%) trailing SBARs = 24/315 (8%) 1 No response was given for one of the 73 stories. 2 Trailing constituents (SBARs and PPs) are computed by counting the number of SBARs (or PPs) not designated as an argument of (contained in) a verb phrase. For a comparison, we conducted a second analysis in which we used the same parser on just the first sentence of each of the 73 stories. In this second analysis, the parsing results included 817 noun phrases (NP) and 316 clauses (S). A summary of these results is shown in Figure 4. Note that, across the board, the percentages are higher in this analysis than in the results shown in Figure 3 (ranging from 12% higher—in the case of trailing PPs—to 1500% higher in the case of time expressions), indicating that our choices of deletion in the Hedge Trimmer algorithm are well-grounded. preposed adjuncts = 2/73 (2.7%) conjoined S = 3/73 (4%) conjoined VP = 20/73 (27%) relative clauses = 29/817 (3.5%) determiners = 205/817 (25%); of these, only 171 were “a” or “the” (21% overall) time expressions = 77/316 (24%) trailing PPs = 184/316 (58%) trailing SBARs = 49/316 (16%) each story. The first step relies on what is referred to as the Projection Principle in linguistic theory (Chomsky, 1981): Predicates project a subject (both dominated by S) in the surface structure. Our human-generated headlines always conformed to this rule; thus, we adopted it as a constraint in our algorithm. An example of the application of step 1 above is the following, where boldfaced material from the parse tree representation is retained and italicized material is eliminated: with government]] officials said Tuesday.] Output of step 1: Rebels agree to talks with government. When the parser produces a correct tree, this step provides a grammatical headline. However, the parser often produces an incorrect output. Human inspection of our 624-sentence DUC-2003 evaluation set revealed that there were two such scenarios, illustrated by the following cases: In the first case, an S exists, but it does not conform to the requirements of step 1. This occurred in 2.6% of the sentences in the DUC-2003 evaluation data. We resolve this by selecting the lowest leftmost S, i.e., the entire string “What started as a local controversy has evolved into an international scandal” in the example above. In the second case, there is no S available. This occurred in 3.4% of the sentences in the evaluation data. We resolve this by selecting the root of the parse tree; this would be the entire string “Bangladesh and India signed a water sharing accord” above. No other parser errors were encountered in the DUC-2003 evaluation data. Step 2 of our algorithm eliminates low-content units. We start with the simplest low-content units: the determiners a and the. Other determiners were not considered for deletion because our analysis of the humanconstructed headlines revealed that most of the other determiners provide important information, e.g., negation (not), quantifiers (each, many, several), and deictics (this, that). Beyond these, we found that the human-generated headlines contained very few time expressions which, although certainly not content-free, do not contribute toward conveying the overall “who/what content” of the story. Since our goal is to provide an informative headline (i.e., the action and its participants), the identification and elimination of time expressions provided a significant boost in the performance of our automatic headline generator. We identified time expressions in the stories using BBN’s IdentiFinderTM (Bikel et al, 1999). We implemented the elimination of time expressions as a twostep process: where X is tagged as part of a time expression The following examples illustrate the application of this step: Output of step 2: State Department lifted ban it has imposed on foreign fliers. Output of step 2: International relief agency announced that it is withdrawing from North Korea. We found that 53.2% of the stories we examined contained at least one time expression which could be deleted. Human inspection of the 50 deleted time expressions showed that 38 were desirable deletions, 10 were locally undesirable because they introduced an ungrammatical fragment,3 and 2 were undesirable because they removed a potentially relevant constituent. However, even an undesirable deletion often pans out for two reasons: (1) the ungrammatical fragment is frequently deleted later by some other rule; and (2) every time a constituent is removed it makes room under the threshold for some other, possibly more relevant constituent. Consider the following examples. Example (7) was produced by a system which did not remove time expressions. Example (8) shows that if the time expression Sunday were removed, it would make room below the 10-word threshold for another important piece of information. The final step, iterative shortening, removes linguistically peripheral material—through successive deletions—until the sentence is shorter than a given threshold. We took the threshold to be 10 for the DUC task, but it is a configurable parameter. Also, given that the human-generated headlines tended to retain earlier material more often than later material, much of our iterative shortening is focused on deleting the rightmost phrasal categories until the length is below threshold. There are four types of iterative shortening rules. The first type is a rule we call “XP-over-XP,” which is implemented as follows: In constructions of the form [XP [XP ...] ...] remove the other children of the higher XP, where XP is NP, VP or S. This is a linguistic generalization that allowed us apply a single rule to capture three different phenomena (relative clauses, verb-phrase conjunction, and sentential conjunction). The rule is applied iteratively, from the deepest rightmost applicable node backwards, until the length threshold is reached. The impact of XP-over-XP can be seen in these examples of NP-over-NP (relative clauses), VP-over-VP (verb-phrase conjunction), and S-over-S (sentential conjunction), respectively: Parse: [S [Det A] fire killed [Det a] [NP [NP firefighter] [SBAR who was fatally injured as he searched the house] ]] Output of NP-over-NP: fire killed firefighter has outpaced state laws, but the state says the company doesn’t have the proper licenses. Parse: [S [Det A] company offering blood cholesterol tests in grocery stores says [S [S medical technology has outpaced state laws], [CC but] [S [Det the] state stays [Det the] company doesn’t have [Det the] proper licenses.]] ] Output of S-over-S: Company offering blood cholesterol tests in grocery store says medical technology has outpaced state laws The second type of iterative shortening is the removal of preposed adjuncts. The motivation for this type of shortening is that all of the human-generated headlines ignored what we refer to as the preamble of the story. Assuming the Projection principle has been satisfied, the preamble is viewed as the phrasal material occurring before the subject of the sentence. Thus, adjuncts are identified linguistically as any XP unit preceding the first NP (the subject) under the S chosen by step 1. This type of phrasal modifier is invisible to the XP-over-XP rule, which deletes material under a node only if it dominates another node of the same phrasal category. The impact of this type of shortening can be seen in the following example: Parse: [S [PP According to a now-finalized blueprint described by U.S. officials and other sources] [Det the] Bush administration plans to take complete, unilateral control of [Det a] postSaddam Hussein Iraq ] Output of Preposed Adjunct Removal: Bush administration plans to take complete unilateral control of post-Saddam Hussein Iraq The third and fourth types of iterative shortening are the removal of trailing PPs and SBARs, respectively: These are the riskiest of the iterative shortening rules, as indicated in our analysis of the human-generated headlines. Thus, we apply these conservatively, only when there are no other categories of rules to apply. Moreover, these rules are applied with a backoff option to avoid over-trimming the parse tree. First the PP shortening rule is applied. If the threshold has been reached, no more shortening is done. However, if the threshold has not been reached, the system reverts to the parse tree as it was before any PPs were removed, and applies the SBAR shortening rule. If the threshold still has not been reached, the PP rule is applied to the result of the SBAR rule. Other sequences of shortening rules are possible. The one above was observed to produce the best results on a 73-sentence development set of stories from the TIPSTER corpus. The intuition is that, when removing constituents from a parse tree, it’s best to remove smaller portions during each iteration, to avoid producing trees with undesirably few words. PPs tend to represent small parts of the tree while SBARs represent large parts of the tree. Thus we try to reach the threshold by removing small constituents, but if we can’t reach the threshold that way, we restore the small constituents, remove a large constituent and resume the deletion of small constituents. The impact of these two types of shortening can be seen in the following examples: Parse: [S More oil-covered sea birds were found [PP over the weekend]] Output of PP Removal: More oil-covered sea birds were found. Parse: [S Visiting China Interpol chief expressed confidence in Hong Kong’s smooth transition [SBAR while assuring closer cooperation after Hong Kong returns]] Output of SBAR Removal: Visiting China Interpol chief expressed confidence in Hong Kong’s smooth transition We conducted two evaluations. One was an informal human assessment and one was a formal automatic evaluation. We compared our current system to a statistical headline generation system we presented at the 2001 DUC Summarization Workshop (Zajic et al., 2002), which we will refer to as HMM Hedge. HMM Hedge treats the summarization problem as analogous to statistical machine translation. The verbose language, articles, is treated as the result of a concise language, headlines, being transmitted through a noisy channel. The result of the transmission is that extra words are added and some morphological variations occur. The Viterbi algorithm is used to calculate the most likely unseen headline to have generated the seen article. The Viterbi algorithm is biased to favor headline-like characteristics gleaned from observation of human performance of the headline-construction task. Since the 2002 Workshop, HMM Hedge has been enhanced by incorporating part of speech of information into the decoding process, rejecting headlines that do not contain a word that was used as a verb in the story, and allowing morphological variation only on words that were used as verbs in the story. HMM Hedge was trained on 700,000 news articles and headlines from the TIPSTER corpus. BLEU (Papineni et al, 2002) is a system for automatic evaluation of machine translation. BLEU uses a modified n-gram precision measure to compare machine translations to reference human translations. We treat summarization as a type of translation from a verbose language to a concise one, and compare automatically generated headlines to human generated headlines. For this evaluation we used 100 headlines created for 100 AP stories from the TIPSTER collection for August 6, 1990 as reference summarizations for those stories. These 100 stories had never been run through either system or evaluated by the authors prior to this evaluation. We also used the 2496 manual abstracts for the DUC2003 10-word summarization task as reference translations for the 624 test documents of that task. We used two variants of HMM Hedge, one which selects headline words from the first 60 words of the story, and one which selects words from the first sentence of the story. Table 1 shows the BLEU score using trigrams, and the 95% confidence interval for the score. These results show that although Hedge Trimmer scores slightly higher than HMM Hedge on both data sets, the results are not statistically significant. However, we believe that the difference in the quality of the systems is not adequately reflected by this automatic evaluation. Human evaluation indicates significantly higher scores than might be guessed from the automatic evaluation. For the 100 AP stories from the TIPSTER corpus for August 6, 1990, the output of Hedge Trimmer and HMM Hedge was evaluated by one human. Each headline was given a subjective score from 1 to 5, with 1 being the worst and 5 being the best. The average score of HMM Hedge was 3.01 with standard deviation of 1.11. The average score of Hedge Trimmer was 3.72 with standard deviation of 1.26. Using a t-score, the difference is significant with greater than 99.9% confidence. The types of problems exhibited by the two systems are qualitatively different. The probabilistic system is more likely to produce an ungrammatical result or omit a necessary argument, as in the examples below. In contrast, the parser-based system is more likely to fail by producing a grammatical but semantically useless headline. Finally, even when both systems produce acceptable output, Hedge Trimmer usually produces headlines which are more fluent or include more useful information. demanding that Chinese authorities respect culture. We have shown the effectiveness of constructing headlines by selecting words in order from a newspaper story. The practice of selecting words from the early part of the document has been justified by analyzing the behavior of humans doing the task, and by automatic evaluation of a system operating on a similar principle. We have compared two systems that use this basic technique, one taking a statistical approach and the other a linguistic approach. The results of the linguistically motivated approach show that we can build a working system with minimal linguistic knowledge and circumvent the need for large amounts of training data. We should be able to quickly produce a comparable system for other languages, especially in light of current multi-lingual initiatives that include automatic parser induction for new languages, e.g. the TIDES initiative. We plan to enhance Hedge Trimmer by using a language model of Headlinese, the language of newspaper headlines (Mårdh 1980) to guide the system in which constituents to remove. We Also we plan to allow for morphological variation in verbs to produce the present tense headlines typical of Headlinese. Hedge Trimmer will be installed in a translingual detection system for enhanced display of document surrogates for cross-language question answering. This system will be evaluated in upcoming iCLEF conferences. The University of Maryland authors are supported, in part, by BBNT Contract 020124-7157, DARPA/ITO Contract N66001-97-C-8540, and NSF CISE Research Infrastructure Award EIA0130422. We would like to thank Naomi Chang and Jon Teske for generating reference headlines.    \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is significantly improved using this approach. We are given a source (‘French’) sentence fJ1 = f1, ... , fj, ... , fJ, which is to be translated into a target (‘English’) sentence eI1 = e1, ... , ei, ... , eI. Among all possible target sentences, we will choose the sentence with the highest probability:1 The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. According to Bayes’ decision rule, we can equivalently to Eq. 1 perform the following maximization: This approach is referred to as source-channel approach to statistical MT. Sometimes, it is also referred to as the ‘fundamental equation of statistical MT’ (Brown et al., 1993). Here, Pr(eI1) is the language model of the target language, whereas Pr(fJ1 |eI1) is the translation model. Typically, Eq. 2 is favored over the direct translation model of Eq. 1 with the argument that it yields a modular approach. Instead of modeling one probability distribution, we obtain two different knowledge sources that are trained independently. The overall architecture of the source-channel approach is summarized in Figure 1. In general, as shown in this figure, there may be additional transformations to make the translation task simpler for the algorithm. Typically, training is performed by applying a maximum likelihood approach. If the language model Pr(eI1) = pγ(eI1) depends on parameters γ and the translation model Pr(fJ1 |eI1) = pθ(fJ1 |eI1) depends on parameters θ, then the optimal parameter values are obtained by maximizing the likelihood on a parallel training corpus fS1 , eS1 (Brown et al., 1993): We obtain the following decision rule: instead of Eq. 5 (Och et al., 1999): State-of-the-art statistical MT systems are based on this approach. Yet, the use of this decision rule has various problems: Here, we replaced pˆ�(fJ1 |ei) by pˆ�(ei|fJ1 ). From a theoretical framework of the sourcechannel approach, this approach is hard to justify. Yet, if both decision rules yield the same translation quality, we can use that decision rule which is better suited for efficient search. As alternative to the source-channel approach, we directly model the posterior probability Pr(ei|fJ1 ). An especially well-founded framework for doing this is maximum entropy (Berger et al., 1996). In this framework, we have a set of M feature functions hm(ei, fJ1 ), m = 1, ... , M. For each feature function, there exists a model parameter am, m = 1, ... , M. The direct translation probability is given the following two feature functions: This approach has been suggested by (Papineni et al., 1997; Papineni et al., 1998) for a natural language understanding task. We obtain the following decision rule: Hence, the time-consuming renormalization in Eq. 8 is not needed in search. The overall architecture of the direct maximum entropy models is summarized in Figure 2. Interestingly, this framework contains as special case the source channel approach (Eq. 5) if we use and set A1 = A2 = 1. Optimizing the corresponding parameters A1 and A2 of the model in Eq. 8 is equivalent to the optimization of model scaling factors, which is a standard approach in other areas such as speech recognition or pattern recognition. The use of an ‘inverted’ translation model in the unconventional decision rule of Eq. 6 results if we use the feature function log Pr(eI1|fJ1 ) instead of log Pr(fJ1 |eI1). In this framework, this feature can be as good as log Pr(fJ1 |eI1). It has to be empirically verified, which of the two features yields better results. We even can use both features log Pr(eI1|fJ1 ) and log Pr(fJ1 |eI1), obtaining a more symmetric translation model. As training criterion, we use the maximum class posterior probability criterion: This corresponds to maximizing the equivocation or maximizing the likelihood of the direct translation model. This direct optimization of the posterior probability in Bayes decision rule is referred to as discriminative training (Ney, 1995) because we directly take into account the overlap in the probability distributions. The optimization problem has one global optimum and the optimization criterion is convex. Typically, the probability Pr(fJ1 |eI1) is decomposed via additional hidden variables. In statistical alignment models Pr(fJ1 , aJ1 |eI1), the alignment aJ1 is introduced as a hidden variable: As specific MT method, we use the alignment template approach (Och et al., 1999). The key elements of this approach are the alignment templates, which are pairs of source and target language phrases together with an alignment between the words within the phrases. The advantage of the alignment template approach compared to single word-based statistical translation models is that word context and local changes in word order are explicitly considered. The alignment template model refines the translation probability Pr(fJ1 |eI1) by introducing two hidden variables zK1 and aK1 for the K alignment templates and the alignment of the alignment templates: The alignment mapping is j → i = aj from source position j to target position i = aj. Search is performed using the so-called maximum approximation: Hence, the search space consists of the set of all possible target language sentences eI1 and all possible alignments aJ1 . Generalizing this approach to direct translation models, we extend the feature functions to include the dependence on the additional hidden variable. Using M feature functions of the form hm(eI1, fJ1 , aJ1), m = 1, ... , M, we obtain the following model: Obviously, we can perform the same step for translation models with an even richer structure of hidden variables than only the alignment aJ1 . To simplify the notation, we shall omit in the following the dependence on the hidden variables of the model. Hence, we obtain three different probability distributions: Pr(aK1 |eI1), Pr(zK1 |aK1 , eI1) and Pr(fJ1 |zK1 ,aK1 ,eI1). Here, we omit a detailed description of modeling, training and search, as this is not relevant for the subsequent exposition. For further details, see (Och et al., 1999). To use these three component models in a direct maximum entropy approach, we define three different feature functions for each component of the translation model instead of one feature function for the whole translation model p(fJ1 |eI1). The feature functions have then not only a dependence on fJ1 and eI1 but also on zK1 , aK1 . So far, we use the logarithm of the components of a translation model as feature functions. This is a very convenient approach to improve the quality of a baseline system. Yet, we are not limited to train only model scaling factors, but we have many possibilities: This corresponds to a word penalty for each produced target word. • We could use grammatical features that relate certain grammatical dependencies of source and target language. For example, using a function k(·) that counts how many verb groups exist in the source or the target sentence, we can define the following feature, which is 1 if each of the two sentences contains the same number of verb groups: In the same way, we can introduce semantic features or pragmatic features such as the dialogue act classification. We can use numerous additional features that deal with specific problems of the baseline statistical MT system. In this paper, we shall use the first three of these features. As additional language model, we use a class-based five-gram language model. This feature and the word penalty feature allow a straightforward integration into the used dynamic programming search algorithm (Och et al., 1999). As this is not possible for the conventional dictionary feature, we use n-best rescoring for this feature. To train the model parameters λM1 of the direct translation model according to Eq. 11, we use the GIS (Generalized Iterative Scaling) algorithm (Darroch and Ratcliff, 1972). It should be noted that, as was already shown by (Darroch and Ratcliff, 1972), by applying suitable transformations, the GIS algorithm is able to handle any type of real-valued features. To apply this algorithm, we have to solve various practical problems. The renormalization needed in Eq. 8 requires a sum over a large number of possible sentences, for which we do not know an efficient algorithm. Hence, we approximate this sum by sampling the space of all possible sentences by a large set of highly probable sentences. The set of considered sentences is computed by an appropriately extended version of the used search algorithm (Och et al., 1999) computing an approximate n-best list of translations. Unlike automatic speech recognition, we do not have one reference sentence, but there exists a number of reference sentences. Yet, the criterion as it is described in Eq. 11 allows for only one reference translation. Hence, we change the criterion to allow Rs reference translations es,1, ... , es,Rs for the sentence es: We use this optimization criterion instead of the optimization criterion shown in Eq. 11. In addition, we might have the problem that no single of the reference translations is part of the nbest list because the search algorithm performs pruning, which in principle limits the possible translations that can be produced given a certain input sentence. To solve this problem, we define for maximum entropy training each sentence as reference translation that has the minimal number of word errors with respect to any of the reference translations. We present results on the VERBMOBIL task, which is a speech translation task in the domain of appointment scheduling, travel planning, and hotel reservation (Wahlster, 1993). Table 1 shows the corpus statistics of this task. We use a training corpus, which is used to train the alignment template model and the language models, a development corpus, which is used to estimate the model scaling factors, and a test corpus. So far, in machine translation research does not exist one generally accepted criterion for the evaluation of the experimental results. Therefore, we use a large variety of different criteria and show that the obtained results improve on most or all of these criteria. In all experiments, we use the following six error criteria: of the target sentence, so that the WER measure alone could be misleading. To overcome this problem, we introduce as additional measure the position-independent word error rate (PER). This measure compares the words in the two sentences ignoring the word order. more detailed analysis, subjective judgments by test persons are necessary. Each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0 (NieBen et al., 2000). • IER (information item error rate): The test sentences are segmented into information items. For each of them, if the intended information is conveyed and there are no syntactic errors, the sentence is counted as correct (NieBen et al., 2000). In the following, we present the results of this approach. Table 2 shows the results if we use a direct translation model (Eq. 6). As baseline features, we use a normal word trigram language model and the three component models of the alignment templates. The first row shows the results using only the four baseline features with λ1 = · · · = λ4 = 1. The second row shows the result if we train the model scaling factors. We see a systematic improvement on all error rates. The following three rows show the results if we add the word penalty, an additional class-based five-gram GIS algorithm for maximum entropy training of alignment templates. language model and the conventional dictionary features. We observe improved error rates for using the word penalty and the class-based language model as additional features. Figure 3 show how the sentence error rate (SER) on the test corpus improves during the iterations of the GIS algorithm. We see that the sentence error rates converges after about 4000 iterations. We do not observe significant overfitting. Table 3 shows the resulting normalized model scaling factors. Multiplying each model scaling factor by a constant positive value does not affect the decision rule. We see that adding new features also has an effect on the other model scaling factors. The use of direct maximum entropy translation models for statistical machine translation has been suggested by (Papineni et al., 1997; Papineni et al., 1998). They train models for natural language understanding rather than natural language translation. In contrast to their approach, we include a dependence on the hidden variable of the translation model in the direct translation model. Therefore, we are able to use statistical alignment models, which have been shown to be a very powerful component for statistical machine translation systems. In speech recognition, training the parameters of the acoustic model by optimizing the (average) mutual information and conditional entropy as they are defined in information theory is a standard approach (Bahl et al., 1986; Ney, 1995). Combining various probabilistic models for speech and language modeling has been suggested in (Beyerlein, 1997; Peters and Klakow, 1999). We have presented a framework for statistical MT for natural languages, which is more general than the widely used source-channel approach. It allows a baseline MT system to be extended easily by adding new feature functions. We have shown that a baseline statistical MT system can be significantly improved using this framework. There are two possible interpretations for a statistical MT system structured according to the sourcechannel approach, hence including a model for Pr(ei) and a model for Pr(fi Iei). We can interpret it as an approximation to the Bayes decision rule in Eq. 2 or as an instance of a direct maximum entropy model with feature functions log Pr(ei) and log Pr(fi |ei). As soon as we want to use model scaling factors, we can only do this in a theoretically justified way using the second interpretation. Yet, the main advantage comes from the large number of additional possibilities that we obtain by using the second interpretation. An important open problem of this approach is the handling of complex features in search. An interesting question is to come up with features that allow an efficient handling using conventional dynamic programming search algorithms. In addition, it might be promising to optimize the parameters directly with respect to the error rate of the MT system as is suggested in the field of pattern and speech recognition (Juang et al., 1995; Schl¨uter and Ney, 2001).    \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                summary  \n",
              "0                                                                                                                 We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics. We propose a window method introducing the concept of semantic stickiness of two words as the relatively frequent close occurrence between them (less than 500 words distance).  \n",
              "1  We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets. These FSAs are good representations of paraphrases. They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets. Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations. We describe a syntax-based algorithm that builds word lattices from parallel translations which can be used to generate new para phrases. We propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences.  \n",
              "2                                                                                                                                                                                                                                                                                                                                                   We describe a procedure for arranging into a time-line the contents of news stories describing the development of some situation. We describe the parts of the system that deal with 1. breaking sentences into event-clauses and 2. resolving both explicit and implicit temporal references. Evaluations show a performance of 52%, compared to humans. We infer time values based on the most recently assigned date of the date of the article.  \n",
              "3               This paper presents Hedge Trimmer, a HEaDline GEneration system that creates a headline for a newspaper story using linguistically-motivated heuristics to guide the choice of a potential headline. We present feasibility tests used to establish the validity of an approach that constructs a headline by selecting words in order from a story. In addition, we describe experimental results that demonstrate the effectiveness of our linguistically-motivated approach over a HMM-based model, using both human evaluation and automatic metrics for comparing the two approaches. Our approach focuses on extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length.  \n",
              "4                                                                                                                                                                                                   We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is significantly improved using this approach.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-21debac4-c414-4f2a-9cc9-0fac346f8740\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Class-Based N-Gram Models Of Natural Language</td>\n",
              "      <td>We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics. IBM T. J. Watson Research Center We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics. In a number of natural language processing tasks, we face the problem of recovering a string of English words after it has been garbled by passage through a noisy channel. To tackle this problem successfully, we must be able to estimate the probability with which any particular string of English words will be presented as input to the noisy channel. In this paper, we discuss a method for making such estimates. We also discuss the related topic of assigning words to classes according to statistical behavior in a large body of text. In the next section, we review the concept of a language model and give a definition of n-gram models. In Section 3, we look at the subset of n-gram models in which the words are divided into classes. We show that for n = 2 the maximum likelihood assignment of words to classes is equivalent to the assignment for which the average mutual information of adjacent classes is greatest. Finding an optimal assignment of words to classes is computationally hard, but we describe two algorithms for finding a suboptimal assignment. In Section 4, we apply mutual information to two other forms of word clustering. First, we use it to find pairs of words that function together as a single lexical entity. Then, by examining the probability that two words will appear within a reasonable distance of one another, we use it to find classes that have some loose semantic coherence. In describing our work, we draw freely on terminology and notation from the mathematical theory of communication. The reader who is unfamiliar with this field or who has allowed his or her facility with some of its concepts to fall into disrepair may profit from a brief perusal of Feller (1950) and Gallagher (1968). In the first of these, the reader should focus on conditional probabilities and on Markov chains; in the second, on entropy and mutual information. Source-channel setup. Figure 1 shows a model that has long been used in automatic speech recognition (Bahl, Jelinek, and Mercer 1983) and has recently been proposed for machine translation (Brown et al. 1990) and for automatic spelling correction (Mays, Demerau, and Mercer 1990). In automatic speech recognition, y is an acoustic signal; in machine translation, y is a sequence of words in another language; and in spelling correction, y is a sequence of characters produced by a possibly imperfect typist. In all three applications, given a signal y, we seek to determine the string of English words, w, which gave rise to it. In general, many different word strings can give rise to the same signal and so we cannot hope to recover w successfully in all cases. We can, however, minimize our probability of error by choosing as our estimate of w that string W for which the a posteriori probability of W given y is greatest. For a fixed choice of y, this probability is proportional to the joint probability of * and y which, as shown in Figure 1, is the product of two terms: the a priori probability of W and the probability that y will appear at the output of the channel when * is placed at the input. The a priori probability of W, Pr (W), is the probability that the string W will arise in English. We do not attempt a formal definition of English or of the concept of arising in English. Rather, we blithely assume that the production of English text can be characterized by a set of conditional probabilities, Pr(wk w), in terms of which the probability of a string of words, w, can be expressed as a product: Here, wki-1 represents the string wi w2 • • • wk_i . In the conditional probability Pr(wk I -1 , ) we call wk-1 the history and wk the prediction. We refer to a computational mechanism for obtaining these conditional probabilities as a language model. Often we must choose which of two different language models is the better one. The performance of a language model in a complete system depends on a delicate interplay between the language model and other components of the system. One language model may surpass another as part of a speech recognition system but perform less well in a translation system. However, because it is expensive to evaluate a language model in the context of a complete system, we are led to seek an intrinsic measure of the quality of a language model. We might, for example, use each language model to compute the joint probability of some collection of strings and judge as better the language model that yields the greater probability The perplexity of a language model with respect to a sample of text, S. is the reciprocal of the geometric average of the probabilities of the predictions in S. If S has I S I words, then the perplexity is Pr (5)-1/1s1. Thus, the language model with the smaller perplexity will be the one that assigns the larger probability to S. Because the perplexity depends not only on the language model but also on the text with respect to which it is measured, it is important that the text be representative of that for which the language model is intended. Because perplexity is subject to sampling error, making fine distinctions between language models may require that the perplexity be measured with respect to a large sample. In an n-gram language model, we treat two histories as equivalent if they end in the same n - 1 words, i.e., we assume that for k &gt; n, Pr (wk w1k-1) is equal to Pr (wk I wifcin1+1). For a vocabulary of size V, a 1-gram model has V - 1 independent parameters, one for each word minus one for the constraint that all of the probabilities add up to 1. A 2-gram model has V(V - 1) independent parameters of the form Pr (102 I wi ) and V - 1 of the form Pr (w) for a total of V2 - 1 independent parameters. In general, an n-gram model has V&amp;quot; - 1 independent parameters: V&amp;quot;-1(V - 1) of the form Pr (wn I wr1), which we call the order-n parameters, plus the 17n-1-1 parameters of an (n - 1)-gram model. We estimate the parameters of an n-gram model by examining a sample of text, tf, which we call the training text, in a process called training. If C(w) is the number of times that the string w occurs in the string 1-T, then for a 1-gram language model the maximum likelihood estimate for the parameter Pr (w) is C(w)/T. To estimate the parameters of an n-gram model, we estimate the parameters of the (n -1)-gram model that it contains and then choose the order-n parameters so as to maximize Pr (tnT trii -1). Thus, the order-n parameters are We call this method of parameter estimation sequential maximum likelihood estimation. We can think of the order-n parameters of an n-gram model as constituting the transition matrix of a Markov model the states of which are sequences of n - 1 words. Thus, the probability of a transition between the state W1W2 • • ' Wn-1 and the state w2w3 • • • wn is Pr (w I W1102 • • • wn-i ) . The steady-state distribution for this transition matrix assigns a probability to each (n - 1)-gram, which we denote S(w7-1). We say that an n-gram language model is consistent if, for each string w7-1, the probability that the model assigns to win-1 is S(win-1). Sequential maximum likelihood estimation does not, in general, lead to a consistent model, although for large values of T, the model will be very nearly consistent. Maximum likelihood estimation of the parameters of a consistent n-gram language model is an interesting topic, but is beyond the scope of this paper. The vocabulary of English is very large and so, even for small values of n, the number of parameters in an n-gram model is enormous. The IBM Tangora speech recognition system has a vocabulary of about 20,000 words and employs a 3-gram language model with over eight trillion parameters (Averbuch et al. 1987). We can illustrate the problems attendant to parameter estimation for a 3-gram language model with the data in Table 1. Here, we show the number of 1-, 2-, and 3-grams appearing with various frequencies in a sample of 365,893,263 words of English text from a variety of sources. The vocabulary consists of the 260,740 different words plus a special Number of n-grams with various frequencies in 365,893,263 words of running text. unknown word into which all other words are mapped. Of the 6.799 x 1010 2-grams that might have occurred in the data, only 14,494,217 actually did occur and of these, 8,045,024 occurred only once each. Similarly, of the 1.773 x 1016 3-grams that might have occurred, only 75,349,888 actually did occur and of these, 53,737,350 occurred only once each. From these data and Turing's formula (Good 1953), we can expect that maximum likelihood estimates will be 0 for 14.7 percent of the 3-grams and for 2.2 percent of the 2-grams in a new sample of English text. We can be confident that any 3-gram that does not appear in our sample is, in fact, rare, but there are so many of them that their aggregate probability is substantial. As n increases, the accuracy of an n-gram model increases, but the reliability of our parameter estimates, drawn as they must be from a limited training text, decreases. Jelinek and Mercer (1980) describe a technique called interpolated estimation that combines the estimates of several language models so as to use the estimates of the more accurate models where they are reliable and, where they are unreliable, to fall back on the more reliable estimates of less accurate models. If Pri (w I &amp;I.-1) is the conditional probability as determined by the jth language model, then the interpolated estimate, Pr(wi I w'i-1), is given by Given values for Pr(i) 0, the A1(w) are chosen, with the help of the EM algorithm, so as to maximize the probability of some additional sample of text called the held-out data (Baum 1972; Dempster, Laird, and Rubin 1977; Jelinek and Mercer 1980). When we use interpolated estimation to combine the estimates from 1-, 2-, and 3-gram models, we choose the As to depend on the history, w1, only through the count of the 2gram, We expect that where the count of the 2-gram is high, the 3-gram estimates will be reliable, and, where the count is low, the estimates will be unreliable. We have constructed an interpolated 3-gram model in which we have divided the As into 1,782 different sets according to the 2-gram counts. We estimated these As from a held-out sample of 4,630,934 words. We measure the performance of our model on the Brown corpus, which contains a variety of English text and is not included in either our training or held-out data (Kiera and Francis 1967). The Brown corpus contains 1,014,312 words and has a perplexity of 244 with respect to our interpolated model. Clearly, some words are similar to other words in their meaning and syntactic function. We would not be surprised to learn that the probability distribution of words in the vicinity of Thursday is very much like that for words in the vicinity of Friday. Of Peter F. Brown and Vincent J. Della Pietra Class-Based n-gram Models of Natural Language course, they will not be identical: we rarely hear someone say Thank God it's Thursday! or worry about Thursday the 13th. If we can successfully assign words to classes, it may be possible to make more reasonable predictions for histories that we have not previously seen by assuming that they are similar to other histories that we have seen. Suppose that we partition a vocabulary of V words into C classes using a function, 7r, which maps a word, wi, into its class, ci. We say that a language model is an ngram class model if it is an n-gram language model and if, in addition, for 1 &lt; k &lt; n, independent parameters: V - C of the form Pr (w j c,), plus the C&amp;quot; - 1 independent parameters of an n-gram language model for a vocabulary of size C. Thus, except in the trivial cases in which C --= V or n 1, an n-gram class language model always has fewer independent parameters than a general n-gram language model. Given training text, tr, the maximum likelihood estimates of the parameters of a 1-gram class model are where by C(c) we mean the number of words in tf for which the class is c. From these equations, we see that, since c = r(w), Pr (w) = Pr (w I c) Pr (c) = C(w)/T. For a 1-gram class model, the choice of the mapping it has no effect. For a 2-gram class model, the sequential maximum likelihood estimates of the order-2 parameters maximize Pr (tII ti) or, equivalently, log Pr(tr I t1) and are given by By definition, Pr (ci c2) = Pr (ci) Pr (c2 I ci), and so, for sequential maximum likelihood estimation, we have Since C(ci ) and Ec c(cio are the numbers of words for which the class is ci in the strings ti. and tiT-1 respectively, the final term in this equation tends to 1 as T tends to infinity. Thus, Pr (ci c2) tends to the relative frequency of ci c2 as consecutive classes in the training text. Therefore, since Ew c(ww2)/(T— 1) tends to the relative frequency of w2 in the training text, and hence to Pr (w2), we must have, in the limit, where H(w) is the entropy of the 1-gram word distribution and /(ci , c2) is the average mutual information of adjacent classes. Because L(7r) depends on 7r only through this average mutual information, the partition that maximizes L(7r) is, in the limit, also the partition that maximizes the average mutual information of adjacent classes. We know of no practical method for finding one of the partitions that maximize the average mutual information. Indeed, given such a partition, we know of no practical method for demonstrating that it does, in fact, maximize the average mutual information. We have, however, obtained interesting results using a greedy algorithm. Initially, we assign each word to a distinct class and compute the average mutual information between adjacent classes. We then merge that pair of classes for which the loss in average mutual information is least. After V — C of these merges, C classes remain. Often, we find that for classes obtained in this way the average mutual information can be made larger by moving some words from one class to another. Therefore, after having derived a set of classes from successive merges, we cycle through the vocabulary moving each word to the class for which the resulting partition has the greatest average mutual information. Eventually no potential reassignment of a word leads to a partition with greater average mutual information. At this point, we stop. It may be possible to find a partition with higher average mutual information by simultaneously reassigning two or more words, but we regard such a search as too costly to be feasible. To make even this suboptimal algorithm practical one must exercise a certain care in implementation. There are approximately (V-02/2 merges that we must investigate to carry out the ith step. The average mutual information remaining after any one of them is the sum of (V — 02 terms, each of which involves a logarithm. Since altogether we must make V — C merges, this straightforward approach to the computation is of order V5. We cannot seriously contemplate such a calculation except for very small values of V. A more frugal organization of the computation must take advantage of the redundancy in this straightforward calculation. As we shall see, we can make the computation of the average mutual information remaining after a merge in constant time, independent of V. Suppose that we have already made V —k merges, resulting in classes Ck(1), Ck (2), , Ck (k) and that we now wish to investigate the merge of Ck (i) with Ck (j for 1 &lt; i &lt;j &lt; k. Let pk(1, m) -= Pr (Ck (0, Ck(m)), i.e., the probability that a word in class Ck (m) follows a word in class Ck(1). Let and let The average mutual information remaining after V — k merges is We use the notation i+ j to represent the cluster obtained by merging Ck(i) and Ck(i)• If we know Ik. SO), and sk(j), then the majority of the time involved in computing Ik(i,j) is devoted to computing the sums on the second line of equation (15). Each of these sums has approximately V - k terms and so we have reduced the problem of evaluating Ik(i,j) from one of order V2 to one of order V. We can improve this further by keeping track of those pairs 1,m for which pk(1,m) is different from 0. We recall from Table 1, for example, that of the 6.799 x 1010 2-grams that might have occurred in the training data, only 14,494,217 actually did occur. Thus, in this case, the sums required in equation (15) have, on average, only about 56 non-zero terms instead of 260,741, as we might expect from the size of the vocabulary By examining all pairs, we can find that pair, i &lt;j, for which the loss in average mutual information, Lk (i, j) - Ik(i, j), is least. We complete the step by merging Ck(i) and Ck(j) to form a new cluster Ck_i (i). If j k, we rename Ck(k) as Ck_i (i) and for 1 i,j, we set Ck-i (1) to Ck(/). Obviously, Ik-i = Ik(i,j). The values of Pk-1, prk_i, and qk_...1 can be obtained easily from Pk, plk, prk, and qk. If 1 and m both denote indices neither of which is equal to either i or j, then it is easy to establish that Finally, we must evaluate sk_1(i) and Lk_1(/, i) from equations 15 and 16. Thus, the entire update process requires something on the order of V2 computations in the course of which we will determine the next pair of clusters to merge. The algorithm, then, is of order V3. Although we have described this algorithm as one for finding clusters, we actually determine much more. If we continue the algorithm for V - 1 merges, then we will have a single cluster which, of course, will be the entire vocabulary. The order in which clusters are merged, however, determines a binary tree the root of which corresponds reps representatives representative rep Sample subtrees from a 1,000-word mutual information tree. to this single cluster and the leaves of which correspond to the words in the vocabulary. Intermediate nodes of the tree correspond to groupings of words intermediate between single words and the entire vocabulary. Words that are statistically similar with respect to their immediate neighbors in running text will be close together in the tree. We have applied this tree-building algorithm to vocabularies of up to 5,000 words. Figure 2 shows some of the substructures in a tree constructed in this manner for the 1,000 most frequent words in a collection of office correspondence. Beyond 5,000 words this algorithm also fails of practicality. To obtain clusters for larger vocabularies, we proceed as follows. We arrange the words in the vocabulary in order of frequency with the most frequent words first and assign each of the first C words to its own, distinct class. At the first step of the algorithm, we assign the (C + 1)st most probable word to a new class and merge that pair among the resulting C + 1 classes for which the loss in average mutual information is least. At the kth step of the algorithm, we assign the (C + k)th most probable word to a new class. This restores the number of classes to C + 1, and we again merge that pair for which the loss in average mutual information is least. After V — C steps, each of the words in the vocabulary will have been assigned to one of C classes. We have used this algorithm to divide the 260,741-word vocabulary of Table 1 into 1,000 classes. Table 2 contains examples of classes that we find particularly interesting. Table 3 contains examples that were selected at random. Each of the lines in the tables contains members of a different class. The average class has 260 words and so to make the table manageable, we include only words that occur at least ten times and we include no more than the ten most frequent words of any class (the other two months would appear with the class of months if we extended this limit to twelve). The degree to which the classes capture both syntactic and semantic aspects of English is quite surprising given that they were constructed from nothing more than counts of bigrams. The class {that tha theat} is interesting because although tha and theat are not English words, the computer has discovered that in our data each of them is most often a mistyped that. Table 4 shows the number of class 1-, 2-, and 3-grams occurring in the text with various frequencies. We can expect from these data that maximum likelihood estimates will assign a probability of 0 to about 3.8 percent of the class 3-grams and to about .02 percent of the class 2-grams in a new sample of English text. This is a substantial improvement over the corresponding numbers for a 3-gram language model, which are 14.7 percent for word 3-grams and 2.2 percent for word 2-grams, but we have achieved this at the expense of precision in the model. With a class model, we distinguish between two different words of the same class only according to their relative frequencies in the text as a whole. Looking at the classes in Tables 2 and 3, we feel that this is reasonable for pairs like John and George or liberal and conservative but perhaps less so for pairs like little and prima or Minister and mover. We used these classes to construct an interpolated 3-gram class model using the same training text and held-out data as we used for the word-based language model we discussed above. We measured the perplexity of the Brown corpus with respect to this model and found it to be 271. We then interpolated the class-based estimators with the word-based estimators and found the perplexity of the test data to be 236, which is a small improvement over the perplexity of 244 we obtained with the word-based model. In the previous section, we discussed some methods for grouping words together according to the statistical similarity of their surroundings. Here, we discuss two additional types of relations between words that can be discovered by examining various co-occurrence statistics. The mutual information of the pair w1 and w2 as adjacent words is If w2 follows wi less often than we would expect on the basis of their independent frequencies, then the mutual information is negative. If w2 follows wi more often than we would expect, then the mutual information is positive. We say that the pair w1 w2 is sticky if the mutual information for the pair is substantially greater than 0. In Table 5, we list the 20 stickiest pairs of words found in a 59,537,595-word sample of text from the Canadian parliament. The mutual information for each pair is given in bits, which corresponds to using 2 as the base of the logarithm in equation 18. Most of the pairs are proper names such as Pontius Pilate or foreign phrases that have been adopted into English such as mutatis mutandis and avant garde. The mutual information for Hum pty Dumpty, 22.5 bits, means that the pair occurs roughly 6,000,000 times more than one would expect from the individual frequencies of Hum pty and Dumpty. Notice that the property of being a sticky pair is not symmetric and so, while Hum pty Dumpty forms a sticky pair, Dumpty Hum pty does not. Instead of seeking pairs of words that occur next to one another more than we would expect, we can seek pairs of words that simply occur near one another more than we would expect. We avoid finding sticky pairs again by not considering pairs of words that occur too close to one another. To be precise, let Prnear (w1 w2) be the probability that a word chosen at random from the text is w1 and that a second word, chosen at random from a window of 1,001 words centered on wi but excluding the words in a window of 5 centered on w1, is w2. We say that w1 and w2 are semantically sticky if Prnear (W1W2) is much larger than Pr (w1) Pr (w2) . Unlike stickiness, semantic stickiness is symmetric so that if w1 sticks semantically to w2, then w2 sticks semantically to w1. In Table 6, we show some interesting classes that we constructed, using Prnear (w1 w2), in a manner similar to that described in the preceding section. Some classes group together words having the same morphological stem, such as performance, performed, perform, performs, and performing. Other classes contain words that are semantically related but have different stems, such as attorney, counsel, trial, court, and judge. We have described several methods here that we feel clearly demonstrate the value of simple statistical techniques as allies in the struggle to tease from words their linguistic secrets. However, we have not as yet demonstrated the full value of the secrets thus gleaned. At the expense of a slightly greater perplexity, the 3-gram model with word classes requires only about one-third as much storage as the 3-gram language model in which each word is treated as a unique individual (see Tables 1 and 4). Even when we combine the two models, we are not able to achieve much improvement in the perplexity. Nonetheless, we are confident that we will eventually be able to make significant improvements to 3-gram language models with the help of classes of the kind that we have described here. The authors would like to thank John Lafferty for his assistance in constructing word classes described in this paper.</td>\n",
              "      <td>We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics. We propose a window method introducing the concept of semantic stickiness of two words as the relatively frequent close occurrence between them (less than 500 words distance).</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Syntax-Based Alignment Of Multiple Translations: Extracting Paraphrases And Generating New Sentences</td>\n",
              "      <td>We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets. These FSAs are good representations of paraphrases. They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets. Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations. In the past, paraphrases have come under the scrutiny of many research communities. Information retrieval researchers have used paraphrasing techniques for query reformulation in order to increase the recall of information retrieval engines (Sparck Jones and Tait, 1984). Natural language generation researchers have used paraphrasing to increase the expressive power of generation systems (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999). And researchers in multi-document text summarization (Barzilay et al., 1999), information extraction (Shinyama et al., 2002), and question answering (Lin and Pantel, 2001; Hermjakob et al., 2002) have focused on identifying and exploiting paraphrases in the context of recognizing redundancies, alternative formulations of the same meaning, and improving the performance of question answering systems. In previous work (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Shinyama et al., 2002), paraphrases are represented as sets or pairs of semantically equivalent words, phrases, and patterns. Although this is adequate in the context of some applications, it is clearly too weak from a generative perspective. Assume, for example, that we know that text pairs (stock market rose, stock market gained) and (stock market rose, stock prices rose) have the same meaning. If we memorized only these two pairs, it would be impossible to infer that, in fact, consistent with our intuition, any of the following sets of phrases are also semantically equivalent: {stock market rose, stock market gained, stock prices rose, stock prices gained } and {stock market, stock prices } in the context of rose or gained; {market rose }, {market gained }, {prices rose } and {prices gained } in the context of stock; and so on. In this paper, we propose solutions for two problems: the problem ofparaphrase representation and the problem of paraphrase induction. We propose a new, finite-statebased representation of paraphrases that enables one to encode compactly large numbers of paraphrases. We also propose algorithms that automatically derive such representations from inputs that are now routinely released in conjunction with large scale machine translation evaluations (DARPA, 2002): multiple English translations of many foreign language texts. For instance, when given as input the 11 semantically equivalent English translations in Figure 1, our algorithm automatically induces the FSA in Figure 2, which represents compactly 49 distinct renderings of the same semantic meaning. Our FSAs capture both lexical paraphrases, such as {fighting, battle}, {died, were killed} and structural paraphrases such as {last week’s fighting, the battle of last week}. The contexts in which these are correct paraphrases are also conveniently captured in the representation. In previous work, Langkilde and Knight (1998) used word lattices for language generation, but their method involved hand-crafted rules. Bangalore et al. (2001) and Barzilay and Lee (2002) both applied the technique of multi-sequence alignment (MSA) to align parallel corpora and produced similar FSAs. For their purposes, they mainly need to ensure the correctness of consensus among different translations, so that different constituent orderings in input sentences do not pose a serious problem. In contrast, we want to ensure the correctness of all paths represented by the FSAs, and direct application of MSA in the presence of different constituent orderings can be problematic. For example, when given as input the same sentences in Figure 1, one instantiation of the MSA algorithm produces the FSA in Figure 3, which contains many “bad” paths such as the battle of last week’s fighting took at least 12 people lost their people died in the fighting last week’s fighting (See Section 4.2.2 for a more quantitative analysis.). It’s still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)). But we chose to approach this problem from another direction. As a result, we propose a new syntax-based algorithm to produce FSAs. In this paper, we first introduce the multiple translation corpus that we use in our experiments (see Section 2). We then present the algorithms that we developed to induce finite-state paraphrase representations from such data (see Section 3). An important part of the paper is dedicated to evaluating the quality of the finite-state representations that we derive (see Section 4). Since our representations encode thousands and sometimes millions of equivalent verbalizations of the same meaning, we use both manual and automatic evaluation techniques. Some of the automatic evaluations we perform are novel as well. The data we use in this work is the LDC-available Multiple-Translation Chinese (MTC) Corpus1 developed for machine translation evaluation, which contains 105 news stories (993 sentences) from three sources of journalistic Mandarin Chinese text. These stories were independently translated into English by 11 translation agencies. Each sentence group, which consists of 11 semantically equivalent translations, is a rich source for learning lexical and structural paraphrases. In our experiments, we use 899 of the sentence groups — the sentence groups with sentences longer than 45 words were dropped. Our syntax-based alignment algorithm, whose pseudocode is shown in Figure 4, works in three steps. In the first step (lines 1-5 in Figure 4), we parse every sentence in a sentence group and merge all resulting parse trees into a parse forest. In the second step (line 6), we extract an FSA from the parse forest and then we compact it further using a limited form of bottom-up alignment, which we call squeezing (line 7). In what follows, we describe each step in turn. Top-down merging. Given a sentence group, we pass each of the 11 sentences to Charniak’s (2000) parser to get 11 parse trees. The first step in the algorithm is to merge these parse trees into one parse-forest-like structure using a top-down process. Let’s consider a simple case in which the parse forest contains one single tree, Tree 1 in Figure 5, and we are adding Tree 2 to it. Since the two trees correspond to sentences that have the same meaning and since both trees expand an S node into an NP and a V P, it is reasonable to assume that NP1 is a paraphrase of NP2 and V P1 is a paraphrase of V P2. We merge NP1 with NP2 and V P1 with V P2 and continue the merging process on each of the subtrees recursively, until we either reach the leaves of the trees or the two nodes that we examine are expanded using different syntactic rules. When we apply this process to the trees in Figure 5, the NP nodes are merged all the way down to the leaves, and we get “12” as a paraphrase of “twelve” and “people” as a paraphrase of “persons”; in contrast, the two VPs are expanded in different ways, so no merging is done beyond this level, and we are left with the information that “were killed” is a paraphrase of “died”. We repeat this top-down merging procedure with each of the 11 parse trees in a sentence group. So far, only constituents with same syntactic type are treated as paraphrases. However, later we shall see that we can match word spans whose syntactic types differ. Keyword checking. The matching process described above appears quite strict – the expansions must match exactly for two nodes to be merged. But consider the following parse trees: 1. (S (NP1 people)(VP1 were killed in this battle)) 2. (S (NP2 this battle)(VP2 killed people)) If we applied the algorithm described above, we would mistakenly align NP1 with NP2 and V P1 with V P2 — the algorithm described so far makes no use of lexical information. To prevent such erroneous alignments, we also implement a simple keyword checking procedure. We note that since the word “battle” appears in both V P1 and NP2, this can serve as an evidence against the merging of (NP1, NP2) and (V P1, V P2). A similar argument can be constructed for the word “people”. So in this example we actually have double evidence against merging; in general, one such clue suffices to stop the merging. Our keyword checking procedure acts as a filter. A list of keywords is maintained for each node in a syntactic tree. This list contains all the nouns, verbs, and adjectives that are spanned by a syntactic node. Before merging two nodes, we check to see whether the keyword lists associated with them share words with other nodes. That is, supposed we just merged nodes A and B, and they are expanded with the same syntactic rule into A1A2...An and B1B2...Bn respectively; before we merge each Ai with Bi, we check for each Bi if its keyword list shares common words with any Aj (j =� i). If they do not, we continue the top-down merging process; otherwise we stop. In our current implementation, a pair of synonyms can not stop an otherwise legitimate merging, but it’s possible to extend our keyword checking process with the help of lexical resources such as WordNet in future work. Mapping Parse Forests into Finite State Automata. The process of mapping Parse Forests into Finite State Automata is simple. We simply traverse the parse forest top-down and create alternative paths for every merged node. For example, the parse forest in Figure 5 is mapped into the FSA shown at the bottom of the same figure. In the FSA, there is a word associated with each edge. Different paths between any two nodes are assumed to be paraphrases of each other. Each path that starts from the BEGIN node and ends at the END node corresponds to either an original input sentence or a paraphrase sentence. Squeezing. Since we adopted a very strict matching criterion in top-down merging, a small difference in the syntactic structure of two trees prevents some legitimate mergings from taking place. This behavior is also exacerbated by errors in syntactic parsing. Hence, for instance, three edges labeled detroit at the leftmost of the top FSA in Figure 6 were kept apart. To compensate for this effect, our algorithm implements an additional step, which we call squeezing. If two different edges that go into (or out of) the same node in an FSA are labeled with the same word, the nodes on the other end of the edges are merged. We apply this operation exhaustively over the FSAs produced by the top-down merging procedure. Figure 6 illustrates the effect of this operation: the FSA at the top of this figure is compressed into the more compact FSA shown at the bottom of it. Note that in addition to reducing the redundant edges, this also gives us paraphrases not available in the FSA before squeezing (e.g. {reduced to rubble, blasted to ground}). Therefore, the squeezing operation, which implements a limited form of lexically driven alignment similar to that exploited by MSA algorithms, leads to FSAs that have a larger number of paths The evaluation for our finite state representations and algorithm requires careful examination. Obviously, what counts as a good result largely depends on the application one has in mind. If we are extracting paraphrases for question-reformulation, it doesn’t really matter if we output a few syntactically incorrect paraphrases, as long as we produce a large number of semantically correct ones. If we want to use the FSA for MT evaluation (for example, comparing a sentence to be evaluated with the possible paths in FSA), we would want all paths to be relatively good (which we will focus on in this paper), while in some other applications, we may only care about the quality of the best path (not addressed in this paper). Section 4.1 concentrates on evaluating the paraphrase pairs that can be extracted from the FSAs built by our system, while Section 4.2 is dedicated to evaluating the FSAs directly. By construction, different paths between any two nodes in the FSA representations that we derive are paraphrases (in the context in which the nodes occur). To evaluate our algorithm, we extract paraphrases from our FSAs and ask human judges to evaluate their correctness. We compare the paraphrases we collect with paraphrases that are derivable from the same corpus using a cotraining-based paraphrase extraction algorithm (Barzilay and McKeown, 2001). To the best of our knowledge, this is the most relevant work to compare against since it aims at extracting paraphrase pairs from parallel corpus. Unlike our syntax-based algorithm which treats a sentence as a tree structure and uses this hierarchical structural information to guide the merging process, their algorithm treats a sentence as a sequence of phrases with surrounding contexts (no hierarchical structure involved) and cotrains classifiers to detect paraphrases and contexts for paraphrases. It would be interesting to compare the results from two algorithms so different from each other. For the purpose of this experiment, we randomly selected 300 paraphrase pairs (Ssyn) from the FSAs produced by our system. Since the co-training-based algorithm of Barzilay and McKeown (2001) takes parallel corpus as input, we created out of the MTC corpus 55 × 993 sentence pairs (Each equivalent translation set of cardinality 11 was mapped into (2) equivalent translation pairs.). Regina Barzilay kindly provided us the list of paraphrases extracted by their algorithm from this parallel corpus, from which we randomly selected another set of 300 paraphrases (Scotr). phrases produced by the syntax-based alignment (Ssyn) and co-training-based (Scotr) algorithms. The resulting 600 paraphrase pairs were mixed and presented in random order to four human judges. Each judge was asked to assess the correctness of 150 paraphrase pairs (75 pairs from each system) based on the context, i.e., the sentence group, from which the paraphrase pair was extracted. Judges were given three choices: “Correct”, for perfect paraphrases, “Partially correct”, for paraphrases in which there is only a partial overlap between the meaning of two paraphrases (e.g. while {saving set, aid package} is a correct paraphrase pair in the given context, {set, aide package} is considered partially correct), and “Incorrect”. The results of the evaluation are presented in Table 1. Although the four evaluators were judging four different sets, each clearly rated a higher percentage of the outputs produced by the syntax-based alignment algorithm as “Correct”. We should note that there are parameters specific to the co-training algorithm that we did not tune to work for this particular corpus. In addition, the cotraining algorithm recovered more paraphrase pairs: the syntax-based algorithm extracted 8666 pairs in total with 1051 of them extracted at least twice (i.e. more or less reliable), while the numbers for the co-training algorithm is 2934 out of a total of 16993 pairs. This means we are not comparing the accuracy on the same recall level. Aside from evaluating the correctness of the paraphrases, we are also interested in the degree of overlap between the paraphrase pairs discovered by the two algorithms so different from each other. We find that out of the 1051 paraphrase pairs that were extracted from more than one sentence group by the syntax-based algorithm, 62.3% were also extracted by the co-training algorithm; and out of the 2934 paraphrase pairs from the results of co-training algorithm, 33.4% were also extracted by the syntax-based algorithm. This shows that in spite of the very different cues the two different algorithms rely on, they do discover a lot of common pairs. In order to (roughly) estimate the recall (of lexical synonyms) of our algorithm, we use the synonymy relation in WordNet to extract all the synonym pairs present in our corpus. This extraction process yields the list of all WordNet-consistent synonym pairs that are present in our data. (Note that some of the pairs identified as synonyms by WordNet, like “follow/be”, are not really synonyms in the contexts defined in our data set, which may lead to artificial deflation of our recall estimate.) Once we have the list of WordNet-consistent paraphrases, we can check how many of them are recovered by our method. Table 2 gives the percentage of pairs recovered for each range of average sentence length (ASL) in the group. Not surprisingly, we get higher recall with shorter sentences, since long sentences tend to differ in their syntactic structures fairly high up in the parse trees, which leads to fewer mergings at the lexical level. The recall on the task of extracting lexical synonyms, as defined by WordNet, is not high. But after all, this is not what our algorithm has been designed for. It’s worth noticing that the syntax-based algorithm also picks up many paraphrases that are not identified as synonyms in WordNet. Out of 3217 lexical paraphrases that are learned by our system, only 493 (15.3%) are WordNet synonyms, which suggests that paraphrasing is a much richer and looser relation than synonymy. However, the WordNetbased recall figures suggest that WordNet can be used as an additional source of information to be exploited by our algorithm. We noted before that apart from being a natural representation of paraphrases, the FSAs that we build have their own merit and deserve to be evaluated directly. Since our FSAs contain large numbers of paths, we design automatic evaluation metrics to assess their qualities. If we take our claims seriously, each path in our FSAs that connects the start and end nodes should correspond to a well-formed sentence. We are interested in both quantity (how many sentences our automata are able to produce) and quality (how good these sentences are). To answer the first question, we simply count the number of paths produced by our FSAs. Table 3 gives the statistics on the number of paths produced by our FSAs, reported by the average length of sentences in the input sentence groups. For example, the sentence groups that have between 10 and 20 words produce, on average, automata that can yield 4468 alternative, semantically equivalent formulations. Note that if we always get the same degree of merging per word across all sentence groups, the number of paths would tend to increase with the sentence length. This is not the case here. Apparently we are getting less merging with longer sentences. But still, given 11 sentences, we are capable of generating hundreds, thousands, and in some cases even millions of sentences. Obviously, we should not get too happy with our ability to boost the number of equivalent meanings if they are incorrect. To assess the quality of the FSAs generated by our algorithm, we use a language model-based metric. We train a 4-gram model over one year of the Wall Street Journal using the CMU-Cambridge Statistical Language Modeling toolkit (v2). For each sentence group SG, we use this language model to estimate the average entropy of the 11 original sentences in that group (ent(SG)). We also compute the average entropy of all the sentences in the corresponding FSA built by our syntax-based algorithm (ent(FSA)). As the statistics in Table 4 show, there is little difference between the average entropy of the original sentences and the average entropy of the paraphrase sentences we produce. To better calibrate this result, we compare it with the average entropy of 6 corresponding machine translation outputs (ent(MTS)), which were also made available by LDC in conjunction with the same corpus. As one can see, the difference between the average entropy of the machine produced output and the average entropy of the original 11 sentences is much higher than the difference between the average entropy of the FSA-produced outputs and the average entropy of the original 11 sentences. Obviously, this does not mean that our FSAs only produce well-formed sentences. But it does mean that our FSAs produce sentences that look more like human produced sentences than machine produced ones according to a language model. Not surprisingly, the language model we used in Section 4.2.1 is far from being a perfect judge of sentence quality. Recall the example of “bad” path we gave in Section 1: the battle of last week’s fighting took at least 12 people lost their people died in the fighting last week’s fighting. Our 4-gram based language model will not find any fault with this sentence. Notice, however, that some words (such as “fighting” and “people”) appear at least twice in this path, although they are not repeated in any of the source sentences. These erroneous repetitions indicate mis-alignment. By measuring the frequency of words that are mistakenly repeated, we can now examine quantitatively whether a direct application of the MSA algorithm suffers from different constituent orderings as we expected. For each sentence group, we get a list of words that never appear more than once in any sentence in this group. Given a word from this list and the FSA built from this group, we count the total number of paths that contain this word (C) and the number of paths in which this word appears at least twice (CT, i.e. number of erroneous repetitions). We define the repetition ratio to be CT/C, which is the proportion of “bad” paths in this FSA according to this word. If we compute this ratio for all the words in the lists of the first 499 groups2 and the corresponding FSAs produced by an instantiation of the MSA algorithm3, the average repetition ratio is 0.0304992 (14.76% of the words have a non-zero repetition ratio, and the average ratio for these words is 0.206671). In comparison, the average repetition ratio for our algorithm is 0.0035074 (2.16% of the words have a non-zero repetition ratio4, and the average ratio for these words is 0.162309). The presence of different constituent orderings does pose a more serious problem to the MSA algorithm. Recently, Papineni et al. (2002) have proposed an automatic MT system evaluation technique (the BLEU score). Given an MT system output and a set of reference translations, one can estimate the “goodness” of the MT output by measuring the n-gram overlap between the output and the reference set. The higher the overlap, i.e., the closer an output string is to a set of reference translations, the better a translation it is. We hypothesize that our FSAs provide a better representation against which the outputs of MT systems can be evaluated because they encode not just a few but thousands of equivalent semantic formulations of the desired meaning. Ideally, if the FSAs we build accept all and only the correct renderings of a given meaning, we can just give a test sentence to the reference FSA and see if it is accepted by it. Since this is not a realistic expectation, we measure the edit distance between a string and an FSA instead: the smaller this distance is, the closer it is to the meaning represented by the FSA. To assess whether our FSAs are more appropriate representations for evaluating the output of MT systems, we perform the following experiment. For each sentence group, we hold out one sentence as test sentence, and try to evaluate how much of it can be predicted from the other 10 sentences. We compare two different ways of estimating the predictive power. (a) we compute the edit distance between the test sentence and the other 10 sentences in the set. The minimum of this distance is ed(input). (b) we use dynamic programming to efficiently compute the minimum distance (ed(FSA)) between the test sentence and all the paths in the FSA built from the other 10 sentences. The smaller the edit distance is, the better we are predicting a test sentence. Mathematically, the difference between these two measures ed(input) − ed(FSA) characterizes how much is gained in predictive power by building the FSA. We carry out the experiment described above in a “leave-one-out” fashion (i.e. each sentence serves as a test sentence once). Now let edgain be the average of ed(input) − ed(FSA) over the 11 runs for a given group. We compute this for all 899 groups and find the mean for edgain to be 0.91 (std. dev = 0.78). Table 5 gives the count for groups whose edgain falls into the specified range. We can see that the majority of edgain falls under 2. We are also interested in the relation between the predictive power of the FSAs and the number of reference translations they are derived from. For a given group, we randomly order the sentences in it, set the last one as the test sentence, and try to predict it with the first 1, 2, 3, ... 10 sentences. We investigate whether more sentences Let ed(FSAn) be the edit distance from the test sentence to the FSA built on the first n sentences; similarly, let ed(inputn) be the minimum edit distance from the test sentence to an input set that consists of only the first n sentences. Table 6 reports the effect of using different number of reference translations. The first column shows that each translation is contributing to the predictive power of our FSA. Even when we add the tenth translation to our FSA, we still improve its predictive power. The second column shows that the more sentences we add to the FSA the larger the difference between its predictive power and that of a simple set. The results in Table 6 suggest that our FSA may be used in order to refine the BLEU metric (Papineni et al., 2002). In this paper, we presented a new syntax-based algorithm that learns paraphrases from a newly available dataset. The multiple translation corpus that we use in this paper is the first instance in a series of similar corpora that are built and made publicly available by LDC in the context of a series of DARPA-sponsored MT evaluations. The algorithm we proposed constructs finite state representations of paraphrases that are useful in many contexts: to induce large lists of lexical and structural paraphrases; to generate semantically equivalent renderings of a given meaning; and to estimate the quality of machine translation systems. More experiments need to be carried out in order to assess extrinsically whether the FSAs we produce can be used to yield higher agreement scores between human and automatic assessments of translation quality. In our future work, we wish to experiment with more flexible merging algorithms and to integrate better the top-down and bottom-up processes that are used to induce FSAs. We also wish to extract more abstract paraphrase patterns from the current representation. Such patterns are more likely to get reused – which would help us get reliable statistics for them in the extraction phase, and also have a better chance of being applicable to unseen data. We thank Hal Daum´e III, Ulrich Germann, and Ulf Hermjakob for help and discussions; Eric Breck, Hubert Chen, Stephen Chong, Dan Kifer, and Kevin O’Neill for participating in the human evaluation; and the Cornell NLP group and the reviewers for their comments on this paper. We especially want to thank Regina Barzilay and Lillian Lee for many valuable suggestions and help at various stages of this work. Portions of this work were done while the first author was visiting Information Sciences Institute. This work was supported by the Advanced Research and Development Activity (ARDA)’s Advance Question Answering for Intelligence (AQUAINT) Program under contract number MDA908-02-C-0007, the National Science Foundation under ITR/IM grant IIS0081334 and a Sloan Research Fellowship to Lillian Lee. Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the National Science Foundation or the Sloan Foundation.</td>\n",
              "      <td>We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets. These FSAs are good representations of paraphrases. They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets. Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations. We describe a syntax-based algorithm that builds word lattices from parallel translations which can be used to generate new para phrases. We propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Assigning Time-Stamps To Event-Clauses</td>\n",
              "      <td>We describe a procedure for arranging into a time-line the contents of news stories describing the development of some situation. We describe the parts of the system that deal with 1. breaking sentences into event-clauses and 2. resolving both explicit and implicit temporal references. Evaluations show a performance of 52%, compared to humans. Linguists who have analyzed news stories (Schokkenbroek,1999; Bell,1997; Ohtsuka and Brewer,1992, etc.) noticed that “narratives1 are about more than one event and these events are temporally ordered. Though it seems most logical to recapitulate events in the order in which they happened, i.e. in chronological order, the events are often presented in a different sequence”. The same paper states that “it is important to reconstruct the underlying event order2 for narrative analysis to assign meaning to the sequence in which the events are narrated at the level of discourse structure....If the underlying event structure cannot be reconstructed, it may well be impossible to understand the narrative at all, let alone assign meaning to its structure”. Several psycholinguistic experiments show the influence of event-arrangement in news stories on the ease of comprehension by readers. Duszak (1991) had readers reconstruct a news story from the randomized sentences. According to his experiments readers have a default strategy by which—in the absence of cues to the contrary—they re-impose chronological order on events in the discourse. The problem of reconstructing the chronological order of events becomes more complicated if we have to deal with separate news stories, written at different times and describing the development of some situation, as is the case for multidocument summarization. By judicious definition, one can make this problem easy or hard. Selecting only specific items to assign time-points to, and then measuring correctness on them alone, may give high performance but leave much of the text unassigned. We address the problem of assigning a time-point to every clause in the text. Our approach is to break the news stories into their constituent events and to assign timestamps—either time-points or time-intervals—to these events. When assigning time-stamps we analyze both implicit time references (mainly through the tense system) and explicit ones (temporal adverbials) such as ‘on Monday’, ‘in 1998’, etc. The result of the work is a prototype program which takes as input set of news stories broken into separate sentences and produces as output a text that combines all the events from all the articles, organized in chronological order. As data we used a set of news stories about an earthquake in Afghanistan that occurred at the end of May in 1998. These news stories were taken from CNN, ABC, and APW websites for the DUC-2000 meeting. The stories were all written within one week. Some of the texts were written on the same day. In addition to a description of the May earthquake, these texts contain references to another earthquake that occurred in the same region in February 1998. To divide sentences into event-clauses we use CONTEX (Hermjakob, 1997), a parser that produces a syntactic parse tree augmented with semantic labels. CONTEX uses machine learning techniques to induce a grammar from a given treebanks. A sample output of CONTEX is given in Appendix 1. To divide a sentence into event-clauses the parse tree output by CONTEX is analyzed from left to right (root to leaf). The ::CAT field for each node provides the necessary information about whether the node under consideration forms a part of its upper level event or whether it introduces a new event. ::CAT features that indicate new events are: S-CLAUSE, S-SNT, SSUB-CLAUSE, S-PART-CLAUSE, S-RELCLAUSE. These features mark clauses which contain both subject (one or several NPs) and predicate (VP containing one or several verbs). The above procedure classifies a clause containing more than one verb as a simple clause. Such clauses are treated as one event and only one time-point will be assigned to them. This is fine when the second verb is used in the same tense as the first, but may be wrong in some cases, as in He lives in this house now and will stay here for one more year. There are no such clauses in the analyzed data, so we ignore this complication for the present. The parse tree also gives information about the tense of verbs, used later for time assignment. In order to facilitate subsequent processing, we wish to rephrase relative clauses as full independent sentences. We therefore have to replace pronouns where it is possible by their antecedents. Very often the parser gives information about the referential antecedents (in the example below, Russia). Therefore we introduced the rule: if it is possible to identify the referent, put it into the event-clause: Here the antecedent for which is identified as the relief, and gives which &lt;the relief&gt; was costing lives instead of which &lt;poor coordination&gt; was costing lives. Fortunately, in most cases our rule works correctly. Although the event-identifier works reasonably well, breaking text into event-clauses needs further investigation. Table 1 shows the performance of the system. Two kinds of mistakes are made by the event identifier: those caused by CONTEX (it does not identify clauses with omitted predicate, etc.) and those caused by the fact that our clause identifier does too shallow analysis of the parse tree. According to (Bell, 1997) “time is expressed at different levels—in the morphology and syntax of the verb phrase, in time adverbials whether lexical or phrasal, and in the discourse structure of the stories above the sentence”. For the present work we use slightly modified time representations suggested in (Allen, 1991). Formats used for time representation: We use two anchoring time points: We require that the first sentence for each article contains time information. For example: The date information is in bold. We denote by Ti the reference time-point for the article, where i 3 YYYY—year number, DDD—absolute number of the day within the year (1–366), W-—umber of the day in a week (1- Monday, ... 7- Saturday). If it is impossible to point out the day of the week then W is assigned 0. means that it is the time point of article i. The symbol Ti is used as a comparative time-point if the time the article was written is unknown. The information in brackets gives the exact date the article was written, which is the main anchor point for the time-stamper. The information about hours, minutes and seconds is ignored for the present. 2. Last time point assigned in the same sentence While analyzing different event-clauses within the same sentence we keep track of what time-point was most recently assigned within this sentence. If needed, we can refer to this time-point. In case the most recent time information assigned is not a date but an interval we record information about both time boundaries. When the program proceeds to the next sentence, the variable for the most recently assigned date becomes undefined. In most cases this assumption works correctly (example 5.2–5.3): The last time interval assigned for sentence 5.2 is {1998:53:0}---{1998:71:0}, which gives an approximate range of days when the previous earthquake happened. But the information in sentence 5.3 is about the recent earthquake and not about the previous one of 3 months earlier, which is why it would be a mistake to point Monday and Tuesday within that range. Mani and Wilson (2000) point out “over half of the errors [made by his time-stamper] were due to propagation of spreading of an incorrect event time to neighboring events”. The rule of dropping the most recently assigned date as an anchor point when proceeding to the next sentence very often helps us to avoid this problem. There are however cases where dropping the most recent time as an anchor when proceeding to the next sentence causes errors: It is clear that sentence 4.9 is the continuation of sentence 4.8 and refers to the same time point (February earthquake). In this case our rule assigns the wrong time to 4.9.1. Still we retain this rule because it is more frequently correct than incorrect. First, the text divided into event-clauses is run through a program that extracts all the date-stamps (made available by Kevin Knight, ISI). In most cases this program does not miss any date-stamps and extracts only the correct ones. The only cases in which it did not work properly for the texts were: Here the modal verb MAY was assumed to be the month, given that it started with a capital letter. Tuberculosis is already common in the area where people live in close quarters and have poor hygiene here the noun quarters, which in this case is used in the sense immediate contact or close range (Merriam-Webster dictionary), was assumed to be used in the sense the fourth part of a measure of time (Merriam-Webster dictionary). After extracting all the date-phrases we proceed to time assignment. When assigning a time to an event, we select the time to be either the most recently assigned date or, if the value of the most recently assigned date is undefined, to the date of the article. We use a set of rules to perform this selection. These rules can be divided into two main categories: those that work for sentences containing explicit date information, and those that work for sentences that do not. If the day-of-the-week used in the eventclause is the same as that of the article (or the most recently assigned date, if it is defined), and there no words before it could signal that the described event happened earlier or will happen later, then the time-point of the article (or the most recently assigned date, if it is defined) is assigned to this event. If before or after a day-ofthe-week there is a word/words signaling that the event happened earlier of will happen later then the time-point is assigned in accordance with this signal-word and the most recently assigned date, if it is defined. If the day-of-the-week used in the eventclause is not the same as that of the article (or the most recently assigned date, if it is defined), then if there are words pointing out that the event happened before the article was written or the tense used in the clause is past, then the time for the event-clause is assigned in accordance with this word (such words we call signal-words), or the most recent day corresponding to the current day-of-the-week is chosen. If the signal-word points out that the event will happen after the article was written or the tense used in the clause is future, then the time for the event-clause is assigned in accordance with the signal word or the closest subsequent day corresponding to the current day-of-the-week. helicopters evacuated 50 of the most seriously injured to emergency medical centers. The time for article 5 is (06/06/1998:Tuesday 15:17:00). So, the time assigned to this eventclause is: 5.3.1 {1998:151:1}, {1998:152:2}. The rules are the same as for a day-of-theweek, but in this case a time-range is assigned to the event-clause. The left boundary of the range is the first day of the month, the right boundary is the last day of the month, and though it is possible to figure out the days of weeks for these boundaries, this aspect is ignored for the present. earthquake in the same region killed 2,300 people and left thousands of people homeless. The time for article 4 is (05/30/1998:Saturday 14:41:00). So, the time assigned to this eventclause is 4.8.1 {1998:32:0}---{1998:60:0}. In the analyzed corpus there is a case where the presence of a name of month leads to a wrong time-stamping: Because of February, a wrong time-interval is assigned to clause 6.3.3, namely {1998:32:0}--{1998:60:0}. As this event-clause is a description of the latest news as compared to some figures it should have the time-point of the article. Such cases present a good possibility for the use of machine learning techniques to disambiguate between the cases where we should take into account date-phrase information and where not. We might have date-stamps where the words weeks, days, months, years are used with modifiers. For example remote mountainous area rocked three months earlier by another massive quake 5.2.4 that &lt;another massive quake&gt; claimed some 2,300 victims. In event-clause 5.2.3 the expression three months earlier is used. It is clear that to get the time for the event it is not enough to subtract 3 months from the time of the article because the above expression gives an approximate range within which this event could happen and not a particular date. For such cases we invented the following rule: For event 5.2.3 the time range will be {1998:53:0)---{1998:71:0) (the exact date of the article is {1998:152:2}). If the modifier used with weeks, days, months or years is several, then the multiplier used in (1) is equal to 2. If an event-clause does not contain any datephrase but contains one of the words ‘when’, ‘since’, ‘after’, ‘before’, etc., it might mean that this clause refers to an event, the time of which can be used as a reference point for the event under analysis. In this case we ask the user to insert the time for this reference event manually. This rule can cause problems in cases where ‘after’ or ‘before’ are used not as temporal connectors but as spatial ones, though in the analyzed texts we did not face this problem. If the current event-clause refers to a timepoint in Present/Past Perfect tense, then an openended time-interval is assigned to this event. The starting point is unknown; the end-point is either the most recently assigned date or the time-point of the article. If the current event-clause contains a verb in future tense (one of the verbs ‘shall’, ‘will’, ‘should’, ‘would’, ‘might’ is present in the clause) then the open-ended time-interval assigned to this event-clause has the starting point at either the most recently assigned date or the date of the article. Other tenses that can be identified with the help of CONTEX are Present and Past Indefinite. In the analyzed data all the verbs in Present Indefinite are given the most recently assigned date (or the date of the article). The situation with Past Indefinite is much more complicated and requires further investigation of more data. News stories usually describe the events that already took place at some time in the past, which is why even if the day when the event happened is not over, past tense is very often used for the description (this is especially noticeable for US news of European, Asian, African and Australian events). This means that very often an event-clause containing a verb in Past Indefinite Tense can be assigned the most recently assigned date (or the date of the article). It might prove useful to use machine learned rules for such cases. If there is no verb in the event-clause then the most recently assigned date (or the date of the article) is assigned to the event-clause. We ran the time-stamper program on two types of data: list of event-clauses extracted by the event identifier and list of event-clauses created manually. Tables 2 and 3 show the results. In the former case we analyzed only the correctly identified clauses. One can see that even on manually created data the performance of the time-stamper is not 100%. Why? Some errors are caused by assigning the time based on the date-phrase present in the eventclause, when this date-phrase is not an adverbial time modifier but an attribute. For example, The third event describes the May 30 earthquake but the time interval given for this event is {1998:32:0}---{1998:60:0} (i.e., the event happened in February). It might be possible to use machine learned rules to correct such cases. One more significant source of errors is the writing style: When the reader sees early this morning he or she tends to assign to this clause the time of the article, but later as seeing looked for two days, realizes that the time of the clause containing early this morning is two days earlier than the time of the article. It seems that the errors caused by the writing style can hardly be avoided. If an event happened at some time-point but according to the information in the sentence we can assign only a time-interval to this event (for example, February Earthquake) then we say that the time-interval is assigned correctly if the necessary time-point is within this time-interval After stamping all the news stories from the analyzed set, we arrange the event-clauses from all the articles into a chronological order. After doing that we obtain a new set of event-clauses which can easily be divided into two subsets–the first one containing all the references to the February earthquake, the second one containing the list of event-clauses in chronological order, describing what happened in May. Such a text where all the events are organized in a chronological order might be very helpful in multidocument summarization, where it is important to include into the final summary not only the most important information but also the most recent one. The output of the presented system gives the information about the timeorder of the events described in several documents. Several linguistic and psycholinguistic studies deal with the problem of time-arrangement of different texts. The research presented in these studies highlights many problems but does not solve them. As for computational applications of time theories, most work was done on temporal expressions that appear in scheduling dialogues (Busemann et al., 1997; Alexandresson et al., 1997). There are many constraints on temporal expressions in this domain. The most relevant prior work is (Mani and Wilson, 2000), who implemented their system on news stories, introduced rules spreading time-stamps obtained with the help of explicit temporal expressions throughout the whole article, and invented machine learning rules for disambiguating between specific and generic use of temporal expressions (for example, whether Christmas is used to denote the 25th of December or to denote some period of time around the 25th of December). They also mention a problem of disambiguating between temporal expression and proper name, as in ‘USA Today’. Bell (1997) notices “more research is needed on the effects of time structure on news comprehension. The hypothesis that the noncanonical news format does adversely affect understanding is a reasonable one on the basis of comprehension research into other narrative genres, but the degree to which familiarity with news models may mitigate these problems is unclear”. This research can greatly improve the performance of time-stamper and might lead to a list of machine learning rules for time detection. In this paper we made an attempt to not just analyze and decode temporal expressions but to apply this analysis throughout the whole text and assign time-stamps to such type of clauses, which later could be used as separate sentences in various natural language applications, for example in multidocument summarization. text number of manually number of time point percentage of number created event-clauses correctly assigned to correct manually created clauses assignment target 1 7 6 85.71 target 2 27 20 74.07 target 3 5 4 80.00 target 4 28 26 92.85 target 5 33 30 90.91 target 6 58 37 63.79 Total 158 123 77.85</td>\n",
              "      <td>We describe a procedure for arranging into a time-line the contents of news stories describing the development of some situation. We describe the parts of the system that deal with 1. breaking sentences into event-clauses and 2. resolving both explicit and implicit temporal references. Evaluations show a performance of 52%, compared to humans. We infer time values based on the most recently assigned date of the date of the article.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hedge Trimmer: A Parse-And-Trim Approach To Headline Generation</td>\n",
              "      <td>and Abstracts for Nice Summaries, In Workon Automatic Philadelphia, PA, pp. 9-14. Edmundson, H. (1969). “New methods in automatic of the 16(2). Grefenstett, G. (1998). Producing intelligent telegraphic text reduction to provide an audio scanning serfor the blind. In Notes of the AIII Spring on Intelligent Text Summarization, In this paper we present Hedge Trimmer, a HEaDline GEneration system that creates a headline for a newspaper story by removing constituents from a parse tree of the first sentence until a length threshold has been reached. Linguistically-motivated heuristics guide the choice of which constituents of a story should be preserved, and which ones should be deleted. Our focus is on headline generation for English newspaper texts, with an eye toward the production of document surrogates—for cross-language information retrieval—and the eventual generation of readable headlines from speech broadcasts. In contrast to original newspaper headlines, which are often intended only to catch the eye, our approach produces informative abstracts describing the main theme or event of the newspaper article. We claim that the construction of informative abstracts requires access to deeper linguistic knowledge, in order to make substantial improvements over purely statistical approaches. In this paper, we present our technique for producing headlines using a parse-and-trim approach based on the BBN Parser. As described in Miller et al. (1998), the BBN parser builds augmented parse trees according to a process similar to that described in Collins (1997). The BBN parser has been used successfully for the task of information extraction in the SIFT system (Miller et al., 2000). The next section presents previous work in the area of automatic generation of abstracts. Following this, we present feasibility tests used to establish the validity of an approach that constructs headlines from words in a story, taken in order and focusing on the earlier part of the story. Next, we describe the application of the parse-and-trim approach to the problem of headline generation. We discuss the linguistically-motivated heuristics we use to produce results that are headlinelike. Finally, we evaluate Hedge Trimmer by comparing it to our earlier work on headline generation, a probabilistic model for automatic headline generation (Zajic et al, 2002). In this paper we will refer to this statistical system as HMM Hedge We demonstrate the effectiveness of our linguistically-motivated approach, Hedge Trimmer, over the probabilistic model, HMM Hedge, using both human evaluation and automatic metrics. Other researchers have investigated the topic of automatic generation of abstracts, but the focus has been different, e.g., sentence extraction (Edmundson, 1969; Johnson et al, 1993; Kupiec et al., 1995; Mann et al., 1992; Teufel and Moens, 1997; Zechner, 1995), processing of structured templates (Paice and Jones, 1993), sentence compression (Hori et al., 2002; Knight and Marcu, 2001; Grefenstette, 1998, Luhn, 1958), and generation of abstracts from multiple sources (Radev and McKeown, 1998). We focus instead on the construction of headline-style abstracts from a single story. Headline generation can be viewed as analogous to statistical machine translation, where a concise document is generated from a verbose one using a Noisy Channel Model and the Viterbi search to select the most likely summarization. This approach has been explored in (Zajic et al., 2002) and (Banko et al., 2000). The approach we use in Hedge is most similar to that of (Knight and Marcu, 2001), where a single sentence is shortened using statistical compression. As in this work, we select headline words from story words in the order that they appear in the story—in particular, the first sentence of the story. However, we use linguistically motivated heuristics for shortening the sentence; there is no statistical model, which means we do not require any prior training on a large corpus of story/headline pairs. Linguistically motivated heuristics have been used by (McKeown et al, 2002) to distinguish constituents of parse trees which can be removed without affecting grammaticality or correctness. GLEANS (Daumé et al, 2002) uses parsing and named entity tagging to fill values in headline templates. Consider the following excerpt from a news story: In this case, the words in bold form a fluent and accurate headline for the story. Italicized words are deleted based on information provided in a parse-tree representation of the sentence. Our approach is based on the selection of words from the original story, in the order that they appear in the story, and allowing for morphological variation. To determine the feasibility of our headline-generation approach, we first attempted to apply our “select-wordsin-order” technique by hand. We asked two subjects to write headline headlines for 73 AP stories from the TIPSTER corpus for January 1, 1989, by selecting words in order from the story. Of the 146 headlines, 2 did not meet the “select-words-in-order” criteria because of accidental word reordering. We found that at least one fluent and accurate headline meeting the criteria was created for each of the stories. The average length of the headlines was 10.76 words. Later we examined the distribution of the headline words among the sentences of the stories, i.e. how many came from the first sentence of a story, how many from the second sentence, etc. The results of this study are shown in Figure 1. We observe that 86.8% of the headline words were chosen from the first sentence of their stories. We performed a subsequent study in which two subjects created 100 headlines for 100 AP stories from August 6, 1990. 51.4% of the headline words in the second set were chosen from the first sentence. The distribution of headline words for the second set shown in Figure 2. Although humans do not always select headline words from the first sentence, we observe that a large percentage of headline words are often found in the first sentence. The input to Hedge is a story, whose first sentence is immediately passed through the BBN parser. The parse-tree result serves as input to a linguisticallymotivated module that selects story words to form headlines based on key insights gained from our observations of human-constructed headlines. That is, we conducted a human inspection of the 73 TIPSTER stories mentioned in Section 3 for the purpose of developing the Hedge Trimmer algorithm. Based on our observations of human-produced headlines, we developed the following algorithm for parse-tree trimming: More recently, we conducted an automatic analysis of the human-generated headlines that supports several of the insights gleaned from this initial study. We parsed 218 human-produced headlines using the BBN parser and analyzed the results. For this analysis, we used 72 headlines produced by a third participant.1 The parsing results included 957 noun phrases (NP) and 315 clauses (S). We calculated percentages based on headline-level, NP-level, and Sentence-level structures in the parsing results. That is, we counted: Figure 3 summarizes the results of this automatic analysis. In our initial human inspection, we considered each of these categories to be reasonable candidates for deletion in our parse tree and this automatic analysis indicates that we have made reasonable choices for deletion, with the possible exception of trailing PPs, which show up in over half of the human-generated headlines. This suggests that we should proceed with caution with respect to the deletion of trailing PPs; thus we consider this to be an option only if no other is available. preposed adjuncts = 0/218 (0%) conjoined S = 1/218 ( .5%) conjoined VP = 7/218 (3%) relative clauses = 3/957 (.3%) determiners = 31/957 (3%); of these, only 16 were “a” or “the” (1.6% overall) S-LEVEL PERCENTAGES2 time expressions = 5/315 (1.5%) trailing PPs = 165/315 (52%) trailing SBARs = 24/315 (8%) 1 No response was given for one of the 73 stories. 2 Trailing constituents (SBARs and PPs) are computed by counting the number of SBARs (or PPs) not designated as an argument of (contained in) a verb phrase. For a comparison, we conducted a second analysis in which we used the same parser on just the first sentence of each of the 73 stories. In this second analysis, the parsing results included 817 noun phrases (NP) and 316 clauses (S). A summary of these results is shown in Figure 4. Note that, across the board, the percentages are higher in this analysis than in the results shown in Figure 3 (ranging from 12% higher—in the case of trailing PPs—to 1500% higher in the case of time expressions), indicating that our choices of deletion in the Hedge Trimmer algorithm are well-grounded. preposed adjuncts = 2/73 (2.7%) conjoined S = 3/73 (4%) conjoined VP = 20/73 (27%) relative clauses = 29/817 (3.5%) determiners = 205/817 (25%); of these, only 171 were “a” or “the” (21% overall) time expressions = 77/316 (24%) trailing PPs = 184/316 (58%) trailing SBARs = 49/316 (16%) each story. The first step relies on what is referred to as the Projection Principle in linguistic theory (Chomsky, 1981): Predicates project a subject (both dominated by S) in the surface structure. Our human-generated headlines always conformed to this rule; thus, we adopted it as a constraint in our algorithm. An example of the application of step 1 above is the following, where boldfaced material from the parse tree representation is retained and italicized material is eliminated: with government]] officials said Tuesday.] Output of step 1: Rebels agree to talks with government. When the parser produces a correct tree, this step provides a grammatical headline. However, the parser often produces an incorrect output. Human inspection of our 624-sentence DUC-2003 evaluation set revealed that there were two such scenarios, illustrated by the following cases: In the first case, an S exists, but it does not conform to the requirements of step 1. This occurred in 2.6% of the sentences in the DUC-2003 evaluation data. We resolve this by selecting the lowest leftmost S, i.e., the entire string “What started as a local controversy has evolved into an international scandal” in the example above. In the second case, there is no S available. This occurred in 3.4% of the sentences in the evaluation data. We resolve this by selecting the root of the parse tree; this would be the entire string “Bangladesh and India signed a water sharing accord” above. No other parser errors were encountered in the DUC-2003 evaluation data. Step 2 of our algorithm eliminates low-content units. We start with the simplest low-content units: the determiners a and the. Other determiners were not considered for deletion because our analysis of the humanconstructed headlines revealed that most of the other determiners provide important information, e.g., negation (not), quantifiers (each, many, several), and deictics (this, that). Beyond these, we found that the human-generated headlines contained very few time expressions which, although certainly not content-free, do not contribute toward conveying the overall “who/what content” of the story. Since our goal is to provide an informative headline (i.e., the action and its participants), the identification and elimination of time expressions provided a significant boost in the performance of our automatic headline generator. We identified time expressions in the stories using BBN’s IdentiFinderTM (Bikel et al, 1999). We implemented the elimination of time expressions as a twostep process: where X is tagged as part of a time expression The following examples illustrate the application of this step: Output of step 2: State Department lifted ban it has imposed on foreign fliers. Output of step 2: International relief agency announced that it is withdrawing from North Korea. We found that 53.2% of the stories we examined contained at least one time expression which could be deleted. Human inspection of the 50 deleted time expressions showed that 38 were desirable deletions, 10 were locally undesirable because they introduced an ungrammatical fragment,3 and 2 were undesirable because they removed a potentially relevant constituent. However, even an undesirable deletion often pans out for two reasons: (1) the ungrammatical fragment is frequently deleted later by some other rule; and (2) every time a constituent is removed it makes room under the threshold for some other, possibly more relevant constituent. Consider the following examples. Example (7) was produced by a system which did not remove time expressions. Example (8) shows that if the time expression Sunday were removed, it would make room below the 10-word threshold for another important piece of information. The final step, iterative shortening, removes linguistically peripheral material—through successive deletions—until the sentence is shorter than a given threshold. We took the threshold to be 10 for the DUC task, but it is a configurable parameter. Also, given that the human-generated headlines tended to retain earlier material more often than later material, much of our iterative shortening is focused on deleting the rightmost phrasal categories until the length is below threshold. There are four types of iterative shortening rules. The first type is a rule we call “XP-over-XP,” which is implemented as follows: In constructions of the form [XP [XP ...] ...] remove the other children of the higher XP, where XP is NP, VP or S. This is a linguistic generalization that allowed us apply a single rule to capture three different phenomena (relative clauses, verb-phrase conjunction, and sentential conjunction). The rule is applied iteratively, from the deepest rightmost applicable node backwards, until the length threshold is reached. The impact of XP-over-XP can be seen in these examples of NP-over-NP (relative clauses), VP-over-VP (verb-phrase conjunction), and S-over-S (sentential conjunction), respectively: Parse: [S [Det A] fire killed [Det a] [NP [NP firefighter] [SBAR who was fatally injured as he searched the house] ]] Output of NP-over-NP: fire killed firefighter has outpaced state laws, but the state says the company doesn’t have the proper licenses. Parse: [S [Det A] company offering blood cholesterol tests in grocery stores says [S [S medical technology has outpaced state laws], [CC but] [S [Det the] state stays [Det the] company doesn’t have [Det the] proper licenses.]] ] Output of S-over-S: Company offering blood cholesterol tests in grocery store says medical technology has outpaced state laws The second type of iterative shortening is the removal of preposed adjuncts. The motivation for this type of shortening is that all of the human-generated headlines ignored what we refer to as the preamble of the story. Assuming the Projection principle has been satisfied, the preamble is viewed as the phrasal material occurring before the subject of the sentence. Thus, adjuncts are identified linguistically as any XP unit preceding the first NP (the subject) under the S chosen by step 1. This type of phrasal modifier is invisible to the XP-over-XP rule, which deletes material under a node only if it dominates another node of the same phrasal category. The impact of this type of shortening can be seen in the following example: Parse: [S [PP According to a now-finalized blueprint described by U.S. officials and other sources] [Det the] Bush administration plans to take complete, unilateral control of [Det a] postSaddam Hussein Iraq ] Output of Preposed Adjunct Removal: Bush administration plans to take complete unilateral control of post-Saddam Hussein Iraq The third and fourth types of iterative shortening are the removal of trailing PPs and SBARs, respectively: These are the riskiest of the iterative shortening rules, as indicated in our analysis of the human-generated headlines. Thus, we apply these conservatively, only when there are no other categories of rules to apply. Moreover, these rules are applied with a backoff option to avoid over-trimming the parse tree. First the PP shortening rule is applied. If the threshold has been reached, no more shortening is done. However, if the threshold has not been reached, the system reverts to the parse tree as it was before any PPs were removed, and applies the SBAR shortening rule. If the threshold still has not been reached, the PP rule is applied to the result of the SBAR rule. Other sequences of shortening rules are possible. The one above was observed to produce the best results on a 73-sentence development set of stories from the TIPSTER corpus. The intuition is that, when removing constituents from a parse tree, it’s best to remove smaller portions during each iteration, to avoid producing trees with undesirably few words. PPs tend to represent small parts of the tree while SBARs represent large parts of the tree. Thus we try to reach the threshold by removing small constituents, but if we can’t reach the threshold that way, we restore the small constituents, remove a large constituent and resume the deletion of small constituents. The impact of these two types of shortening can be seen in the following examples: Parse: [S More oil-covered sea birds were found [PP over the weekend]] Output of PP Removal: More oil-covered sea birds were found. Parse: [S Visiting China Interpol chief expressed confidence in Hong Kong’s smooth transition [SBAR while assuring closer cooperation after Hong Kong returns]] Output of SBAR Removal: Visiting China Interpol chief expressed confidence in Hong Kong’s smooth transition We conducted two evaluations. One was an informal human assessment and one was a formal automatic evaluation. We compared our current system to a statistical headline generation system we presented at the 2001 DUC Summarization Workshop (Zajic et al., 2002), which we will refer to as HMM Hedge. HMM Hedge treats the summarization problem as analogous to statistical machine translation. The verbose language, articles, is treated as the result of a concise language, headlines, being transmitted through a noisy channel. The result of the transmission is that extra words are added and some morphological variations occur. The Viterbi algorithm is used to calculate the most likely unseen headline to have generated the seen article. The Viterbi algorithm is biased to favor headline-like characteristics gleaned from observation of human performance of the headline-construction task. Since the 2002 Workshop, HMM Hedge has been enhanced by incorporating part of speech of information into the decoding process, rejecting headlines that do not contain a word that was used as a verb in the story, and allowing morphological variation only on words that were used as verbs in the story. HMM Hedge was trained on 700,000 news articles and headlines from the TIPSTER corpus. BLEU (Papineni et al, 2002) is a system for automatic evaluation of machine translation. BLEU uses a modified n-gram precision measure to compare machine translations to reference human translations. We treat summarization as a type of translation from a verbose language to a concise one, and compare automatically generated headlines to human generated headlines. For this evaluation we used 100 headlines created for 100 AP stories from the TIPSTER collection for August 6, 1990 as reference summarizations for those stories. These 100 stories had never been run through either system or evaluated by the authors prior to this evaluation. We also used the 2496 manual abstracts for the DUC2003 10-word summarization task as reference translations for the 624 test documents of that task. We used two variants of HMM Hedge, one which selects headline words from the first 60 words of the story, and one which selects words from the first sentence of the story. Table 1 shows the BLEU score using trigrams, and the 95% confidence interval for the score. These results show that although Hedge Trimmer scores slightly higher than HMM Hedge on both data sets, the results are not statistically significant. However, we believe that the difference in the quality of the systems is not adequately reflected by this automatic evaluation. Human evaluation indicates significantly higher scores than might be guessed from the automatic evaluation. For the 100 AP stories from the TIPSTER corpus for August 6, 1990, the output of Hedge Trimmer and HMM Hedge was evaluated by one human. Each headline was given a subjective score from 1 to 5, with 1 being the worst and 5 being the best. The average score of HMM Hedge was 3.01 with standard deviation of 1.11. The average score of Hedge Trimmer was 3.72 with standard deviation of 1.26. Using a t-score, the difference is significant with greater than 99.9% confidence. The types of problems exhibited by the two systems are qualitatively different. The probabilistic system is more likely to produce an ungrammatical result or omit a necessary argument, as in the examples below. In contrast, the parser-based system is more likely to fail by producing a grammatical but semantically useless headline. Finally, even when both systems produce acceptable output, Hedge Trimmer usually produces headlines which are more fluent or include more useful information. demanding that Chinese authorities respect culture. We have shown the effectiveness of constructing headlines by selecting words in order from a newspaper story. The practice of selecting words from the early part of the document has been justified by analyzing the behavior of humans doing the task, and by automatic evaluation of a system operating on a similar principle. We have compared two systems that use this basic technique, one taking a statistical approach and the other a linguistic approach. The results of the linguistically motivated approach show that we can build a working system with minimal linguistic knowledge and circumvent the need for large amounts of training data. We should be able to quickly produce a comparable system for other languages, especially in light of current multi-lingual initiatives that include automatic parser induction for new languages, e.g. the TIDES initiative. We plan to enhance Hedge Trimmer by using a language model of Headlinese, the language of newspaper headlines (Mårdh 1980) to guide the system in which constituents to remove. We Also we plan to allow for morphological variation in verbs to produce the present tense headlines typical of Headlinese. Hedge Trimmer will be installed in a translingual detection system for enhanced display of document surrogates for cross-language question answering. This system will be evaluated in upcoming iCLEF conferences. The University of Maryland authors are supported, in part, by BBNT Contract 020124-7157, DARPA/ITO Contract N66001-97-C-8540, and NSF CISE Research Infrastructure Award EIA0130422. We would like to thank Naomi Chang and Jon Teske for generating reference headlines.</td>\n",
              "      <td>This paper presents Hedge Trimmer, a HEaDline GEneration system that creates a headline for a newspaper story using linguistically-motivated heuristics to guide the choice of a potential headline. We present feasibility tests used to establish the validity of an approach that constructs a headline by selecting words in order from a story. In addition, we describe experimental results that demonstrate the effectiveness of our linguistically-motivated approach over a HMM-based model, using both human evaluation and automatic metrics for comparing the two approaches. Our approach focuses on extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Discriminative Training And Maximum Entropy Models For Statistical Machine Translation</td>\n",
              "      <td>We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is significantly improved using this approach. We are given a source (‘French’) sentence fJ1 = f1, ... , fj, ... , fJ, which is to be translated into a target (‘English’) sentence eI1 = e1, ... , ei, ... , eI. Among all possible target sentences, we will choose the sentence with the highest probability:1 The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. According to Bayes’ decision rule, we can equivalently to Eq. 1 perform the following maximization: This approach is referred to as source-channel approach to statistical MT. Sometimes, it is also referred to as the ‘fundamental equation of statistical MT’ (Brown et al., 1993). Here, Pr(eI1) is the language model of the target language, whereas Pr(fJ1 |eI1) is the translation model. Typically, Eq. 2 is favored over the direct translation model of Eq. 1 with the argument that it yields a modular approach. Instead of modeling one probability distribution, we obtain two different knowledge sources that are trained independently. The overall architecture of the source-channel approach is summarized in Figure 1. In general, as shown in this figure, there may be additional transformations to make the translation task simpler for the algorithm. Typically, training is performed by applying a maximum likelihood approach. If the language model Pr(eI1) = pγ(eI1) depends on parameters γ and the translation model Pr(fJ1 |eI1) = pθ(fJ1 |eI1) depends on parameters θ, then the optimal parameter values are obtained by maximizing the likelihood on a parallel training corpus fS1 , eS1 (Brown et al., 1993): We obtain the following decision rule: instead of Eq. 5 (Och et al., 1999): State-of-the-art statistical MT systems are based on this approach. Yet, the use of this decision rule has various problems: Here, we replaced pˆ�(fJ1 |ei) by pˆ�(ei|fJ1 ). From a theoretical framework of the sourcechannel approach, this approach is hard to justify. Yet, if both decision rules yield the same translation quality, we can use that decision rule which is better suited for efficient search. As alternative to the source-channel approach, we directly model the posterior probability Pr(ei|fJ1 ). An especially well-founded framework for doing this is maximum entropy (Berger et al., 1996). In this framework, we have a set of M feature functions hm(ei, fJ1 ), m = 1, ... , M. For each feature function, there exists a model parameter am, m = 1, ... , M. The direct translation probability is given the following two feature functions: This approach has been suggested by (Papineni et al., 1997; Papineni et al., 1998) for a natural language understanding task. We obtain the following decision rule: Hence, the time-consuming renormalization in Eq. 8 is not needed in search. The overall architecture of the direct maximum entropy models is summarized in Figure 2. Interestingly, this framework contains as special case the source channel approach (Eq. 5) if we use and set A1 = A2 = 1. Optimizing the corresponding parameters A1 and A2 of the model in Eq. 8 is equivalent to the optimization of model scaling factors, which is a standard approach in other areas such as speech recognition or pattern recognition. The use of an ‘inverted’ translation model in the unconventional decision rule of Eq. 6 results if we use the feature function log Pr(eI1|fJ1 ) instead of log Pr(fJ1 |eI1). In this framework, this feature can be as good as log Pr(fJ1 |eI1). It has to be empirically verified, which of the two features yields better results. We even can use both features log Pr(eI1|fJ1 ) and log Pr(fJ1 |eI1), obtaining a more symmetric translation model. As training criterion, we use the maximum class posterior probability criterion: This corresponds to maximizing the equivocation or maximizing the likelihood of the direct translation model. This direct optimization of the posterior probability in Bayes decision rule is referred to as discriminative training (Ney, 1995) because we directly take into account the overlap in the probability distributions. The optimization problem has one global optimum and the optimization criterion is convex. Typically, the probability Pr(fJ1 |eI1) is decomposed via additional hidden variables. In statistical alignment models Pr(fJ1 , aJ1 |eI1), the alignment aJ1 is introduced as a hidden variable: As specific MT method, we use the alignment template approach (Och et al., 1999). The key elements of this approach are the alignment templates, which are pairs of source and target language phrases together with an alignment between the words within the phrases. The advantage of the alignment template approach compared to single word-based statistical translation models is that word context and local changes in word order are explicitly considered. The alignment template model refines the translation probability Pr(fJ1 |eI1) by introducing two hidden variables zK1 and aK1 for the K alignment templates and the alignment of the alignment templates: The alignment mapping is j → i = aj from source position j to target position i = aj. Search is performed using the so-called maximum approximation: Hence, the search space consists of the set of all possible target language sentences eI1 and all possible alignments aJ1 . Generalizing this approach to direct translation models, we extend the feature functions to include the dependence on the additional hidden variable. Using M feature functions of the form hm(eI1, fJ1 , aJ1), m = 1, ... , M, we obtain the following model: Obviously, we can perform the same step for translation models with an even richer structure of hidden variables than only the alignment aJ1 . To simplify the notation, we shall omit in the following the dependence on the hidden variables of the model. Hence, we obtain three different probability distributions: Pr(aK1 |eI1), Pr(zK1 |aK1 , eI1) and Pr(fJ1 |zK1 ,aK1 ,eI1). Here, we omit a detailed description of modeling, training and search, as this is not relevant for the subsequent exposition. For further details, see (Och et al., 1999). To use these three component models in a direct maximum entropy approach, we define three different feature functions for each component of the translation model instead of one feature function for the whole translation model p(fJ1 |eI1). The feature functions have then not only a dependence on fJ1 and eI1 but also on zK1 , aK1 . So far, we use the logarithm of the components of a translation model as feature functions. This is a very convenient approach to improve the quality of a baseline system. Yet, we are not limited to train only model scaling factors, but we have many possibilities: This corresponds to a word penalty for each produced target word. • We could use grammatical features that relate certain grammatical dependencies of source and target language. For example, using a function k(·) that counts how many verb groups exist in the source or the target sentence, we can define the following feature, which is 1 if each of the two sentences contains the same number of verb groups: In the same way, we can introduce semantic features or pragmatic features such as the dialogue act classification. We can use numerous additional features that deal with specific problems of the baseline statistical MT system. In this paper, we shall use the first three of these features. As additional language model, we use a class-based five-gram language model. This feature and the word penalty feature allow a straightforward integration into the used dynamic programming search algorithm (Och et al., 1999). As this is not possible for the conventional dictionary feature, we use n-best rescoring for this feature. To train the model parameters λM1 of the direct translation model according to Eq. 11, we use the GIS (Generalized Iterative Scaling) algorithm (Darroch and Ratcliff, 1972). It should be noted that, as was already shown by (Darroch and Ratcliff, 1972), by applying suitable transformations, the GIS algorithm is able to handle any type of real-valued features. To apply this algorithm, we have to solve various practical problems. The renormalization needed in Eq. 8 requires a sum over a large number of possible sentences, for which we do not know an efficient algorithm. Hence, we approximate this sum by sampling the space of all possible sentences by a large set of highly probable sentences. The set of considered sentences is computed by an appropriately extended version of the used search algorithm (Och et al., 1999) computing an approximate n-best list of translations. Unlike automatic speech recognition, we do not have one reference sentence, but there exists a number of reference sentences. Yet, the criterion as it is described in Eq. 11 allows for only one reference translation. Hence, we change the criterion to allow Rs reference translations es,1, ... , es,Rs for the sentence es: We use this optimization criterion instead of the optimization criterion shown in Eq. 11. In addition, we might have the problem that no single of the reference translations is part of the nbest list because the search algorithm performs pruning, which in principle limits the possible translations that can be produced given a certain input sentence. To solve this problem, we define for maximum entropy training each sentence as reference translation that has the minimal number of word errors with respect to any of the reference translations. We present results on the VERBMOBIL task, which is a speech translation task in the domain of appointment scheduling, travel planning, and hotel reservation (Wahlster, 1993). Table 1 shows the corpus statistics of this task. We use a training corpus, which is used to train the alignment template model and the language models, a development corpus, which is used to estimate the model scaling factors, and a test corpus. So far, in machine translation research does not exist one generally accepted criterion for the evaluation of the experimental results. Therefore, we use a large variety of different criteria and show that the obtained results improve on most or all of these criteria. In all experiments, we use the following six error criteria: of the target sentence, so that the WER measure alone could be misleading. To overcome this problem, we introduce as additional measure the position-independent word error rate (PER). This measure compares the words in the two sentences ignoring the word order. more detailed analysis, subjective judgments by test persons are necessary. Each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0 (NieBen et al., 2000). • IER (information item error rate): The test sentences are segmented into information items. For each of them, if the intended information is conveyed and there are no syntactic errors, the sentence is counted as correct (NieBen et al., 2000). In the following, we present the results of this approach. Table 2 shows the results if we use a direct translation model (Eq. 6). As baseline features, we use a normal word trigram language model and the three component models of the alignment templates. The first row shows the results using only the four baseline features with λ1 = · · · = λ4 = 1. The second row shows the result if we train the model scaling factors. We see a systematic improvement on all error rates. The following three rows show the results if we add the word penalty, an additional class-based five-gram GIS algorithm for maximum entropy training of alignment templates. language model and the conventional dictionary features. We observe improved error rates for using the word penalty and the class-based language model as additional features. Figure 3 show how the sentence error rate (SER) on the test corpus improves during the iterations of the GIS algorithm. We see that the sentence error rates converges after about 4000 iterations. We do not observe significant overfitting. Table 3 shows the resulting normalized model scaling factors. Multiplying each model scaling factor by a constant positive value does not affect the decision rule. We see that adding new features also has an effect on the other model scaling factors. The use of direct maximum entropy translation models for statistical machine translation has been suggested by (Papineni et al., 1997; Papineni et al., 1998). They train models for natural language understanding rather than natural language translation. In contrast to their approach, we include a dependence on the hidden variable of the translation model in the direct translation model. Therefore, we are able to use statistical alignment models, which have been shown to be a very powerful component for statistical machine translation systems. In speech recognition, training the parameters of the acoustic model by optimizing the (average) mutual information and conditional entropy as they are defined in information theory is a standard approach (Bahl et al., 1986; Ney, 1995). Combining various probabilistic models for speech and language modeling has been suggested in (Beyerlein, 1997; Peters and Klakow, 1999). We have presented a framework for statistical MT for natural languages, which is more general than the widely used source-channel approach. It allows a baseline MT system to be extended easily by adding new feature functions. We have shown that a baseline statistical MT system can be significantly improved using this framework. There are two possible interpretations for a statistical MT system structured according to the sourcechannel approach, hence including a model for Pr(ei) and a model for Pr(fi Iei). We can interpret it as an approximation to the Bayes decision rule in Eq. 2 or as an instance of a direct maximum entropy model with feature functions log Pr(ei) and log Pr(fi |ei). As soon as we want to use model scaling factors, we can only do this in a theoretically justified way using the second interpretation. Yet, the main advantage comes from the large number of additional possibilities that we obtain by using the second interpretation. An important open problem of this approach is the handling of complex features in search. An interesting question is to come up with features that allow an efficient handling using conventional dynamic programming search algorithms. In addition, it might be promising to optimize the parameters directly with respect to the error rate of the MT system as is suggested in the field of pattern and speech recognition (Juang et al., 1995; Schl¨uter and Ney, 2001).</td>\n",
              "      <td>We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is significantly improved using this approach.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21debac4-c414-4f2a-9cc9-0fac346f8740')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-21debac4-c414-4f2a-9cc9-0fac346f8740 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-21debac4-c414-4f2a-9cc9-0fac346f8740');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-235ce9c0-ccc3-4198-8ddf-4098247fc4a1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-235ce9c0-ccc3-4198-8ddf-4098247fc4a1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-235ce9c0-ccc3-4198-8ddf-4098247fc4a1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_processed",
              "summary": "{\n  \"name\": \"df_processed\",\n  \"rows\": 1009,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 998,\n        \"samples\": [\n          \"Applied Text Generation\",\n          \"Text Segmentation Based On Similarity Between Words\",\n          \"Exploiting Diverse Knowledge Sources Via Maximum Entropy In Named Entity Recognition\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1008,\n        \"samples\": [\n          \"present a statistical machine translation model that uses that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations. Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation. We describe our system\\u2019s training and decoding methods in detail, and evaluate it for translation speed and translation accuracy. Using BLEU as a metric of translation accuracy, we find that our system performs significantly better than the Alignment Template System, a state-of-the-art phrasebased system. We present a statistical machine translation model that uses hierarchical phrases\\u2014phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations. Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation. We describe our system\\u2019s training and decoding methods in detail, and evaluate it for translation speed and translation accuracy. Using BLEU as a metric of translation accuracy, we find that our system performs significantly better than the Alignment Template System, a state-of-the-art phrasebased system. The alignment template translation model (Och and Ney 2004) and related phrase-based models advanced the state of the art in machine translation by expanding the basic unit of translation from words to phrases, that is, substrings of potentially unlimited size (but not necessarily phrases in any syntactic theory). These phrases allow a model to learn local reorderings, translations of multiword expressions, or insertions and deletions that are sensitive to local context. This makes them a simple and powerful mechanism for translation. The basic phrase-based model is an instance of the noisy-channel approach (Brown et al. 1993). Following convention, we call the source language \\u201cFrench\\u201d and the target language \\u201cEnglish\\u201d; the translation of a French sentence f into an English sentence e is modeled as: The phrase-based translation model P( f  |e) \\u201cencodes\\u201d e into f by the following steps: Other phrase-based models model the joint distribution P(e,f ) (Marcu and Wong 2002) or make P(e) and P(f I e) into features of a log-linear model (Och and Ney 2002). But the basic architecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same. Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training. But Koehn, Och, and Marcu (2003) find that phrases longer than three words improve performance little for training corpora of up to 20 million words, suggesting that the data may be too sparse to learn longer phrases. Above the phrase level, some models perform no reordering (Zens and Ney 2004; Kumar, Deng, and Byrne 2006), some have a simple distortion model that reorders phrases independently of their content (Koehn, Och, and Marcu 2003; Och and Ney 2004), and some, for example, the Alignment Template System (Och et al. 2004; Thayer et al. 2004), hereafter ATS, and the IBM phrase-based system (Tillmann 2004; Tillmann and Zhang 2005), have phrase-reordering models that add some lexical sensitivity. But, as an illustration of the limitations of phrase reordering, consider the following Mandarin example and its English translation: M1)11'1 \\ufffd \\u00c6 JLF01 P \\ufffd\\ufffd n \\ufffd\\ufffd 0* \\ufffdAozhou shi yu Beihan you bangjiao de shaoshu guojia zhiyi Australia is with North Korea have dipl. rels. that few countries one of . . Australia is one of the few countries that have diplomatic relations with North Korea. If we count zhiyi (literally, \\u2018of-one\\u2019) as a single token, then translating this sentence correctly into English requires identifying a sequence of five word groups that need to be reversed. When we run a phrase-based system, ATS, on this sentence (using the experimental setup described herein), we get the following phrases with translations: [Aozhou] [shi]1 [yu Beihan]2 [you] [bangjiao] [de shaoshu guojia zhiyi] [.] [Australia] [has] [dipl. rels.] [with North Korea]2 [is]1 [one of the few countries] [.] where we have used subscripts to indicate the reordering of phrases. The phrase-based model is able to order \\u201chas diplomatic relations with North Korea\\u201d correctly (using phrase reordering) and \\u201cis one of the few countries\\u201d correctly (using a combination of phrase translation and phrase reordering), but does not invert these two groups as it should. We propose a solution to these problems that does not interfere with the strengths of the phrase-based approach, but rather capitalizes on them: Because phrases are good for learning reorderings of words, we can use them to learn reorderings of phrases as well. In order to do this we need hierarchical phrases that can contain other phrases. For example, a hierarchical phrase pair that might help with the above example is (yu 1 you 2 , have 2 with 1 ) (3) where 1 and 2 are placeholders for subphrases (Chiang 2005). This would capture the fact that Chinese prepositional phrases almost always modify verb phrases on the left, whereas English prepositional phrases usually modify verb phrases on the right. Because it generalizes over possible prepositional objects and direct objects, it acts both as a discontinuous phrase pair and as a phrase-reordering rule. Thus it is considerably more powerful than a conventional phrase pair. Similarly, the hierarchical phrase pair ( 1 de 2 , the 2 that 1 ) (4) would capture the fact that Chinese relative clauses modify NPs on the left, whereas English relative clauses modify on the right; and the pair ( 1 zhiyi, one of 1 ) (5) would render the construction zhiyi in English word order. These three rules, along with some conventional phrase pairs, suffice to translate the sentence correctly: [Aozhou] [shi] [[[yu [Beihan]1 you [bangjiao]2] de [shaoshu guojia]3] zhiyi] [Australia] [is] [one of [the [few countries]3 that [have [dipl. rels. ]2 with [N. Korea]1]]] The system we describe in this article uses rules like (3), (4), and (5), which we formalize in the next section as rules of a synchronous context-free grammar (CFG).1 Moreover, the system is able to learn them automatically from a parallel text without syntactic annotation. Because our system uses a synchronous CFG, it could be thought of as an example of syntax-based statistical machine translation (MT), joining a line of research (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001) that has been fruitful but has not previously produced systems that can compete with phrase-based systems in large-scale translation tasks such as the evaluations held by NIST. Our approach differs from early syntax-based statistical translation models in combining the idea of hierarchical structure with key insights from phrase-based MT: Crucially, by incorporating the use of elementary structures with possibly many words, we hope to inherit phrase-based MT\\u2019s capacity for memorizing translations from parallel data. Other insights borrowed from the current state of the art include minimum-error-rate training of log-linear models (Och and Ney 2002; Och 2003) and use of an m-gram language model. The conjunction of these various elements presents a considerable challenge for implementation, which we discuss in detail in this article. The result is the first system employing a grammar (to our knowledge) to perform better than phrase-based systems in large-scale evaluations.2 Approaches to syntax-based statistical MT have varied in their reliance on syntactic theories, or annotations made according to syntactic theories. At one extreme are those, exemplified by that of Wu (1997), that have no dependence on syntactic theory beyond the idea that natural language is hierarchical. If these methods distinguish between different categories, they typically do not distinguish very many. Our approach, as presented here, falls squarely into this family. By contrast, other approaches, exemplified by that of Yamada and Knight (2001), do make use of parallel data with syntactic annotations, either in the form of phrase-structure trees or dependency trees (Ding and Palmer 2005; Quirk, Menezes, and Cherry 2005). Because syntactically annotated corpora are comparatively small, obtaining parsed parallel text in quantity usually entails running an automatic parser on a parallel corpus to produce noisy annotations. Both of these strands of research have recently begun to explore extraction of larger rules, guided by word alignments. The extraction method we use, which is a straightforward generalization of phrase extraction from word-aligned parallel text, has been independently proposed before in various settings. The method of Block (2000) is the earliest instance we are aware of, though it is restricted to rules with one variable. The same method has also been used by Probst et al. (2002) and Xia and McCord (2004) in conjunction with syntactic annotations to extract rules that are used for reordering prior to translation. Finally, Galley et al. (2004) use the same method to extract a very large grammar from syntactically annotated data. The discontinuous phrases used by Simard et al. (2005) have a similar purpose to synchronous grammar rules; but they have variables that stand for single words rather than subderivations, and they can interleave in non-hierarchical ways. The model is based on a synchronous CFG, elsewhere known as a syntax-directed transduction grammar (Lewis and Stearns 1968). We give here an informal definition and then describe in detail how we build a synchronous CFG for our model. In a synchronous CFG the elementary structures are rewrite rules with aligned pairs of right-hand sides: where X is a nonterminal, \\u03b3 and \\u03b1 are both strings of terminals and nonterminals, and \\u2014 is a one-to-one correspondence between nonterminal occurrences in \\u03b3 and nonterminal occurrences in \\u03b1. For example, the hierarchical phrase pairs (3), (4), and (5) previously presented could be formalized in a synchronous CFG as: where we have used boxed indices to indicate which nonterminal occurrences are linked by \\u2014. The conventional phrase pairs would be formalized as: A synchronous CFG derivation begins with a pair of linked start symbols. At each step, two linked nonterminals are rewritten using the two components of a single rule. When denoting links with boxed indices, we must consistently reindex the newly introduced symbols apart from the symbols already present. For an example using these rules, see Figure 1. The bulk of the grammar consists of automatically extracted rules. The extraction process begins with a word-aligned corpus: a set of triples (f, e, \\u2014), where f is a French sentence, e is an English sentence, and \\u2014 is a (many-to-many) binary relation between positions off and positions of e. The word alignments are obtained by running GIZA++ (Och and Ney 2000) on the corpus in both directions, and forming the union of the two sets of word alignments. We then extract from each word-aligned sentence pair a set of rules that are consistent with the word alignments. This can be thought of in two steps. First, we identify initial phrase pairs using the same criterion as most phrase-based systems (Och and Ney 2004), namely, there must be at least one word inside one phrase aligned to a word inside the other, but no word inside one phrase can be aligned to a word outside the other phrase. For example, suppose our training data contained the fragment Example derivation of a synchronous CFG. Numbers above arrows are rules used at each step. with word alignments as shown in Figure 2a. The initial phrases that would be extracted are shown in Figure 2b. More formally: Definition 1 Given a word-aligned sentence pair ( f, e, \\u2014), let fji stand for the substring of f from position i to position j inclusive, and similarly for eji. Then a rule ( fj, ele ) is an initial phrase pair of ( f, e, \\u2014) iff: Second, in order to obtain rules from the phrases, we look for phrases that contain other phrases and replace the subphrases with nonterminal symbols. For example, given the initial phrases shown in Figure 2b, we could form the rule where k is an index not used in \\u03b3 and \\u03b1, is a rule of (f, e, \\u2014). This scheme generates a very large number of rules, which is undesirable not only because it makes training and decoding very slow, but also because it creates spurious ambiguity\\u2014a situation where the decoder produces many derivations that are distinct yet have the same model feature vectors and give the same translation. This can result in k-best lists with very few different translations or feature vectors, which is problematic for the minimum-error-rate training algorithm (see Section 4.3). To avoid this, we filter our grammar according to the following constraints, chosen to balance grammar size and performance on our development set: Glue rules. Having extracted rules from the training data, we could let X be the grammar\\u2019s start symbol and translate new sentences using only the extracted rules. But for robustness and for continuity with phrase-based translation models, we allow the grammar to divide a French sentence into a sequence of chunks and translate one chunk at a time. We formalize this inside a synchronous CFG using the rules (14) and (15), which we call the glue rules, repeated here: These rules analyze an S (the start symbol) as a sequence of Xs which are translated without reordering. Note that if we restricted our grammar to comprise only the glue rules and conventional phrase pairs (that is, rules without nonterminal symbols on the right-hand side), the model would reduce to a phrase-based model with monotone translation (no phrase reordering). Entity rules. Finally, for each sentence to be translated, we run some specialized translation modules to translate the numbers, dates, numbers, and bylines in the sentence, and insert these translations into the grammar as new rules.3 Such modules are often used by phrase-based systems as well, but here their translations can plug into hierarchical phrases, for example, into the rule allowing it to generalize over numbers of years. Given a French sentence f, a synchronous CFG will have, in general, many derivations that yield f on the French side, and therefore (in general) many possible translations e. We now define a model over derivations D to predict which translations are more likely than others. Following Och and Ney (2002), we depart from the traditional noisy-channel approach and use a more general log-linear model over derivations D: 3 These modules are due to U. Germann and F. J. Och. In a previous paper (Chiang et al. 2005) we reported on translation modules for numbers and names. The present modules are not the same as those, though the mechanism for integrating them is identical. 209 Computational Linguistics Volume 33, Number 2 where the \\u03c6i are features defined on derivations and the \\u03bbi are feature weights. One of the features is an m-gram language model PLM(e); the remainder of the features we will define as products of functions on the rules used in a derivation: The factors other than the language model factor can be put into a particularly convenient form. A weighted synchronous CFG is a synchronous CFG together with a function w that assigns weights to rules. This function induces a weight function over derivations: It is easy to write dynamic-programming algorithms to find the highest-weight translation or k-best translations with a weighted synchronous CFG. Therefore it is problematic that w(D) does not include the language model, which is extremely important for translation quality. We return to this challenge in Section 5. For our experiments, we use a feature set analogous to the default feature set of Pharaoh (Koehn, Och, and Marcu 2003). The rules extracted from the training bitext have the following features: Finally, for all the rules, there is a word penalty exp(\\u2212#T(\\u03b1)), where #T just counts terminal symbols. This allows the model to learn a general preference for shorter or longer outputs. In order to estimate the parameters of the phrase translation and lexical-weighting features, we need counts for the extracted rules. For each sentence pair in the training data, there is in general more than one derivation of the sentence pair using the rules extracted from it. Because we have observed the sentence pair but have not observed the derivations, we do not know how many times each derivation has been seen, and therefore we do not actually know how many times each rule has been seen. Following Och and others, we use heuristics to hypothesize a distribution of possible rules as though we observed them in the training data, a distribution that does not necessarily maximize the likelihood of the training data.5 Och\\u2019s method gives a count of one to each extracted phrase pair occurrence. We likewise give a count of one to each initial phrase pair occurrence, then distribute its weight equally among the rules obtained by subtracting subphrases from it. Treating this distribution as our observed data, we use relative-frequency estimation to obtain P(\\u03b3  |\\u03b1) and P(\\u03b1  |\\u03b3). Finally, the parameters \\u03bbi of the log-linear model (18) are learned by minimumerror-rate training (Och 2003), which tries to set the parameters so as to maximize the BLEU score (Papineni et al. 2002) of a development set. This gives a weighted synchronous CFG according to (22) that is ready to be used by the decoder. 4 This feature uses word alignment information, which is discarded in the final grammar. If a rule occurs in training with more than one possible word alignment, Koehn, Och, and Marcu take the maximum lexical weight; we take a weighted average. 5 This approach is similar to that taken by many parsers, such as SPATTER (Magerman 1995) and its successors, which use heuristics to hypothesize an augmented version of the training data, but it is especially reminiscent of the Data Oriented Parsing method (Bod 1992), which hypothesizes a distribution over many possible derivations of each training example from subtrees of varying sizes. In brief, our decoder is a CKY (Cocke-Kasami-Younger) parser with beam search together with a postprocessor for mapping French derivations to English derivations. Given a French sentence f, it finds the English yield of the single best derivation that has French yield f: \\ufffde\\u02c6 = e arg max P(D) (24) Ds.t.f(D)=f Note that this is not necessarily the highest-probability English string, which would require a more expensive summation over derivations. We now discuss the details of the decoder, focusing attention on efficiently calculating English language-model probabilities for possible translations, which is the primary technical challenge. In the following we present several parsers as deductive proof systems (Shieber, Schabes, and Pereira 1995; Goodman 1999). A parser in this notation defines a space of weighted items, in which some items are designated axioms and some items are designated goals (the items to be proven), and a set of inference rules of the form which means that if all the items Ii (called the antecedents) are provable, with weight wi, then I (called the consequent) is provable, with weight w, provided the side condition \\u03c6 holds. The parsing process grows a set of provable items: It starts with the axioms, and proceeds by applying inference rules to prove more and more items until a goal is proven. For example, the well-known CKY algorithm for CFGs in Chomsky normal form can be thought of as a deductive proof system whose items can take one of two forms: The axioms would be 6 Treating grammar rules as axioms is not standard practice, but advocated by Goodman (1999). Here, it has the benefit of simplifying the presentation in Section 5.3.4. and the inference rules would be and the goal would be [S, 0, n], where S is the start symbol of the grammar and n is the length of the input string f. Given a synchronous CFG, we could convert its French-side grammar into Chomsky normal form, and then for each sentence, we could find the best parse using CKY. Then it would be a straightforward matter to revert the best parse from Chomsky normal form into the original form and map it into its corresponding English tree, whose yield is the output translation. However, because we have already restricted the number of nonterminal symbols in our rules to two, it is more convenient to use a modified CKY algorithm that operates on our grammar directly, without any conversion to Chomsky normal form. The axioms, inference rules, and goals for the basic decoder are shown in Figure 3. Its time complexity is O(n3), just as CKY\\u2019s is. Because this algorithm does not yet incorporate a language model, let us call it the \\u2212LM parser. The actual search procedure is given by the pseudocode in Figure 4. It organizes the proved items into an array chart whose cells chart[X, i, j] are sets of items. The cells are ordered such that every item comes after its possible antecedents: smaller spans before larger spans, and X items before S items (because of the unary rule S \\u2192 \\ufffdX 1 , X 1 )). Then the parser can proceed by visiting the chart cells in order and trying to prove all the items for each cell. Whenever it proves a new item, it adds the item to the Search procedure for the \\u2212LM parser. appropriate chart cell; in order to reconstruct the derivations later, it must also store, with each item, a tuple of back-pointers to the antecedents from which the item was deduced (for axioms, an empty tuple is used). If two items are added to a cell that are equivalent except for their weights or back-pointers, then they are merged (in the MT decoding literature, this is also known as hypothesis recombination), with the merged item taking its weight and back-pointers from the better of the two equivalent items. (However, if we are interested in finding the k-best derivations, the merged item gets the multiset of all the tuples of back-pointers from the equivalent items. These backpointers are used below in Section 5.2.) The algorithm in Figure 4 does not completely search the space of proofs, because it has a constraint that prohibits any X from spanning a substring longer than a fixed limit \\u039b on the French side, corresponding to the maximum length constraint on initial rules during training. This gives the decoding algorithm an asymptotic time complexity of O(n). In principle \\u039b should match the initial phrase length limit used in training (as it does in our experiments), but in practice it can be adjusted separately to maximize accuracy or speed. We often want to find not only the best derivation for a French sentence but a list of the k-best derivations. These are used for minimum-error-rate training and for rescoring with a language model (Section 5.3.1). We describe here how to do this using the lazy algorithm of Huang and Chiang (2005). Part of this method will also be reused in our algorithm for fast parsing with a language model (Section 5.3.4). If we conceive of lists as functions from indices to values, we may create a virtual list, a function that computes member values on demand instead of storing all the values statically. The heart of the k-best algorithm is a function MERGEPRODUCTS, which takes a set G of tuples of (virtual) lists with an operator \\u2297 and returns a virtual list: Example illustrating MERGEPRODUCTS, where L1 = {1, 2,6, 10} and L2 = {1, 4, 7}. Numbers are negative log-probabilities. It assumes that the input lists are sorted and returns a sorted list. A naive implementation of MERGEPRODUCTS would simply calculate all possible products and sort; however, if we are only interested in the top part of the result, we can implement MERGEPRODUCTS so that the output values are computed lazily and the input lists are accessed only as needed. To do this, we must assume that the multiplication operator \\u00ae is monotonic in each of its arguments. By way of motivation, consider the simple case G = {(L1,L2)}. The full set of possible products can be arranged in a two-dimensional grid (see Figure 5a), which we could then sort to obtain MERGEPRODUCTS(G). But because of our assumptions, we know that the first element of MERGEPRODUCTS(G) must be L1[1] \\u00ae L2[1]. Moreover, we know that the second element must be either L1[1] \\u00ae L2[2] or L1[2] \\u00ae L2[1]. In general (see Figure 5b), if some of the cells have been previously enumerated, the next cell must be one of the cells (shaded gray) adjacent to the previously enumerated ones and we need not consider the others (shaded white). In this way, if we only want to compute the first few elements of MERGEPRODUCTS(G), we can do so by performing a small number of products and discarding the rest of the grid. Figure 6 shows the pseudocode for MERGEPRODUCTS.7 In lines 2\\u20135, a priority queue is initialized with the best element from each L E G, where L ranges over tuples of lists, and 1 stands for a vector whose elements all have the value 1 (the dimensionality of the vector should be evident from the context). The rest of the function creates the virtual list: To enumerate the next element of the list, we first insert the elements adjacent to the previously enumerated element, if any (lines 9\\u201313, where bi stands for the vector whose ith element is 1 and is zero elsewhere), and then enumerate the best element in the priority queue, if any (lines 14\\u201318). We assume standard implementations of 7 This version corrects the behavior of the previously published version in some boundary conditions. Thanks to D. Smith and J. May for pointing those cases out. In the actual implementation, an earlier version is used which has the correct behavior but not for cyclic forests (which the parser never produces). Function for computing the union of products of sorted lists (Huang and Chiang 2005). the priority queue subroutines HEAPIFY, INSERT, and EXTRACTBEST (Cormen et al. 2001). The k-best list generator is then easy to define (Figure 7). First, we generate a parse forest; then we simply apply MERGEPRODUCTS recursively to the whole forest, using memoization to ensure that we generate only one k-best list for each item in the forest. The pseudocode in Figure 7 will find only the weights for the k-best derivations; extending it to output the translations as well is a matter of modifying line 5 to package the English sides of rules together with the weights w, and replacing the real multiplication operator \\u00d7 in line 9 with one that not only multiplies weights but also builds partial translations out of subtranslations. We now turn to the problem of incorporating the language model (LM), describing three methods: first, using the \\u2212LM parser to obtain a k-best list of translations and rescoring it with the LM; second, incorporating the LM directly into the grammar in a construction reminiscent of the intersection of a CFG with a finite-state automaton; third, a hybrid method which we call cube pruning. 5.3.1 Rescoring. One easy way to incorporate the LM into the model would be to decode first using the \\u2212LM parser to produce a k-best list of translations, then to rescore the k-best list using the LM. This method has the potential to be very fast: linear in k. However, because the number of possible translations is exponential in n, we may have to set k extremely high in order to find the true best translation (taking the LM into account) or something acceptably close to it. 5.3.2 Intersection. A more principled solution would be to calculate the LM probabilities online. To do this, we view an m-gram LM as a weighted finite state machine M in which each state corresponds to a sequence of (m \\u2212 1) English terminal symbols. We can then intersect the English side of our weighted CFG G with this finite-state machine to produce a new weighted CFG that incorporates M. Thus PLM would be part of the rule weights (22) just like the other features. (For notational consistency, however, we write the LM probabilities separately from the rule weights.) In principle this method should admit no search errors, though in practice the blow-up in the effective size of the grammar necessitates pruning of the search space, which can cause search errors. The classic construction for intersecting a (non-synchronous) CFG with a finitestate machine is due to Bar-Hillel, Perles, and Shamir (1964), but we use a slightly different construction proposed by Wu (1996) for inversion transduction grammar and bigram LMs. We present an adaptation of his algorithm to synchronous CFGs with two nonterminals per right-hand side and general m-gram LMs. First, assume that the LM expects a whole sentence to be preceded by (m \\u2212 1) start-of-sentence symbols (s) and followed by a single end-of-sentence symbol (/s). The grammar can be made to do this simply by adding a rule and making S\\u2019 the new start symbol. First, we define two functions p and q which operate on strings over T U {*}, where T is the English terminal alphabet, and * is a special placeholder symbol that stands for an elided part of an English string. Values of p and q in the \\u201ccgisf\\u201d example. The function p calculates LM probabilities for all the complete m-grams in a string; the function q elides symbols when all their m-grams have been accounted for. These functions let us correctly calculate the LM score of a sentence piecemeal. For example, let m = 3 and \\u201cc g i s f\\u201d stand for \\u201ccolorless green ideas sleep furiously.\\u201d Then Table 1 shows some values of p and q. Then we may extend the \\u2212LM parser as shown in Figure 8 to use p and q to calculate LM probabilities. We call this parser the +LM parser. The items are of the form [X, i, j; e], signifying that a subtree rooted in X has been recognized spanning from i to j on the French side, and its English translation (possibly with parts elided) is e. The theoretical running time of this algorithm is O(n3|T|4(m\\u22121)), because a deduction can combine up to two starred strings, which each have up to 2(m \\u2212 1) terminal symbols. This is far too slow to use in practice, so we must use beam-search to prune the search space down to a reasonable size. 5.3.3 Pruning. The chart is organized into cells, each of which contains all the items standing for X spanning fji+1. The rule items are also organized into cells, each of which contains all the rules with the same French side and left-hand side. From here on, let us Inference rules for the +LM parser. Here w[x/X] means the string w with the string x substituted for the symbol X. The function q is defined in the text. consider the item scores as costs, that is, negative log (base-10) probabilities. Then, for each cell, we throw out any item that has a score worse than: In the +LM parser, the score of an item [X, i, j; e] in the chart does not reflect the LM probability of generating the first (m \\u2212 1) words of e. Thus two items [X, i, j; e] and [X, i, j; e'] are not directly comparable. To enable more meaningful comparisons, we define a heuristic When comparing items for pruning (and only for pruning), we add this heuristic function to the score of each item. 5.3.4 Cube Pruning. Now we can develop a compromise between the rescoring and intersection methods. Consider Figure 9a. To the left of the grid we have four rules with the same French side, and above we have three items with the same category and span, that is, they belong to the same chart cell. Any of the twelve combinations of these rules and items can be used to deduce a new item (whose scores are shown in the grid), and all these new items will go into the same chart cell (partially listed on the right). The intersection method would compute all twelve items and add them to the new chart cell, where most of them will likely be pruned away. In actuality, the grid may be a cube (one dimension for rules and two dimensions for two nonterminals) with up to b3 elements, whereas the target chart cell can hold at most b items (where b is the limit on the size of the cell imposed during pruning). Thus the vast majority of computed items are pruned. But it is possible to compute only a small corner of the cube and preemptively prune the rest of the items without computing them, a method we refer to as cube pruning. The situation pictured in Figure 9a is very similar to k-best list generation. The four rules to the left of the grid can be thought of like a 4-best list for a single \\u2212LM rule item (X --\\ufffd cong X); the three items above the grid, like a 3-best list for the single \\u2212LM item [X, 6,8]; and the new items to be deduced, like a k-best list for [X, 5, 8], except that we don\\u2019t know what k is in advance. If we could use MERGEPRODUCTS to enumerate the new items best-first, then we could enumerate them until one of them was pruned from the new cell; then the rest of items, which would have a worse score than the pruned item, could be preemptively pruned. MERGEPRODUCTS expects its input lists to be sorted best-first, and the \\u00ae operator to be monotonic in each of its arguments. For cube pruning, we sort items (both in the inputs to MERGEPRODUCTS and in the priority queue inside MERGEPRODUCTS) according to their +LM score, including the heuristic function h. The \\u00ae operator we use takes one or more antecedent items and forms their consequent item according to Example illustrating hybrid method for incorporating the LM. Numbers are negative the +LM parser. Note that the LM makes this \\u2297 only approximately monotonic. This means that the enumeration of new items will not necessarily be best-first. To alleviate this problem, we stop the enumeration not as soon as an item falls outside the beam, but as soon as an item falls outside the beam by a margin of e. This quantity e expresses our guess as to how much the scores of the enumerated items can fluctuate because of the LM. A simpler approach, and probably better in practice, would be simply to set e = 0, that is, to ignore any fluctuation, but increase R and b to compensate. See Figure 9b for an example of cube pruning. The upper-left grid cell is enumerated first, as in the k-best example in Section 5.2, but the choice of the second is different, because of the added LM costs. Then, the third item is enumerated and merged with the first (unlike in the k-best algorithm). Supposing a threshold beam of R = 5 and a margin of e = 0.5, we quit upon considering the next item, because, with a score of 7.7, it falls outside the beam by more than e. The rest of the grid is then discarded. The pseudocode is given in Figure 10. The function INFER+LM is used as the \\u00ae operator; it takes a tuple of antecedent +LM items and returns a consequent +LM item according to the inference rules in Figure 8. The procedure REPARSE+LM takes a \\u2212LM chart chart as input and produces a +LM chart chart'. The variables u, v stand for items in \\u2212LM and u', v', for items in +LM, and the relation v \\ufffdi v' is defined as follows: For each cell in the input chart, it takes the single item from the cell and constructs the virtual list L of all of its +LM counterparts (lines 9\\u201315). Then, it adds the top items of L to the target cell until the cell is judged to be full (lines 16\\u201320). The implementation of our system, named Hiero, is in Python, a bytecode-interpreted language, and optimized using Psyco, a just-in-time compiler (Rigo 2004), and Pyrex, a Python-like compiled language, with C++ code from the SRI Language Modeling Toolkit (Stolcke 2002). In this section we report on experiments with Mandarin-toEnglish translation. Our evaluation metric is case-insensitive BLEU-4 (Papineni et al. 2002), as defined by NIST, that is, using the shortest (as opposed to closest) reference sentence length for the brevity penalty. We ran the grammar extractor of Section 3.2 on the parallel corpora listed in Table 2 with the exception of the United Nations data, for a total of 28 million words (English side).8 We then filtered this grammar for our development set, which was the 2002 NIST MT evaluation dry-run data, and our test sets, which were the data from the 2003\\u20132005 NIST MT evaluations. Some example rules are shown in Table 3, and the sizes of the filtered grammars are shown in Table 4. We also used the SRI Language Modeling Toolkit to train two trigram language models with modified Kneser\\u2013Ney smoothing (Kneser and Ney 1995; Chen and Goodman 1998): one on 2.8 billion words from the English Gigaword corpus, and the other on the English side of the parallel text (28 million words). Table 5 shows the average decoding time on part of the development set for the three LM-incorporation methods described in Section 5.3, on a single processor of a dual 3 GHz Xeon machine. For these experiments, only the Gigaword language model was used. We set b = 30, R = 1 for X cells, b = 15, R = 1 for S cells, and b = 100 for rules except where noted in Table 5. Note that values for R and e are only meaningful relative to the scale of the feature weights; here, the language model weight was 0.06. The feature weights were obtained by minimum-error-rate training using the cube-pruning (e = 0.1) decoder. For the LM rescoring decoder, parsing and k-best list generation used feature weights optimized for the \\u2212LM model, but rescoring used the same weights as the other experiments. We tested the rescoring method (k = 103 and 104), the intersection method, and the cube-pruning method (e = 0, 0.1, and 0.2). The LM rescoring decoder (k = 104) is the fastest but has the poorest BLEU score. Identifying and rescoring the k-best derivations is very quick; the execution time is dominated by reconstructing the output strings for the k-best derivations, so it is possible that further optimization could reduce these times. The intersecting decoder has the best score but runs very slowly. Finally, the cubepruning decoder runs almost as fast as the rescoring decoder and translates almost as well as the intersecting decoder. Among these tests, e = 0.1 gives the best results, but in general the optimal setting will depend on the other beam settings and the scale of the feature weights. We compared Hiero against two baselines: the state-of-the-art phrase-based system ATS (Och et al. 2004; Thayer et al. 2004), and Hiero itself run as a conventional phrase-based system with monotone translation (no phrase reordering). The ATS baseline was trained on all the parallel data listed in Table 1, for a total of 159 million words (English side). The second language model was also trained on the English side of the whole bitext. Phrases of up to 10 in length on the French side were extracted from the parallel text, and minimum-error-rate training (Och 2003) was performed on the development set for 17 features, the same as used in the NIST 2004 and 2005 evaluations.9 These features are similar to the features used for our system, but also include features for phrase-reordering (which are not applicable to our system), IBM Model 1 in both directions, a missing word penalty, and a feature that controls a fallback lexicon. The other baseline, which we call Hiero Monotone, is the same as Hiero except with the limitation that extracted rules cannot have any nonterminal symbols on their righthand sides. In other words, only conventional phrases can be extracted, of length up to 5. These phrases are combined using the glue rules only, which makes the grammar equivalent to a conventional phrase-based model with monotone translation. Thus this system represents the nearest phrase-based equivalent to our model, to provide a controlled test of the effect of hierarchical phrases. We performed minimum-error-rate training separately on Hiero and Hiero Monotone to maximize their BLEU scores on the development set; the feature weights for Hiero are shown in Table 6. The beam settings used for both decoders were R = 30, b = 30 for X cells, R = 30, b = 15 for S cells, b = 100 for rules, and e = 3. On the test set, we found that Hiero improves over both baselines in all three tests (see Table 7). All improvements are statistically significant (p < 0.01) using the sign test as described by Collins, Koehn, and Ku\\u02c7cerov\\u00b4a (2005). Syntax-based statistical machine translation is a twofold challenge. It is a modeling challenge, in part because of the difficulty of coordinating syntactic structures with potentially messy parallel corpora; it is an implementation challenge, because of the added complexity introduced by hierarchical structures. Here we have addressed the modeling challenge by taking only the fundamental idea from syntax, that language is hierarchically structured, and integrating it conservatively into a phrase-based model typical of the current state of the art. This fusion does no violence to the latter; indeed, we have presented our approach as a logical outgrowth of the phrase-based approach. Moreover, hierarchical structure improves translation accuracy significantly. Feature weights obtained by minimum-error-rate training. language model (large) 1.00 language model (bitext) 1.03 The choice to use hierarchical structures that are more complex than flat structures, as well as rules that contain multiple lexical items instead of one, an m-gram model whose structure cuts across the structure of context-free derivations, and large amounts of training data for meaningful comparison with modern systems\\u2014these all threaten to make training a synchronous grammar and translating with it intractable. We have shown how, through training with simple methods inspired by phrase-based models, and translating using a modified CKY with cube pruning, this challenge can be met. Clearly, however, we have only scratched the surface of the modeling challenge. The fact that moving from flat structures to hierarchical structures significantly improves translation quality suggests that more specific ideas from syntax may be valuable as well. There are many possibilities for enriching the simple framework that the present model provides. But the course taken here is one of organic development of an approach known to work well at large-scale tasks, and we plan to stay this course in future work towards more syntactically informed statistical machine translation. I would like to thank Liang Huang, Philipp Koehn, Adam Lopez, Nitin Madnani, Daniel Marcu, Christof Monz, Dragos Munteanu, Philip Resnik, Michael Subotin, Wei Wang, and the anonymous reviewers. This work was partially supported by ONR MURI contract FCPO.810548265, by Department of Defense contract RD-02-5700, and under the GALE program of the Defense Advanced Research Projects Agency, contract HR 0011-06-C-0022. S. D. G. \",\n          \"Over the last few years, two of the main research directions in machine learning of natural language processing have been the study of semi-supervised learning algorithms as a way to train classifiers when the labeled data is scarce, and the study of ways to exploit knowledge and global information in structured learning tasks. In this paper, we suggest a method for incorporating domain knowledge in semi-supervised learning algorithms. Our novel framework unifies can exploit several kinds of specific The experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks. Natural Language Processing (NLP) systems typically require large amounts of knowledge to achieve good performance. Acquiring labeled data is a difficult and expensive task. Therefore, an increasing attention has been recently given to semi-supervised learning, where large amounts of unlabeled data are used to improve the models learned from a small training set (Collins and Singer, 1999; Thelen and Riloff, 2002). The hope is that semi-supervised or even unsupervised approaches, when given enough knowledge about the structure of the problem, will be competitive with the supervised models trained on large training sets. However, in the general case, semi-supervised approaches give mixed results, and sometimes even degrade the model performance (Nigam et al., 2000). In many cases, improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology (Cohen and Sarawagi, 2004; Collins and Singer, 1999; Haghighi and Klein, 2006; Thelen and Riloff, 2002). On the other hand, in the supervised setting, it has been shown that incorporating domain and problem specific structured information can result in substantial improvements (Toutanova et al., 2005; Roth and Yih, 2005). This paper proposes a novel constraints-based learning protocol for guiding semi-supervised learning. We develop a formalism for constraints-based learning that unifies several kinds of constraints: unary, dictionary based and n-ary constraints, which encode structural information and interdependencies among possible labels. One advantage of our formalism is that it allows capturing different levels of constraint violation. Our protocol can be used in the presence of any learning model, including those that acquire additional statistical constraints from observed data while learning (see Section 5. In the experimental part of this paper we use HMMs as the underlying model, and exhibit significant reduction in the number of training examples required in two information extraction problems. As is often the case in semi-supervised learning, the algorithm can be viewed as a process that improves the model by generating feedback through labeling unlabeled examples. Our algorithm pushes this intuition further, in that the use of constraints allows us to better exploit domain information as a way to label, along with the current learned model, unlabeled examples. Given a small amount of labeled data and a large unlabeled pool, our framework initializes the model with the labeled data and then repeatedly: This way, we can generate better \\u201ctraining\\u201d examples during the semi-supervised learning process. The core of our approach, (1), is described in Section 5. The task is described in Section 3 and the Experimental study in Section 6. It is shown there that the improvement on the training examples via the constraints indeed boosts the learned model and the proposed method significantly outperforms the traditional semi-supervised framework. In the semi-supervised domain there are two main approaches for injecting domain specific knowledge. One is using the prior knowledge to accurately tailor the generative model so that it captures the domain structure. For example, (Grenager et al., 2005) proposes Diagonal Transition Models for sequential labeling tasks where neighboring words tend to have the same labels. This is done by constraining the HMM transition matrix, which can be done also for other models, such as CRF. However (Roth and Yih, 2005) showed that reasoning with more expressive, non-sequential constraints can improve the performance for the supervised protocol. A second approach has been to use a small highaccuracy set of labeled tokens as a way to seed and bootstrap the semi-supervised learning. This was used, for example, by (Thelen and Riloff, 2002; Collins and Singer, 1999) in information extraction, and by (Smith and Eisner, 2005) in POS tagging. (Haghighi and Klein, 2006) extends the dictionarybased approach to sequential labeling tasks by propagating the information given in the seeds with contextual word similarity. This follows a conceptually similar approach by (Cohen and Sarawagi, 2004) that uses a large named-entity dictionary, where the similarity between the candidate named-entity and its matching prototype in the dictionary is encoded as a feature in a supervised classifier. In our framework, dictionary lookup approaches are viewed as unary constraints on the output states. We extend these kinds of constraints and allow for more general, n-ary constraints. In the supervised learning setting it has been established that incorporating global information can significantly improve performance on several NLP tasks, including information extraction and semantic role labeling. (Punyakanok et al., 2005; Toutanova et al., 2005; Roth and Yih, 2005). Our formalism is most related to this last work. But, we develop a semi-supervised learning protocol based on this formalism. We also make use of soft constraints and, furthermore, extend the notion of soft constraints to account for multiple levels of constraints\\u2019 violation. Conceptually, although not technically, the most related work to ours is (Shen et al., 2005) that, in a somewhat ad-hoc manner uses soft constraints to guide an unsupervised model that was crafted for mention tracking. To the best of our knowledge, we are the first to suggest a general semi-supervised protocol that is driven by soft constraints. We propose learning with constraints - a framework that combines the approaches described above in a unified and intuitive way. In Section 4 we will develop a general framework for semi-supervised learning with constraints. However, it is useful to illustrate the ideas on concrete problems. Therefore, in this section, we give a brief introduction to the two domains on which we tested our algorithms. We study two information extraction problems in each of which, given text, a set of pre-defined fields is to be identified. Since the fields are typically related and interdependent, these kinds of applications provide a good test case for an approach like ours.1 The first task is to identify fields from citations (McCallum et al., 2000) . The data originally included 500 labeled references, and was later extended with 5,000 unannotated citations collected from papers found on the Internet (Grenager et al., 2005). Given a citation, the task is to extract the each open bracket. The correct assignment was shown in (a). While the predicted label assignment (b) is generally coherent, some constraints are violated. Most obviously, punctuation marks are ignored as cues for state transitions. The constraint \\u201cFields cannot end with stop words (such as \\u201cthe\\u201d)\\u201d may be also good. fields that appear in the given reference. See Fig. 1. There are 13 possible fields including author, title, location, etc. To gain an insight to how the constraints can guide semi-supervised learning, assume that the sentence shown in Figure 1 appears in the unlabeled data pool. Part (a) of the figure shows the correct labeled assignment and part (b) shows the assignment labeled by a HMM trained on 30 labels. However, if we apply the constraint that state transition can occur only on punctuation marks, the same HMM model parameters will result in the correct labeling (a). Therefore, by adding the improved labeled assignment we can generate better training samples during semi-supervised learning. In fact, the punctuation marks are only some of the constraints that can be applied to this problem. The set of constraints we used in our experiments appears in Table 1. Note that some of the constraints are non-local and are very intuitive for people, yet it is very difficult to inject this knowledge into most models. The second problem we consider is extracting fields from advertisements (Grenager et al., 2005). The dataset consists of 8,767 advertisements for apartment rentals in the San Francisco Bay Area downloaded in June 2004 from the Craigslist website. In the dataset, only 302 entries have been labeled with 12 fields, including size, rent, neighborhood, features, and so on. The data was preprocessed using regular expressions for phone numbers, email addresses and URLs. The list of the constraints for this domain is given in Table 1. We implement some global constraints and include unary constraints which were largely imported from the list of seed words used in (Haghighi and Klein, 2006). We slightly modified the seedwords due to difference in preprocessing. given an input sequence x = (x1,... , xN), the task is to find the best assignment to the output variables y = (y1, ... , yM). We denote X to be the space of the possible input sequences and Y to be the set of possible output sequences. We define a structured output classifier as a function h : X \\ue18a Y that uses a global scoring function f : X \\u00d7 Y \\ue18a R to assign scores to each possible input/output pair. Given an input x, a desired function f will assign the correct output y the highest score among all the possible outputs. The global scoring function is often decomposed as a weighted sum of feature functions, This decomposition applies both to discriminative linear models and to generative models such as HMMs and CRFs, in which case the linear sum corresponds to log likelihood assigned to the input/output pair by the model (for details see (Roth, 1999) for the classification case and (Collins, 2002) for the structured case). Even when not dictated by the model, the feature functions fi(x, y) used are local to allow inference tractability. Local feature function can capture some context for each input or output variable, yet it is very limited to allow dynamic programming decoding during inference. Now, consider a scenario where we have a set of constraints C1, ... , CK. We define a constraint C : X \\u00d7 Y \\ue18a {0, 1} as a function that indicates whether the input/output sequence violates some desired properties. When the constraints are hard, the solution is given by from citations and advertisements. Some constraints (represented in the first block of each domain) are global and are relatively difficult to inject into traditional models. While all the constraints hold for the vast majority of the data, some of them are violated by some correct labeled assignments. where 1C(x) is a subset of Y for which all Ci assign the value 1 for the given (x, y). When the constraints are soft, we want to incur some penalty for their violation. Moreover, we want to incorporate into our cost function a measure for the amount of violation incurred by violating the constraint. A generic way to capture this intuition is to introduce a distance function d(y, 1Ci(x)) between the space of outputs that respect the constraint,1Ci(x), and the given output sequence y. One possible way to implement this distance function is as the minimal Hamming distance to a sequence that respects the constraint Ci, that is: d(y, 1Ci(x)) = min(y'E1C(.)) H(y, y'). If the penalty for violating the soft constraint Ci is pi, we write the score function as: We refer to d(y, 1C(x)) as the valuation of the constraint C on (x, y). The intuition behind (1) is as follows. Instead of merely maximizing the model\\u2019s likelihood, we also want to bias the model using some knowledge. The first term of (1) is used to learn from data. The second term biases the mode by using the knowledge encoded in the constraints. Note that we do not normalize our objective function to be a true probability distribution. In this section we present a new constraint-driven learning algorithm (CODL) for using constraints to guide semi-supervised learning. The task is to learn the parameter vector A by using the new objective function (1). While our formulation allows us to train also the coefficients of the constraints valuation, pi, we choose not to do it, since we view this as a way to bias (or enforce) the prior knowledge into the learned model, rather than allowing the data to brush it away. Our experiments demonstrate that the proposed approach is robust to inaccurate approximation of the prior knowledge (assigning the same penalty to all the pi ). We note that in the presence of constraints, the inference procedure (for finding the output y that maximizes the cost function) is usually done with search techniques (rather than Viterbi decoding, see (Toutanova et al., 2005; Roth and Yih, 2005) for a discussion), we chose beamsearch decoding. The semi-supervised learning with constraints is done with an EM-like procedure. We initialize the model with traditional supervised learning (ignoring the constraints) on a small labeled set. Given an unlabeled set U, in the estimation step, the traditional EM algorithm assigns a distribution over labeled assignments Y of each x E U, and in the maximization step, the set of model parameters is learned from the distributions assigned in the estimation step. However, in the presence of constraints, assigning the complete distributions in the estimation step is infeasible since the constraints reshape the distribution in an arbitrary way. As in existing methods for training a model by maximizing a linear cost function (maximize likelihood or discriminative maximization), the distribution over y is represented as the set of scores assigned to it; rather than considering the score assigned to all y's, we truncate the distribution to the top K assignments as returned by the search. Given a set of K top assignments yi, , yK, we approximate the estimation step by assigning uniform probability to the top K candidates, and zero to the other output sequences. We denote this algorithm top-K hard EM. In this paper, we use beamsearch to generate K candidates according to (1). Our training algorithm is summarized in Figure 2. Several things about the algorithm should be clarified: the Top-K-Inference procedure in line 7, the learning procedure in line 9, and the new parameter estimation in line 9. The Top-K-Inference is a procedure that returns the K labeled assignments that maximize the new objective function (1). In our case we used the topK elements in the beam, but this could be applied to any other inference procedure. The fact that the constraints are used in the inference procedure (in particular, for generating new training examples) allows us to use a learning algorithm that ignores the constraints, which is a lot more efficient (although algorithms that do take the constraints into account can be used too). We used maximum likelihood estimation of A but, in general, perceptron or quasiNewton can also be used. It is known that traditional semi-supervised training can degrade the learned model\\u2019s performance. (Nigam et al., 2000) has suggested to balance the contribution of labeled and unlabeled data to the parameters. The intuition is that when iteratively estimating the parameters with EM, we disallow the parameters to drift too far from the supervised model. The parameter re-estimation in line 9, uses a similar intuition, but instead of weighting data instances, we introduced a smoothing parameter -y which controls the convex combination of models induced by the labeled and the unlabeled data. Unlike the technique mentioned above which focuses on naive Bayes, our method allows us to weight linear models generated by different learning algorithms. Another way to look the algorithm is from the self-training perspective (McClosky et al., 2006). Similarly to self-training, we use the current model to generate new training examples from the unlaTop-K-Inference, we use beamsearch to find the Kbest solution according to Eq. (1). beled set. However, there are two important differences. One is that in self-training, once an unlabeled sample was labeled, it is never labeled again. In our case all the samples are relabeled in each iteration. In self-training it is often the case that only high-confidence samples are added to the labeled data pool. While we include all the samples in the training pool, we could also limit ourselves to the high-confidence samples. The second difference is that each unlabeled example generates K labeled instances. The case of one iteration of top-1 hard EM is equivalent to self training, where all the unlabeled samples are added to the labeled pool. There are several possible benefits to using K > 1 samples. (1) It effectively increases the training set by a factor of K (albeit by somewhat noisy examples). In the structured scenario, each of the top-K assignments is likely to have some good components so generating top-K assignments helps leveraging the noise. (2) Given an assignment that does not satisfy some constraints, using top-K allows for multiple ways to correct it. For example, consider the output 11101000 with the constraint that it should belong to the language 1*0*. If the two top scoring corrections are 11111000 and 11100000, considering only one of those can negatively bias the model. In this section, we present empirical results of our algorithms on two domains: citations and advertisements. Both problems are modeled with a simple token-based HMM. We stress that token-based HMM cannot represent many of our constraints. The function d(y, 1C(x)) used is an approximation of a Hamming distance function, discussed in Section 7. For both domains, and all the experiments, -y was set to 0.1. The constraints violation penalty p is set to \\u2212 log 10\\u22124 and \\u2212 log 10\\u22121 for citations and advertisements, resp.2 Note that all constraints share the same penalty. The number of semi-supervised training cycles (line 3 of Figure 2) was set to 5. The constraints for the two domains are listed in Table 1. We trained models on training sets of size varying from 5 to 300 for the citations and from 5 to 100 for the advertisements. Additionally, in all the semi-supervised experiments, 1000 unlabeled examples are used. We report token-based3 accuracy on 100 held-out examples (which do not overlap neither with the training nor with the unlabeled data). We ran 5 experiments in each setting, randomly choosing the training set. The results reported below are the averages over these 5 runs. To verify our claims we implemented several baselines. The first baseline is the supervised learning protocol denoted by sup. The second baseline was a traditional top-1 Hard EM also known as truncated EM4 (denoted by H for Hard). In the third baseline, denoted H&W, we balanced the weight of the supervised and unsupervised models as described in line 9 of Figure 2. We compare these baselines to our proposed protocol, H&W&C, where we added the constraints to guide the H&W protocol. We experimented with two flavors of the algorithm: the top-1 and the top-K version. In the top-K version, the algorithm uses K-best predictions (K=50) for each instance in order to update the model as described in Figure 2. The experimental results for both domains are in given Table 2. As hypothesized, hard EM sometimes from citations and advertisements. N is the number of labeled samples. H is the traditional hard-EM and H&W weighs labeled and unlabeled data as mentioned in Sec. 5. Our proposed model is H&W&C, which uses constraints in the learning procedure. I refers to using constraints during inference at evaluation time. Note that adding constraints improves the accuracy during both learning and inference. degrade the performance. Indeed, with 300 labeled examples in the citations domain, the performance decreases from 86.1 to 80.7. The usefulness of injecting constraints in semi-supervised learning is exhibited in the two right most columns: using constraints H&W&C improves the performance over H&W quite significantly. We carefully examined the contribution of using constraints to the learning stage and the testing stage, and two separate results are presented: testing with constraints (denoted I for inference) and without constraints (no I). The I results are consistently better. And, it is also clear from Table 2, that using constraints in training always improves the model and the amount of improvement depends on the amount of labeled data. Figure 3 compares two protocols on the advertisements domain: H&W+I, where we first run the H&W protocol and then apply the constraints during testing stage, and H&W&C+I, which uses constraints to guide the model during learning and uses it also in testing. Although injecting constraints in the learning process helps, testing with constraints is more important than using constraints during learning, especially when the labeled data size is large. This confirms results reported for the supervised learning case in (Punyakanok et al., 2005; Roth and Yih, 2005). However, as shown, our proposed algorithm H&W&C for training with constraints is critical when the amount labeled data is small. Figure 4 further strengthens this point. In the citations domain, H&W&C+I achieves with 20 labeled samples similar performance to the supervised version without constraints with 300 labeled samples. (Grenager et al., 2005) and (Haghighi and Klein, 2006) also report results for semi-supervised learning for these domains. However, due to different preprocessing, the comparison is not straightforward. For the citation domain, when 20 labeled and 300 unlabeled samples are available, (Grenager et al., 2005) observed an increase from 65.2% to 71.3%. Our improvement is from 70.1% to 79.4%. For the advertisement domain, they observed no improvement, while our model improves from 68.1% to 74.6% with 20 labeled samples. Moreover, we successfully use out-of-domain data (web data) to improve our model, while they report that this data did not improve their unsupervised model. (Haghighi and Klein, 2006) also worked on one of our data sets. Their underlying model, Markov Random Fields, allows more expressive features. Nevertheless, when they use only unary constraints they get 53.75%. When they use their final model, along with a mechanism for extending the prototypes to other tokens, they get results that are comparable to our model with 10 labeled examples. Additionally, in their framework, it is not clear how to use small amounts of labeled data when available. Our model outperforms theirs once we add 10 more examples. This section discusses the importance of using soft constraints rather than hard constraints, the choice of Hamming distance for d(y, 1C(x)) and how we approximate it. We use two constraints to illustrate the ideas. (C1): \\u201cstate transitions can only occur on punctuation marks or newlines\\u201d, and (C2): \\u201cthe field TITLE must appear\\u201d. First, we claim that defining d(y, 1C(x)) to be the Hamming distance is superior to using a binary value, d(y, 1C(x)) = 0 if y E 1C(x) and 1 otherwise. Consider, for example, the constraint C1 in the advertisements domain. While the vast majority of the instances satisfy the constraint, some violate it in more than one place. Therefore, once the binary distance is set to 1, the algorithm looses the ability to discriminate constraint violations in other locations of the same instance. This may hurt the performance in both the inference and the learning stage. Computing the Hamming distance exactly can be a computationally hard problem. Furthermore, it is unreasonable to implement the exact computation for each constraint. Therefore, we implemented a generic approximation for the hamming distance assuming only that we are given a boolean function OC(yN) that returns whether labeling the token xN with state yN violates constraint with respect to an already labeled consider the prefix x1, x2, x3, x4, which contains no punctuation or newlines and was labeled AUTH, AUTH, DATE, DATE. This labeling violates C1, the minimal hamming distance is 2, and our approximation gives 1, (since there is only one transition that violates the constraint.) For constraints which cannot be validated based on prefix information, our approximation resorts to binary violation count. For instance, the constraint C2 cannot be implemented with prefix information when the assignment is not complete. Otherwise, it would mean that the field TITLE should appear as early as possible in the assignment. While (Roth and Yih, 2005) showed the significance of using hard constraints, our experiments show that using soft constraints is a superior option. For example, in the advertisements domain, C1 holds for the large majority of the gold-labeled instances, but is sometimes violated. In supervised training with 100 labeled examples on this domain, sup gave 76.3% accuracy. When the constraint violation penalty p was infinity (equivalent to hard constraint), the accuracy improved to 78.7%, but when the penalty was set to \\u2212log(0.1), the accuracy of the model jumped to 80.6%. We proposed to use constraints as a way to guide semi-supervised learning. The framework developed is general both in terms of the representation and expressiveness of the constraints, and in terms of the underlying model being learned \\u2013 HMM in the current implementation. Moreover, our framework is a useful tool when the domain knowledge cannot be expressed by the model. The results show that constraints improve not only the performance of the final inference stage but also propagate useful information during the semisupervised learning process and that training with the constraints is especially significant when the number of labeled training data is small. Acknowledgments: This work is supported by NSF SoDHCER-0613885 and by a grant from Boeing. Part of this work was done while Dan Roth visited the Technion, Israel, supported by a Lady Davis Fellowship. \",\n          \"Our work adopts major components of the algorithm from (Luo & Roukos 1996): language model (LM) parameter estimation from a segmented corpus and input segmentation on the basis of LM probabilities. However, our work diverges from their work in two crucial respects: (i) new technique of computing all possible segmentations of a word into prefix*-stem-suffix* for decoding, and (ii) unsupervised algorithm for new stem acquisition based on a stem candidate's similarity to stems occurring in the training corpus. (Darwish 2002) presents a supervised technique which identifies the root of an Arabic word by stripping away the prefix and the suffix of the word on the basis of manually acquired dictionary of word-root pairs and the likelihood that a prefix and a suffix would occur with the template from which the root is derived. He reports 92.7% segmentation marking a prefix with '#&quot; and a suffix with '+' will be adopted throughout the paper. accuracy on a 9,606 word evaluation corpus. His technique pre-supposes at most one prefix and one suffix per stem regardless of the actual number and meanings of prefixes/suffixes associated with the stem. (Beesley 1996) presents a finite-state morphological analyzer for Arabic, which displays the root, pattern, and prefixes/suffixes. The analyses are based on manually acquired lexicons and rules. Although his analyzer is comprehensive in the types of knowledge it presents, it has been criticized for their extensive development time and lack of robustness, cf. (Darwish 2002). (Yarowsky and Wicentowsky 2000) presents a minimally supervised morphological analysis with a performance of over 99.2% accuracy for the 3,888 past-tense test cases in English. The core algorithm lies in the estimation of a probabilistic alignment between inflected forms and root forms. The probability estimation is based on the lemma alignment by frequency ratio similarity among different inflectional forms derived from the same lemma, given a table of inflectional parts-of-speech, a list of the canonical suffixes for each part of speech, and a list of the candidate noun, verb and adjective roots of the language. Their algorithm does not handle multiple affixes per word. (Goldsmith 2000) presents an unsupervised technique based on the expectationmaximization algorithm and minimum description length to segment exactly one suffix per word, resulting in an F-score of 81.8 for suffix identification in English according to (Schone and Jurafsky 2001). (Schone and Jurafsky 2001) proposes an unsupervised algorithm capable of automatically inducing the morphology of inflectional languages using only text corpora. Their algorithm combines cues from orthography, semantics, and contextual information to induce morphological relationships in German, Dutch, and English, among others. They report Fscores between 85 and 93 for suffix analyses and between 78 and 85 for circumfix analyses in these languages. Although their algorithm captures prefix-suffix combinations or circumfixes, it does not handle the multiple affixes per word we observe in Arabic. Given an Arabic sentence, we use a trigram language model on morphemes to segment it into a sequence of morphemes {m1, m2, ...,mn}. The input to the morpheme segmenter is a sequence of Arabic tokens \\u2013 we use a tokenizer that looks only at white space and other punctuation, e.g. quotation marks, parentheses, period, comma, etc. A sample of a manually segmented corpus is given below2. Here multiple occurrences of prefixes and suffixes per word are marked with an underline. Many instances of prefixes and suffixes in Arabic are meaning bearing and correspond to a word in English such as pronouns and prepositions. Therefore, we choose a segmentation into multiple prefixes and suffixes. Segmentation into one prefix and one suffix per word, cf. (Darwish 2002), is not very useful for applications like statistical machine translation, (Brown et al. 1993), for which an accurate word-to-word alignment between the source and the target languages is critical for high quality translations. The trigram language model probabilities of morpheme sequences, p(mi|mi-1, mi-2), are estimated from the morpheme-segmented corpus. At token boundaries, the morphemes from previous tokens constitute the histories of the current morpheme in the trigram language model. The trigram model is smoothed using deleted interpolation with the bigram and unigram models, (Jelinek 1997), as in (1): w# kAn AyrfAyn Al*y Hl fy Al# mrkz Al# Awl fy jA}z +p Al# nmsA Al# EAm Al# mADy Ely syAr +p fyrAry $Er b# AlAm fy bTn +h ADTr +t +h Aly Al# AnsHAb mn Al# tjArb w# hw s# y# Ewd Aly lndn l# AjrA' Al# fHwS +At Al# Drwry +p Hsb mA A$Ar fryq 2 A manually segmented Arabic corpus containing about 140K word tokens has been provided by LDC (http://www.ldc.upenn.edu). We divided this corpus into training and the development test sets as described in Section 5. A small morpheme-segmented corpus results in a relatively high out of vocabulary rate for the stems. We describe below an unsupervised acquisition of new stems from a large unsegmented Arabic corpus. However, we first describe the segmentation algorithm. We take the unit of decoding to be a sentence that has been tokenized using white space and punctuation. The task of a decoder is to find the morpheme sequence which maximizes the trigram probability of the input sentence, as in (2): (2) SEGMENTATIONbest = Argmax IIi=1, N p(mi|mi-1mi-2), N = number of morphemes in the input. Search algorithm for (2) is informally described for each word token as follows: Step 1: Compute all possible segmentations of the token (to be elaborated in 3.2.1). Step 2: Compute the trigram language model score of each segmentation. For some segmentations of a token, the stem may be an out of vocabulary item. In that case, we use an \\u201cUNKNOWN\\u201d class in the trigram language model with the model probability given by p(UNKNOWN|mi-1, mi-2) * UNK_Fraction, where UNK_Fraction is 1e-9 determined on empirical grounds. This allows us to segment new words with a high accuracy even with a relatively high number of unknown stems in the language model vocabulary, cf. experimental results in Tables 5 & 6. Step 3: Keep the top N highest scored segmentations. Possible segmentations of a word token are restricted to those derivable from a table of prefixes and suffixes of the language for decoder speed-up and improved accuracy. Table 2 shows examples of atomic (e.g. \\u0644\\u0627, \\u062a\\u0627) and multi-component (e.g. \\u0644\\ufe8e\\u0467\\ufe91\\u0648, \\ufe8e\\u0467\\ufeec\\ufe97\\u0627) prefixes and suffixes, along with their component morphemes in native Arabic.3 Each token is assumed to have the structure prefix*-stem-suffix*, and is compared against the prefix/suffix table for segmentation. Given a word token, (i) identify all of the matching prefixes and suffixes from the table, (ii) further segment each matching prefix/suffix at each character position, and (iii) enumerate all prefix*-stem-suffix* sequences derivable from (i) and (ii). Table 3 shows all of its possible segmentations of the token \\ufe8e\\u0647\\u0631\\ufeae\\u0622\\u0627\\u0648 (wAkrrhA; 'and I repeat it'),4 where 0 indicates the null prefix/suffix and the Seg Score is the language model probabilities of each segmentation S1 ... S12. For this token, there are two matching prefixes #\\u0648(w#) and #\\u0627\\u0648(wA#) from the prefix table, and two matching suffixes \\u0627+(+A) and \\ufe8e\\u0647+(+hA) from the suffix table. S1, S2, & S3 are the segmentations given the null prefix 0 and suffixes 0, +A, +hA. S4, S5, & S6 are the segmentations given the prefix w# and suffixes 0, +A, +hA. S7, S8, & S9 are the segmentations given the prefix wA# and suffixes 0, +A, +hA. S10, S11, & S12 are the segmentations given the prefix sequence w# A# derived from the prefix wA# and suffixes 0, +A, +hA. As illustrated by S12, derivation of sub-segmentations of the matching prefixes/suffixes enables the system to identify possible segmentations which would have been missed otherwise. In this case, segmentation including the derived prefix sequence \\ufe8e\\u0647+ \\u0631\\ufeae\\u0622 #\\u0627 #\\u0648(n# A# krr +hA) happens to be the correct one. While the number of possible segmentations is maximized by sub-segmenting matching prefixes and suffixes, some of illegitimate subsegmentations are filtered out on the basis of the knowledge specific to the manually segmented corpus. For instance, subsegmentation of the suffix hA into +h +A is ruled out because there is no suffix sequence +h +A in the training corpus. Likewise, subsegmentation of the prefix Al into A# l# is filtered out. Filtering out improbable prefix/suffix sequences improves the segmentation accuracy, as shown in Table 5. Once the seed segmenter is developed on the basis of a manually segmented corpus, the performance may be improved by iteratively expanding the stem vocabulary and retraining the language model on a large automatically segmented Arabic corpus. Given a small manually segmented corpus and a large unsegmented corpus, segmenter development proceeds as follows. Initialization: Develop the seed segmenter Segmenter0 trained on the manually segmented corpus Corpus0, using the language model vocabulary, Vocab0, acquired from Corpus0. Iteration: For i = 1 to N, N = the number of partitions of the unsegmented corpus Vocabi-1, creating an expanded vocabulary Vocabi. iii. Develop Segmenteri trained on Corpus0 through Corpusi with Vocabi. Optimal Performance Identification: Identify the Corpusi and Vocabi, which result in the best performance, i.e. system training with Corpusi+1 and Vocabi+1 does not improve the performance any more. Unsupervised acquisition of new stems from an automatically segmented new corpus is a three-step process: (i) select new stem candidates on the basis of a frequency threshold, (ii) filter out new stem candidates containing a sub-string with a high likelihood of being a prefix, suffix, or prefix-suffix. The likelihood of a sub-string being a prefix, suffix, and prefix-suffix of a token is computed as in (5) to (7), (iii) further filter out new stem candidates on the basis of contextual information, as in (8). Stem candidates containing a sub-string with a high prefix, suffix, or prefix-suffix likelihood are filtered out. Example sub-strings with the prefix, suffix, prefix-suffix likelihood 0.85 or higher in a 110K word manually segmented corpus are given in Table 4. If a token starts with the sub-string \\u0640\\ufee8\\ufeb1 (sn), and end with \\ufe8e\\ufeec\\u0640 (hA), the sub-string's likelihood of being the prefix-suffix of the token is 1. If a token starts with the sub-string \\ufede\\u0467\\ufedf (ll), the sub-string's likelihood of being the prefix of the token is 0.945, etc. According to (8), if a stem is followed by a potential suffix +m, not present in the training corpus, then it is filtered out as an illegitimate stem. In addition, if a stem is preceded by a prefix and/or followed by a suffix with a significantly higher proportion than that observed in the training corpus, it is filtered out. For instance, the probability for the suffix +A to follow a stem is less than 50% in the training corpus regardless of the stem properties, and therefore, if a candidate stem is followed by +A with the probability of over 70%, e.g. mAnyl +A, then it is filtered out as an illegitimate stem. We present experimental results illustrating the impact of three factors on segmentation error rate: (i) the base algorithm, i.e. language model training and decoding, (ii) language model vocabulary and training corpus size, and (iii) manually segmented training corpus size. Segmentation error rate is defined in (9). Evaluations have been performed on a development test corpus containing 28,449 word tokens. The test set is extracted from 20001115_AFP_ARB.0060.xml.txt through 20001115_AFP_ARB.0236.xml.txt of the LDC Arabic Treebank: Part 1 v 2.0 Corpus. Impact of the core algorithm and the unsupervised stem acquisition has been measured on segmenters developed from 4 different sizes of manually segmented seed corpora: 10K, 20K, 40K, and 110K words. The experimental results are shown in Table 5. The baseline performances are obtained by assigning each token the most frequently occurring segmentation in the manually segmented training corpus. The column headed by '3-gram LM' indicates the impact of the segmenter using only trigram language model probabilities for decoding. Regardless of the manually segmented training corpus size, use of trigram language model probabilities reduces the word error rate of the corresponding baseline by approximately 50%. The column headed by '3-gram LM + PS Filter' indicates the impact of the core algorithm plus Prefix-Suffix Filter discussed in Section 3.2.2. Prefix-Suffix Filter reduces the word error rate ranging from 7.4% for the smallest (10K word) manually segmented corpus to 21.8% for the largest (110K word) manually segmented corpus \\uf8e7- around 1% absolute reduction for all segmenters. The column headed by '3-gram LM + PS Filter + New Stems' shows the impact of unsupervised stem acquisition from a 155 million word Arabic corpus. Word error rate reduction due to the unsupervised stem acquisition is 38% for the segmenter developed from the 10K word manually segmented corpus and 32% for the segmenter developed from 110K word manually segmented corpus. Language model vocabulary size (LM VOC Size) and the unknown stem ratio (OOV ratio) of various segmenters is given in Table 6. For unsupervised stem acquisition, we have set the frequency threshold at 10 for every 10-15 million word corpus, i.e. any new morphemes occurring more than 10 times in a 10-15 million word corpus are considered to be new stem candidates. Prefix, suffix, prefix-suffix likelihood score to further filter out illegitimate stem candidates was set at 0.5 for the segmenters developed from 10K, 20K, and 40K manually segmented corpora, whereas it was set at 0.85 for the segmenters developed from a 110K manually segmented corpus. Both the frequency threshold and the optimal prefix, suffix, prefix-suffix likelihood scores were determined on empirical grounds. Contextual Filter stated in (8) has been applied only to the segmenter developed from 110K manually segmented training corpus.5 Comparison of Tables 5 and 6 indicates a high correlation between the segmentation error rate and the unknown stem ratio. Table 7 gives the error analyses of four segmenters according to three factors: (i) errors due to unknown stems, (ii) errors involving \\u0645\\ufeee\\ufef4\\u0467\\ufedf\\u0627 (Alywm), and (iii) errors due to other factors. Interestingly, the segmenter developed from a 110K manually segmented corpus has the lowest percentage of \\u201cunknown stem\\u201d errors at 39.6% indicating that our unsupervised acquisition of new stems is working well, as well as suggesting to use a larger unsegmented corpus for unsupervised stem acquisition. \\u0645\\ufeee\\ufef4\\u0467\\ufedf\\u0627 (Alywm) should be segmented differently depending on its part-of-speech to capture the semantic ambiguities. If it is an adverb or a proper noun, it is segmented as \\u0645\\ufeee\\ufef4\\u0467\\ufedf\\u0627 'today/Al-Youm', whereas if it is a noun, it is segmented as \\u0645\\ufeee\\u0467\\ufef3 #\\u0644\\u0627 'the day.' Proper segmentation of \\u0645\\ufeee\\ufef4\\u0467\\ufedf\\u0627 primarily requires its part-of-speech information, and cannot be easily handled by morpheme trigram models alone. Other errors include over-segmentation of foreign words such as \\ufee6\\u0467\\ufef4\\u0467\\ufe97\\ufeee\\u0467\\ufe91 (bwtyn) as \\u0628# \\ufee6\\u0467\\ufef4\\u0467\\ufe97\\u0648 and \\ufeae\\u0467\\ufe98\\u0467\\ufef4\\u0467\\ufedf (lytr) 'litre' as \\ufeae\\u0467\\ufe97 #\\u064a #\\u0644. These errors are attributed to the segmentation ambiguities of these tokens: \\ufee6\\u0467\\ufef4\\u0467\\ufe97\\ufeee\\u0467\\ufe91 is ambiguous between '\\ufee6\\u0467\\ufef4\\u0467\\ufe97\\ufeee\\u0467\\ufe91 (Putin)' and '\\u0628# \\ufee6\\u0467\\ufef4\\u0467\\ufe97\\u0648 (by aorta)'. \\ufeae\\u0467\\ufe98\\u0467\\ufef4\\u0467\\ufedf is ambiguous between '\\ufeae\\u0467\\ufe98\\u0467\\ufef4\\u0467\\ufedf (litre)' and ' \\ufeae\\u0467\\ufe97 #\\u064a #\\u0644 (for him to harm)'. These errors may also be corrected by incorporating part-of-speech information for disambiguation. To address the segmentation ambiguity problem, as illustrated by ' \\ufee6\\u0467\\ufef4\\u0467\\ufe97\\ufeee\\u0467\\ufe91 (Putin)' vs. ' \\ufee6\\u0467\\ufef4\\u0467\\ufe97\\u0648 #\\u0628 (by aorta)', we have developed a joint model for segmentation and part-ofspeech tagging for which the best segmentation of an input sentence is obtained according to the formula (10), where ti is the part-of-speech of morpheme mi, and N is the number of morphemes in the input sentence. (10) SEGMENTATIONbest = Argmax \\u03a0i=1,N p(mi|mi-1 mi-2) p(ti|ti-1 ti-2) p(mi|ti) By using the joint model, the segmentation word error rate of the best performing segmenter has been reduced by about 10% from 2.9% (cf. the last column of Table 5) to 2.6%. We have presented a robust word segmentation algorithm which segments a word into a prefix*-stem-suffix* sequence, along with experimental results. Our Arabic word segmentation system implementing the algorithm achieves around 97% segmentation accuracy on a development test corpus containing 28,449 word tokens. Since the algorithm can identify any number of prefixes and suffixes of a given token, it is generally applicable to various language families including agglutinative languages (Korean, Turkish, Finnish), highly inflected languages (Russian, Czech) as well as semitic languages (Arabic, Hebrew). Our future work includes (i) application of the current technique to other highly inflected languages, (ii) application of the unsupervised stem acquisition technique on about 1 billion word unsegmented Arabic corpus, and (iii) adoption of a novel morphological analysis technique to handle irregular morphology, as realized in Arabic broken plurals YL+S (ktAb) 'book' vs. \\ufffd\\ufffd\\u201e\\ufffd< (ktb) 'books'. This work was partially supported by the Defense Advanced Research Projects Agency and monitored by SPAWAR under contract No. N66001-99-2-8916. The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred. We would like to thank Martin Franz for discussions on language model building, and his help with the use of ViaVoice language model toolkit. \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1005,\n        \"samples\": [\n          \"The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor.) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand :mbjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words. In our work, the significance of an association (x, y) is measured by the mutual information I (x, y) ,i.e. the probability of observing x and y together, compared with the probability of observing x and y independently.\",\n          \"Over the last few years, two of the main research directions in machine learning of natural language processing have been the study of semi-supervised learning algorithms as a way to train classifiers when the labeled data is scarce, and the study of ways to exploit knowledge and global information in structured learning tasks. In this paper, we suggest a method for incorporating domain knowledge in semi-supervised learning algorithms. Our novel framework unifies and can exploit several kinds of task specific constraints. The experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks. We introduce constraint driven learning, CoDL. We use constraints at multiple levels, such as sentence-level constraints to specify field boundaries and global constraints to ensure relation-level consistency.\",\n          \"We approximate Arabic\\u2019s rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix* (* denotes zero or more occurrences of a morpheme). Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus. The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input. The language model is initially estimated from a small manually segmented corpus of about 110,000 words. To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus. The resulting Arabic word segmentation system achieves around 97% exact match accuracy on a test corpus containing 28,449 word tokens. We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest. We demonstrate a technique for segmenting Arabic text and use it as a morphological processing step in machine translation.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}